[
  {
    "objectID": "website-information.html",
    "href": "website-information.html",
    "title": "How to use this website for the computer practical sessions",
    "section": "",
    "text": "This website aims to provide you with sufficient information to effectively complete all the tasks and exercises for each computer practical session, which are intended to help you learn some useful and powerful data analysis skills. Specifically, we aim to help you learn some key skills for carrying out common approaches to sampling data, preparing data for analysis, analysing data, and interpreting the results of your analyses.\nWe do try to limit the information presented to just the most critical content given this is an introductory module, and we usually hide more in-depth or optional content via Read/hide buttons, which you can then choose whether to read at all or later. Therefore, if you wish to progress your quantitative research skills after covering this content please see Minerva for recommended further resources.\n\n\n\nEach computer practical session covers one or more related topics covered in a previous lecture, and there is one or more corresponding sections or sub-sections on the website we will work through in the session. We will tell you at the start of each session which section(s) or sub-section(s) you are expected to work through. You can see all the sections listed on the left of the website, and you can use those links to move between sections.\nEach section is designed to provide you with some background information on one or more specific methods within the broader topic covered in that section, such as the rationale for the approaches and some basic theory, followed by one or more exercises that relate to a hypothetical “scenario” where you get to apply the method(s). The idea is that you will hopefully be able to work through each section of the website on your own or with other students (we encourage discussion and helping each other!). However, please ask us for help with any problems, and please ask us for help in understanding and clarifying anything.\nEach scenario is a hypothetical (made-up) situation that describes a realistic example of a situation where you might need to use the method(s) covered (e.g. you need to select a sample of health facilities for a survey). For each scenario we provide some artificial but realistic data. You are then provided with detailed instructions on how to apply the method(s) to the example data. Where more than one specific method is covered these will be listed as “Exercise 1: …” etc.\nNote: once you are in a section you can navigate between the background information, the scenario, instructions on the method etc using the sub-menu that will appear on the right side of the website. We provide step-by-step video-based and text-based instructions on how to carry out each method using the relevant software program, although video-based instructions are not available for all sections/sub-sections. The video- and text-based instructions are identical, so you can choose whether to watch a video or read instructions to understand how to do everything you need to do.\nNote: with the text-based instructions we refer to the names of menus, tools, buttons etc as they appear or are named in the different pieces of software using highlighted text like this.\n\n\n\nThroughout this website we use buttons like the one below to reveal/hide text, such as optional information, or the answers to exercises that you should check only after you’ve completed them:\n\n\nRead/hide\n\nNow you can read the hidden text. You can also click the Read/hide button again to hide the text.\n\n\n\n\n\n\nRead/hide\n\nFor the Introduction to Quantitative Research Methods in International Health module you have to complete a “summative assignment”, which accounts for 100% of your module mark. This assignment will require you to directly apply the skills you learn in these sessions. Please note that when analysing data for the summative assignment we only expect you to use methods that we will cover in these sessions. For further details please see the Assessment Brief.\n\n\n\n\n\n\nRead/hide\n\nStudents often find learning and doing statistics to be hard, sometimes very hard! Unfortunately this is to be expected and there are no short-cuts or secrets. Unless you are a genius then like most people, including ourselves, to really begin to understand statistics more deeply and to be able to confidently and correctly analyse typical datasets independently will take a lot of time and effort. However, we firmly believe that it is perfectly possible for any motivated student to get to any level of skill and understanding they wish to reach if they are simply prepared to put in the necessary effort and practice, and the rewards can be huge. Being able to correctly run and interpret even simple statistical analyses gives you skills that are highly valuable in the world of research, and which are increasingly valuable and sought after in the wider world. So please don’t despair when things seem difficult or impossible. We’ve all felt that way, but like any new skill it really does gets easier and easier with practice, and it can even be enjoyable (no really, being able to take a dataset and generate insights about the world can be a really exciting and enjoyable process!).",
    "crumbs": [
      "How to use this website"
    ]
  },
  {
    "objectID": "website-information.html#aims",
    "href": "website-information.html#aims",
    "title": "How to use this website for the computer practical sessions",
    "section": "",
    "text": "This website aims to provide you with sufficient information to effectively complete all the tasks and exercises for each computer practical session, which are intended to help you learn some useful and powerful data analysis skills. Specifically, we aim to help you learn some key skills for carrying out common approaches to sampling data, preparing data for analysis, analysing data, and interpreting the results of your analyses.\nWe do try to limit the information presented to just the most critical content given this is an introductory module, and we usually hide more in-depth or optional content via Read/hide buttons, which you can then choose whether to read at all or later. Therefore, if you wish to progress your quantitative research skills after covering this content please see Minerva for recommended further resources.",
    "crumbs": [
      "How to use this website"
    ]
  },
  {
    "objectID": "website-information.html#website-structure-computer-practical-sessions",
    "href": "website-information.html#website-structure-computer-practical-sessions",
    "title": "How to use this website for the computer practical sessions",
    "section": "",
    "text": "Each computer practical session covers one or more related topics covered in a previous lecture, and there is one or more corresponding sections or sub-sections on the website we will work through in the session. We will tell you at the start of each session which section(s) or sub-section(s) you are expected to work through. You can see all the sections listed on the left of the website, and you can use those links to move between sections.\nEach section is designed to provide you with some background information on one or more specific methods within the broader topic covered in that section, such as the rationale for the approaches and some basic theory, followed by one or more exercises that relate to a hypothetical “scenario” where you get to apply the method(s). The idea is that you will hopefully be able to work through each section of the website on your own or with other students (we encourage discussion and helping each other!). However, please ask us for help with any problems, and please ask us for help in understanding and clarifying anything.\nEach scenario is a hypothetical (made-up) situation that describes a realistic example of a situation where you might need to use the method(s) covered (e.g. you need to select a sample of health facilities for a survey). For each scenario we provide some artificial but realistic data. You are then provided with detailed instructions on how to apply the method(s) to the example data. Where more than one specific method is covered these will be listed as “Exercise 1: …” etc.\nNote: once you are in a section you can navigate between the background information, the scenario, instructions on the method etc using the sub-menu that will appear on the right side of the website. We provide step-by-step video-based and text-based instructions on how to carry out each method using the relevant software program, although video-based instructions are not available for all sections/sub-sections. The video- and text-based instructions are identical, so you can choose whether to watch a video or read instructions to understand how to do everything you need to do.\nNote: with the text-based instructions we refer to the names of menus, tools, buttons etc as they appear or are named in the different pieces of software using highlighted text like this.",
    "crumbs": [
      "How to use this website"
    ]
  },
  {
    "objectID": "website-information.html#hidden-text",
    "href": "website-information.html#hidden-text",
    "title": "How to use this website for the computer practical sessions",
    "section": "",
    "text": "Throughout this website we use buttons like the one below to reveal/hide text, such as optional information, or the answers to exercises that you should check only after you’ve completed them:\n\n\nRead/hide\n\nNow you can read the hidden text. You can also click the Read/hide button again to hide the text.",
    "crumbs": [
      "How to use this website"
    ]
  },
  {
    "objectID": "website-information.html#how-these-sessions-relate-to-the-summative-assignment",
    "href": "website-information.html#how-these-sessions-relate-to-the-summative-assignment",
    "title": "How to use this website for the computer practical sessions",
    "section": "",
    "text": "Read/hide\n\nFor the Introduction to Quantitative Research Methods in International Health module you have to complete a “summative assignment”, which accounts for 100% of your module mark. This assignment will require you to directly apply the skills you learn in these sessions. Please note that when analysing data for the summative assignment we only expect you to use methods that we will cover in these sessions. For further details please see the Assessment Brief.",
    "crumbs": [
      "How to use this website"
    ]
  },
  {
    "objectID": "website-information.html#a-realistic-word-of-encouragement-if-you-wantneed-it",
    "href": "website-information.html#a-realistic-word-of-encouragement-if-you-wantneed-it",
    "title": "How to use this website for the computer practical sessions",
    "section": "",
    "text": "Read/hide\n\nStudents often find learning and doing statistics to be hard, sometimes very hard! Unfortunately this is to be expected and there are no short-cuts or secrets. Unless you are a genius then like most people, including ourselves, to really begin to understand statistics more deeply and to be able to confidently and correctly analyse typical datasets independently will take a lot of time and effort. However, we firmly believe that it is perfectly possible for any motivated student to get to any level of skill and understanding they wish to reach if they are simply prepared to put in the necessary effort and practice, and the rewards can be huge. Being able to correctly run and interpret even simple statistical analyses gives you skills that are highly valuable in the world of research, and which are increasingly valuable and sought after in the wider world. So please don’t despair when things seem difficult or impossible. We’ve all felt that way, but like any new skill it really does gets easier and easier with practice, and it can even be enjoyable (no really, being able to take a dataset and generate insights about the world can be a really exciting and enjoyable process!).",
    "crumbs": [
      "How to use this website"
    ]
  },
  {
    "objectID": "sample-description.html",
    "href": "sample-description.html",
    "title": "Exploring a dataset and describing your sample’s characteristics",
    "section": "",
    "text": "In this practical we will practice some common approaches used to explore your dataset and describe the important characteristics of your sample.",
    "crumbs": [
      "3. Data exploration & sample description"
    ]
  },
  {
    "objectID": "sample-description.html#explanation",
    "href": "sample-description.html#explanation",
    "title": "Exploring a dataset and describing your sample’s characteristics",
    "section": "Explanation",
    "text": "Explanation\nIn the last section we saw how to prepare a raw dataset for analysis. Once you have done the basic steps to prepare your dataset for analysis the first step in any data analysis is to statistically describe the variables in your dataset, and usually explore key associations as well. This is done for two distinct reasons described below, although the methods used are the same and the results produced may be used for both purposes.\n1. Data exploration and analysis planning.\nStatistically describing the variables in your dataset allows you understand your variables (i.e. the data they contain and the distribution of those values) and some initial aspects of the associations between them. Typically, for a quantitative study with clear research questions it is strongly recommended that you develop a clear analysis plan prior to doing any analyses to avoid the big risks of bias that can come from data-driven analyses. However, it is still acceptable to explore your data first to help inform those pre-planned analyses, while clearly documenting the reasons for any departures/modifications from the analysis plan. You may also be carrying out a purely exploratory study where the analyses are largely determined by your initial data exploration.\nNote: when descriptively exploring variables to understand your data and plan your analyses it is recommended to make extensive use of not just numerical descriptive statistics but also graphs, because we can easily understand and process information visually. However, graphs are not typically used in papers for describing samples, probably mainly due to space constraints. The most commonly used graph to describe a single numerical variable is a histogram, and the most commonly used type of graph to describe a single categorical variable is a bar chart.\nA key part of any initial data exploration is also understanding the extent of any missing data across the variables in your study. A more thorough and in-depth exploration would also look at patterns of missing data, but that is beyond this course.\n2. Describing the key characteristics of the sample.\nStatistically describing key variables in the dataset allows you to describe what the sample “looks” like in terms of its key characteristics. Key characteristics are typically those that we have good reason to believe are important determinants of our outcomes of interest or the associations of interest that our inferential analyses will look at. Describing the sample’s characteristics allows both the research team and readers of your results to understand what the units of observation in the study look like in terms of those key characteristics, and this helps us and readers judge what target population(s) inferential results can be generalised to. These results often come in the first table (table 1) of a results paper. It is also helpful for readers to describe the amount of missing data in each of the variables presented.\nFor example, in the study scenario we will use in the exercise below we are interested in understanding things like the mean systolic blood pressure (in mmHg) and the prevalence of hypertension in our target population (which we look at in the next section), as well as associations between socio-demographic and health related characteristics and systolic blood pressure/hypertension in our target population (which we will look at in later sections). Clearly, age is one (of many) important determinants of blood pressure. Therefore, we would want to be sure that the distribution of ages in our sample closely matches that in our target population. Let’s assume that the mean age for the sample was 45. If we know that the mean age for the target population that we sampled from is actually 30 are our inferential results likely to be robustly generalisable to this target population? Probably not. For example, it’s probably likely that our sample will overestimtate the mean systolic blood pressure in the target population because our sample is quite a bit younger on average. Put another way, we should question whether our inferential results that we hope tell us about our target population will accurately reflect what we would have found if we had been able to sample all individuals in the target population. Similarly, if we want to think about applying our results to other target populations (transportability) then we can also look at their age distributions.\nIn practice, we would of course want to think about a range of key characteristics that are important in relation to our inferential results of interest, not just age, and so we typically calculate descriptive statistics for many characteristics we assume are important in this way. For human-focused studies this will typically at least include the main socio-demographic characteristics, such as sex, age, education level etc, as well as other important characteristics as relate to the outcomes of interest.\nIf you are still unclear on the difference between descriptive statistics and inferential statistics then we strongly recommend reading the below concise summary. Otherwise you risk misunderstanding the rationale for the approaches used in this session and the following session, because the terminology can be quite confusing!\n\nDescriptive vs inferential statistics\n\n\nRead/hide\n\nIt’s important to be very clear on the distinction between descriptive and inferential statistics.\nDescriptive statistics\nIn summary, descriptive statistics, which are sometimes also called sample statistics or summary statistics, summarise statistical properties of individual variables (via “univariate” analyses) or summarise associations between variables (typically via “bivariate” analyses) as they exist in your sample. For example, common univariate statistics are means, which summarise the typical value of a numerical variable, such the the mean systolic blood pressure in mmHg in the sample, and proportions/percentages, which summarise the frequency of occurrence of some event or condition, represented as one level of a binary/categorical variable, such as the proportion/percentage of smokers in the sample (as opposed to non-smokers).\nAssuming you have no sources of bias in your study these descriptive statistics will reflect the truth about your sample. For example, if you have no bias then the true mean systolic blood pressure in mmHg in your sample will be the sample mean of all the systolic blood pressure values.\nInferential statistics\nHowever, on their own these descriptive statistics do not allow you to make robust generalisations about the same statistical properties of individual variables or associations between variables in your target population. Remember, when we are interested in the statistical properties of individual variables or associations between variables in a target population we call the statistical quantities that reflect these properties “population parameters”, and we assume that they are fixed for the population and time point of interest. In theory, if we could take a census of the whole target population and measure our outcomes and associations of interest without error then we could measure our population parameters without error. However, almost always our target population is far too large to do this and we need to take a sample, ideally using robust, representative, probability sampling methods, as we have seen. The equivalent sample statistic is then our best “point estimate” of the population parameter of interest, but as the name implies it is an estimate with an unquantified amount of error.\nIt is easy to see why this is the case. Let’s assume we want to infer the mean systolic blood pressure in mmHg for some target population that contains 100,000 individuals. Let’s assume we take a simple random sample of just 2 individuals and measure, without error, their systolic blood pressure in mmHg. The mean of these 2 systolic blood pressures values will be the true mean systolic blood pressure for our microscopically small sample of 2! However, does anyone believe that a mean of just two individuals’ systolic blood pressures, however representative they are, is likely to accurately reflect the true mean systolic blood pressure for the overall target population? Of course by chance it might be really accurate, but in general we’re very likely to get a sample that produces an estimated mean that is not reflective of the true population-level mean. And if we instead took another simple random sample of 2 other individuals and computed a new mean systolic blood pressure it would be very likely to be different from the first mean. So if each sample mean would vary, and we can usually only collect one sample, how can we tell how accurate our sample mean is?\nThis is the problem of sampling error and sample size. Each sample would likely produce a different estimate of our fixed population parameter, and depending on the sample size the estimate would be likely to vary more/less between each repeated sample.\nTherefore, to let us infer the likely value of our population parameter we need to combine our sample statistic (i.e. our best point estimate) with some inferential measure. As we will discuss in the relevant lectures, this can be done most effectively by calculating the corresponding confidence intervals for the sample statistic, or a different approach involving hypothesis testing would be to compute an associated p-value. Note: this broad inferential approach is not just for when we are infering the statistical properties of individual variables in a target population, but also for when we are infering associations in a target population. We combine the relevant sample statistic with a suitable inferential measure.\nDescriptive statistics and descriptive studies/research\nThe terminology around descriptive statistics and descriptive studies can be a source of confusion. The key thing to remember is that descriptive statistics describe samples only, and are used in all studies to initially explore our data, plan our analyses, and describe the key characteristics of the sample so we can judge how representative our sample is compared to the target population in terms of these key characteristics. Whereas a descriptive study, or a study where one or more quantitative research questions are about description, is almost always about the goal of describing characteristics/associations in a target population via sample and using inferential statistics (even if that target population is not clearly specified). You can certianly find studies that have only described a sample alone using sample statistics and no inferential measures, but in my experience that always seems to be because the authors have misunderstood statistical inference and don’t seem to understand what they are doing or how limited their results are.\nSo if studies say they are aiming to describe characteristics or associations in a given target population, and they have taken a sample from that target population to do this, then if they know what they are doing then they mean that they are going to use inferential statistics (typically confidence intervals around point estimates) to try and infer the likely (but ultimately unknown) values for those characteristics/associations in the target population.",
    "crumbs": [
      "3. Data exploration & sample description"
    ]
  },
  {
    "objectID": "sample-description.html#scenario",
    "href": "sample-description.html#scenario",
    "title": "Exploring a dataset and describing your sample’s characteristics",
    "section": "Scenario",
    "text": "Scenario\nYou and your colleagues have been tasked by the Kapilvastu district authorities in Nepal to help them understand the problem of high blood pressure and hypertension, and the associations between socio-demographic and health related characteristics and blood pressure level/hypertension. You have carried out a cross-sectional survey to address these aims, and collected data on systolic blood pressure, common socio-demographic characteristics, and some additional health-related characteristics. So far, you have cleaned and prepared the data. As per your statistical analysis plan you now need to compute some relevant descriptive statistics to describe the key characteristics of your sample.",
    "crumbs": [
      "3. Data exploration & sample description"
    ]
  },
  {
    "objectID": "sample-description.html#exercise-1-create-a-table-of-descriptive-statistics",
    "href": "sample-description.html#exercise-1-create-a-table-of-descriptive-statistics",
    "title": "Exploring a dataset and describing your sample’s characteristics",
    "section": "Exercise 1: create a table of descriptive statistics",
    "text": "Exercise 1: create a table of descriptive statistics\n\nAim: create a table of descriptive statistics to describe the key socio-demographic and health-related characteristics of the sample, which also describes the amount of missing data for each variable.\n\n\nFirst, load the “SBP data final.sav” SPSS dataset. This has all the errors removed from the “SBP Excel data.xlsx” dataset we looked at last session, and all the SPSS-specific variable properties, like variable labels and value labels etc, have been updated. We will be describing the key socio-demographic and heath-related characteristics of the individuals in the dataset, who are our sample.\nNext, in the “Exercises” folder also open the “Exercises.docx” Word document and scroll down to Descriptive statistics table. Your goal is to complete the empty table by calculating the values of the appropriate descriptive statistics for each of the variables in the table (i.e. the key characteristics of the sample), along with the amount of missing data for each variable, using the instructions below.\nAs per the footnote to the table, for each numeric characteristic/variable we will compute and present its median and then in brackets its interquartile range, e.g. if the median value was 50 and the interquartile range was 10 we would include it in the table like this: 50 (10). As for each categorical characteristic/variable we will compute the percentage and then in bracket frequency count for each category level, e.g. if the percentage of observations for a category level was 10% and the corresponding frequency count was 25 we would include it in the table like this: 10% (25). And we will also present missing data in the same way: as the percentage and in brackets the frequency count.\n\nNote: the footnotes to the table are typical of such tables in papers and allow us to just present a single column of descriptive statistics. However, you could of course present the values in other ways, as long as it is clear what the statistics are. Once you’ve completed your table you can compare it to the completed one at the end of this section to see how you did.\nBelow is an explanation of the different types of descriptive statistics that are appropriate to use to describe numerical or categorical variables. Use this information to plan what types of descriptive statistics to calculate for each of the characteristics in the table, based on the type of variable that characteristic is measured by. In the dataset the variables corresponding to the characteristics in the table should be self-evident, but to avoid confusion they are (characteristic then variable name):\n\nSystolic blood pressure = sbp\nHypertension = htn\nAge = age\nSex = sex\nSocio-economic status = ses\nSalt = salt\nBMI = bmi\nUsed an ACE inhibitor over the last 3 months = ace\n\n\nNumerical variable descriptive statistics\n\nOften you will see the mean and standard deviation presented as descriptive statistics for numerical variables, but I prefer to use the median and interquartile range, so that is what we will calculate here for the numerical variables. My reason is that if the data are skewed the median is a better measure of the central tendency. However, when a variable is not skewed or not heavily skewed the mean and the median will be (often very) similar.\n\n\n\nCategorical variable descriptive statistics\n\nSimply calculate the frequency (count) and the percentage (or proportion, but percentages are typically used) for each category level. E.g. for the variable sex calculate the frequency and the corresponding percentage of individuals who are male and female. This would similarly apply to a binary variable.\n\nPresent values just to 1 decimal place. We have completed the first two variables for illustration.\n\n\nCalculating univariate descriptive statistics in SPSS for numerical and categorical variables\nVideo instructions: calculate common descriptive statistics\nWritten instructions: calculate common descriptive statistics\n\n\nRead/hide\n\nCalculate common descriptive statistics for categorical variables\n\nTo calculate descriptive statistics for the categorical variables we just need to produce frequency tables for each variable and extract (i.e. copy) the counts for each category level and the corresponding percentages. Simple! From the main menu go to: Analyze &gt; Descriptive Statistics &gt; Frequencies. Then in the Frequencies tool window add all categorical variables to the Variable(s): box. You can either drag each variable across one-by-one by clicking and holding down the left mouse button, or you can click on each one and then click the button in between the two boxes to move it over (or to move it back), or you can select multiple variables and move them at the same time in the same way by first holding down ctrl and then clicking on as many variables as you want to before moving them. Note: this is the typical behaviour of these types of variable boxes in SPSS and you will spend quite a lot of time moving variables so it’s good to try the different methods.\nOnce you have moved all the categorical variables in the table over to the Variable(s): box just ensure the Display frequency tables box at the bottom of the tool is ticked and then click OK. After a moment in the output window you should see the tables appear (one for each variable).\nYou can now copy the relevant values from the Valid Percent and Frequency columns into your Table 1 as % (n). The Valid Percent column gives the percentage for each category level after excluding any missing values, while the Percent column gives the percentage for each category level while treating any missing values as their own category level. There is no right or wrong approach but it’s more common to exclude missing data first and separately compute the % of missing values, as we do below.\n\nCalculate common descriptive statistics for numerical variables\n\nNow for numerical variables. Here we will be computing medians and interquartile ranges, but it’s useful to see how to check whether a variable is approximately normally distributed or skewed. There are statistical tests that can be used to formally test whether a variable’s distribution differs “significantly” from a normal distribution, but as they are based on p-value thresholds the result depends strongly on the sample size, and with a big enough sample size you are essentially guaranteed that a variable will fail the test and be classed “non-normal”. However, many statistical tests that rely on normality are quite robust to modest (or sometimes worse) departures from normality, i.e. they still work well with slightly skewed variables and have much more power than equivalent non-parametric tests. Therefore, it’s better to judge normality by eye using histograms even if this seems unscientific or less robust than using a statistical test.\nSo let’s see how to create histograms for numerical variables to check their distributions. In the main menu go: Graphs &gt; Histogram. Then add the sbp variable into the Variable: box and tick the Display normal curve box just below the Variable: box. This adds a curved line to the histogram which shows what the distribution of values would look like assuming the data were normally distributed. If the histogram follows this fairly well we can assume approximate normality. Now just click OK. Do this for each numerical variable in turn and examine the graphs. What do you see? sbp, age and bmi all appear to approximate a normal distribution well, but salt displays some slight right skew.\nHere won’t worry about the skew or lack of and just compute and present medians and interquartile ranges. However, if you wanted to instead present means and standard deviations (or ranges), the following approach computes them all. From the top menu go: Analyze &gt; Descriptive Statistics &gt; Explore. Then in the Explore tool window add each numerical variable to the Dependent List: box. Then click the Statistics button and ensure the Descriptives box is ticked and click Continue. Then back at the main Explore tool window in the Display options at the bottom ensure the Statistics option is selected and then click OK. You’ll then get several tables produced in the output window, one for each variable, with lots of descriptive statistics listed. Simply copy the relevant statistics (median and IQR) from them and add them to your Table 1 as median (IQR).\nRepeat the above processes for each characteristic, given the appropriate descriptive statistic you are calculating, and complete the table.\n\nMissing data\n\nTo calculate the amount of missing data for each variable go Analyze &gt; Missing Value Analysis. Then in the Missing Value Analysis tool window that appears add all variables into the Quantitative Variables: box and click OK. In the table that appears each row corresponds to one of the variables and then in the fifth column you can find the percentage of missing data and in the fourth column you can find the count of missing observations for the variable of interest. Simply copy these statistics into your Table 1’s “Missing value” column for each corresponding variable as % (n).\n\n\n\n\nExample completed table for reference\n\n\nRead/hide\n\n\n\n\nCompleted Table 1 for SBP data\n\n\nIf you have any glaring errors or strange differences try and work out why, and if you can’t then ask for help!",
    "crumbs": [
      "3. Data exploration & sample description"
    ]
  },
  {
    "objectID": "preparing-a-dataset.html",
    "href": "preparing-a-dataset.html",
    "title": "Preparing a dataset for analysis",
    "section": "",
    "text": "In this practical we will practice some basic steps around processing, cleaning and exploring a dataset, which should be carried out before doing any analyses (even if you are working with data collected from another study that is supposed to be “ready for analysis”).",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.2. Preparing a dataset"
    ]
  },
  {
    "objectID": "preparing-a-dataset.html#scenario",
    "href": "preparing-a-dataset.html#scenario",
    "title": "Preparing a dataset for analysis",
    "section": "Scenario",
    "text": "Scenario\nYou and your colleagues have been tasked by the Kapilvastu district authorities in Nepal to help them understand the problem of high blood pressure and hypertension, and the associations between socio-demographic and health related characteristics and blood pressure level/hypertension. You have carried out a cross-sectional survey to address these aims, and collected data on systolic blood pressure, common socio-demographic characteristics, and some additional health-related characteristics. As per your statistical analysis plan you first need to clean and prepare the data for analysis.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.2. Preparing a dataset"
    ]
  },
  {
    "objectID": "preparing-a-dataset.html#exercise-1-loading-excel-data",
    "href": "preparing-a-dataset.html#exercise-1-loading-excel-data",
    "title": "Preparing a dataset for analysis",
    "section": "Exercise 1: loading Excel data",
    "text": "Exercise 1: loading Excel data\nSPSS can load all the main types of data you are likely to need to load, including CSV files, Excel spreadsheets, and data files from various other statistical programs like Stata and SAS. During these practical sessions we will use datasets saved in SPSS’s own format (.sav), which preserves all the useful information on things like variable names and labels, categorical variable coding labels etc that we will see explained below. However, if you use SPSS in the future it is likely you will have to load data in an Excel format at some point. Therefore, the first thing we’ll learn to do is to load an Excel spreadsheet.\nVideo instructions: load Excel data into SPSS\nWritten instructions: load Excel data into SPSS\n\n\nRead/hide\n\n\nGo back to the main window in SPSS and in the main menu click File &gt; Open &gt; Data. Then in the Open Data tool window that opens navigate to the folder where you unzipped the MSc & MPH statistics computer sessions practical files and go into the “Datasets” folder in the same way you would locate a folder in Windows Explorer. Then to allow you to see Excel files look for the Files of type drop-down menu beneath the area in the Open Data tool where the folders and files are displayed, and select the All Files option here. Then click on the “SBP Excel data.xlsx” dataset and then click Open (or double click on the file). A tool window will pop-up called Opening Excel Data Source. Ensure the Read variable names from the first row of data box is ticked and the Worksheet drop down menu is on “SBP data [A1:I557]” and then click OK. You will now see the raw data displayed in the Data View with the variable names in the top row and the data values in the cells. You will see there are nine variables.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.2. Preparing a dataset"
    ]
  },
  {
    "objectID": "preparing-a-dataset.html#exercise-2-set-variable-properties-code-categorical-variables-and-check-categorical-variables-for-obvious-errors",
    "href": "preparing-a-dataset.html#exercise-2-set-variable-properties-code-categorical-variables-and-check-categorical-variables-for-obvious-errors",
    "title": "Preparing a dataset for analysis",
    "section": "Exercise 2: set variable properties, code categorical variables, and check categorical variables for obvious errors",
    "text": "Exercise 2: set variable properties, code categorical variables, and check categorical variables for obvious errors\nNow using our “raw” Excel dataset (that just contains the variable names and their values) let’s see how to set key variable properties, check that all categorical variables are correctly coded and free from obvious errors, and check that all numerical variables are free from obvious errors. For this we will need to understand the key details of our variables. If you are working with data that is not your own then assuming you are using a high quality, well documented dataset, this information will often be presented in the form of a “codebook”. We will present the codebook for this dataset in the form of a table below:\n\n\n\n\n*SBP data* dataset variable details\n\n\nVariable name\nVariable type\nVariable description (units/coding of levels)\n\n\n\n\nid\nNumerical:discrete\nObservation ID\n\n\nsbp\nNumerical: continuous\nSystolic blood pressure (mmHg)\n\n\nhtn\nCategorical: nominal (binary)\nHypertension (SBP &gt;= 140 mmHg, 0 = no/1 = yes)\n\n\nage\nNumerical: discrete\nAge (years)\n\n\nsex\nCategorical: nominal (binary)\nSex (1 = male/2 = female)\n\n\nses\nCategorical: nominal\nSocio-economic status (1 = low/2 = medium/3 = high)\n\n\nsalt\nNumerical: discrete\nSalt consumption (g/day)\n\n\nbmi\nNumerical: continuous\nBMI (kg/m2)\n\n\nace\nCategorical: nominal (binary)\nACE inhibitor usage for at least 3 months (1 = yes/2 = no)\n\n\n\n\n\n\nVideo instructions: set variable properties, code categorical variables, and check categorical variables for obvious errors\nWritten instructions: set variable properties, code categorical variables, and check categorical variables for obvious errors\n\n\nRead/hide\n\n\nFirst we’ll make sure our variables are correctly named, labelled and set to the correct data type, and that all categorical variables are correctly coded. Note: for some of these exercises we will just edit a few of the variables for practice, but leave the rest alone. This is okay because when we move on we will be using a fully updated version of the dataset so we don’t need to update more than enough to understand the processes. To make these updates we need to know what variables are in our dataset, what they each measure, and what their units/category level coding should be. This information would either be available to you if you were the one who collected the data, or it would be made available in a “codebook”, which lists every variable in a dataset alone with basic information such as the variable’s name, units/category levels, often the amont of missing data for the variable (if any), and sometimes also key descriptive statistics for the variable. Refer to the codebook table above for the key details of all variables in this SBP Excel data dataset when updating the variables’ details.\n\nVariable names\n\nVariable names are short terms used to quickly identify each variable. Variable names should be short, clear, understandable and unambiguous. They cannot have spaces in, but you can join words to make a short phrase with underscores (e.g. “age_at_death”). On the Variable View tab look at the Name column. Here you see the variables’ names. We’ll leave these as they are, but just so you can see how to edit them click on the variable name “id”, delete it and re-enter it.\n\n\n\nVariable labels\n\nVariable labels are fuller (but still concise) descriptions of each variable, and can be written with spaces in. Good practice is to include a description of what the variable is measuring, plus the units if it’s a numerical variable or the category levels if it’s a categorical variable. Next in the Variable View look along to the Label column where the variable labels can be stored (there are none in this dataset yet as Excel cannot store such information). Let’s add some for practice:\nFor the variable sbp click on the variable label cell and enter “Systolic blood pressure (mmHg)”, i.e. the brief description of the variable with the units included take from the codebook table.\nNow click on the variable label for ses and enter the description provided in the codebook table (you can just copy and paste it from the table).\nFeel free to update the variable labels for some other variables if you wish, but once you’ve know how to do it you can move on.\n\n\n\nVariable types\n\nIt can be very important that variable types are correctly set in SPSS (and all stats packages), because it can affect the way analyses and other things like graphs may work, with incorrectly defined variable types resulting in errors. To set the variable type go to the Variable View and click on the relevant cell under Measure (it will either say “Scale” or “Nominal”) and select the correct type from the drop-down menu. SPSS will set the Measure based on the nature of the variable: for any variables with any non-numeric (i.e. letters or special characters) values the variable Type will be String and the variable Measure will be Nominal, and for any variables with only numeric (discrete or continuous) values the variable Type will be Numeric and the variable Measure will be Scale. However, these automatic choices may not be correct for the dataset!\nFor our dataset let’s start with sbp. What do you notice about its Type and Measure? The Type is String and the Measure is Nominal. What about if you click on the Measure cell? Only nominal and ordinal options are available! This means that at least one value in the variable is non-numeric, i.e. a letter or special character. Unfortunately, there are no easy ways to check which values are non-numeric (there are ways using more complicated functions, but we don’t have time for that here), but if you create a frequency table (see later for how to do this) and scroll through all the values you may be able to spot the problem values. However, to save time here the issue is observation number 93, which has a value of 1o3, i.e. with a letter “o” instead of a zero.\nTo update the erroneous value go to the Data View and either scroll down to the value (observation 93) and update it, or click on the sbp column variable name/heading to select the whole variable, then press ctrl and f to open the Find and Replace - Data View tool (below the Find tab it should say “Column: sbp”), then click on the Replace tab and then in the Find text box enter “1o3” (without the quotes) and then in the Replace text box enter “103” (without the quotes) and press the Replace button at the bottom to correct the value.\nOnce the error has been updated go back to the Variable View window and update the variable Type for sbp to Numeric, because only Numeric type variables can be Scale variables (sorry for the confusing terminology, I don’t like SPSS’s choices!). Click on the relevant cell for sbp under Type and click the little box with three dots that appears to the right. Then select Numeric and then OK. As there are no more values with strings or non-numeric characters in you should now be able to update the variable’s Measure to Scale!\nYou can go through all other variables and ensure the variable Type and Measure characteristics are set correctly if you wish, but once you’re happy how this is done feel free to move on.\n\n\n\nValue labels: coding categorical variables\n\nCategorical variables must use some type of coding scheme to be understandable. This just refers to how categorical variables’ levels are stored (e.g. our sex variable has two levels: male and female). There are two approaches:\n\n\nString/character coding: each categorical level is referred to via a string (i.e. a set of letters, numbers or special characters). E.g. sex might take either the value “male” or “female”.\nValue/numerical coding: each categorical level is referred to via a value, i.e. a number, with the number linked to a “value label” that then describes what that level represents. E.g. sex might take either the value 1 or 2, where 1 is linked to the value label “male” and 2 is linked to the value label “female”.\n\n\nHere you can see why it can be critical to tell SPSS the type of each variable. If you record sex as male = 1 and female = 2, then your variable’s values will be 1s and 2s. If you don’t tell SPSS that this is actually a categorical variable then it will automatically detect the numerical values and treat it as a discrete/continuous variable, which can lead to various mistakes when running analyses.\n\n\nYou can use either approach in SPSS and you shouldn’t have generally have any problems, as long as you include value labels. String coding can be easier and safer as you don’t have to keep refering back to what each numerical code means, and you’re less likely to accidentally type/select the wrong category level, e.g. “male” when you meant “female”, compared to accidentally typing/selecting the wrong numerical code. Although if you set up value labels and make sure they are displayed in SPSS generally this is not an issue. However, as value coding is often used and requires more understanding of SPSS to add/edit we will follow this approach so you can get the practice. The datasets we will use after this session though will use string coding for the categorical variables.\nSo let’s set-up value labels for our categorical variables sex and ses. To do this we must ensure that they are correctly coded with numbers and that suitable value labels are attached those numbers. In “real life” we would repeat this process for all other categorical variables. Before making any changes though we must first check each categorical variable to find out what levels exist and if there are any errors – you might be surprised. The easiest way to do this is to generate a frequency table: a table listing the number of cases the percentage of cases with each level.\nFrom the main menu go: Analyze &gt; Descriptive Statistics &gt; Frequencies. Note that SPSS tries to work out the type of variable based on the values present, e.g. if there are any non-numerical characters it will assume a categorical variable. In SPSS categorical variables are represented by three coloured circles, and numerical variables by a small ruler symbol. However, these assumptions will not necessarily be correct. Let’s look at sex and ses. Either double click on the sex and ses variables in turn or click once on each while holding the shift button and then click the right pointing arrow to add them. Then ensure the Display frequency tables button is ticked and click OK. The results will appear in a new output window, which is the other main window in SPSS where all results will be displayed along with any error messages.\nNow look at the frequency table to see what levels are present for sex and ses and how they are coded. Note that sex has been entered as a string variable (with levels recorded as words) and ses has been entered as a numeric variable (with levels recorded as numbers). Now you can see why frequency tables are so useful for data cleaning. For sex you can see that there are five levels, and it’s clear that three are errors (both the fact there are clear spelling mistakes/odd values and the fact they only occur once indicate they are mistakes). Specifically the levels “1”, “FM” and “mal” need correcting.\nWe therefore need to first clean/correct the error values, and then convert all values to numerical values and then add the labels. Look at the codebook above and the “Description (units/level coding)” column for the correct coding to apply to each level. To correct the mistakes go back to the Data View and click the top of the sex column (click on the word sex) to highlight the whole column. Then hold ctrl and f to open the Find and Replace - Data View tool. Then in the Find text box enter “1” (without quotes) and in the Replace with text box enter “male” (without quotes) and then click Replace All. Repeat this find and replace process for all other levels that are errors with the obvious correct word. Then use the find and replace process to change all “male” values to the number 1, and all “female” values to the number 2. We can now add value labels to these numerical codes. Be sure to add labels to the correct numerical values or they won’t display.\nNow go back to the main window Variable View and click on the Values cell for sex where it currently says “None”. You’ll see a small box appear to the right of the cell with three dots in it – click on this to open the Value Labels tool. Then click the Value text entry box and type 1. Then click the Label text entry box and type “male”. Then click Add. Then repeat for Value = 2 and Label = “Female” (make sure you remember to click Add). Then once both value labels are added click OK.\nNext let’s correct (if necessary) and code ses. Go back and look at the frequency table for ses. We can see there are four levels with one obviously incorrectly coded level of “11” only observed once. Clearly this should be coded as “1”. Use the find and replace process detailed above to replace level “11” with “1”, and then attach the correct value labels to the levels 1, 2 and 3 based on the information provided in the codebook table (1 = low, 2 = medium and 3 = high).\nNote: variable labels and value labels are not stored in Excel files, so to retain them you must first save your dataset in SPSS “.sav” format. This can be done by going from the main menu to: File &gt; Save As, and then selecting a folder and file name. The .sav format should be the default selected, but ensure that you save your updated dataset in this format.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.2. Preparing a dataset"
    ]
  },
  {
    "objectID": "preparing-a-dataset.html#exercise-3-check-numerical-variables-for-obvious-errors",
    "href": "preparing-a-dataset.html#exercise-3-check-numerical-variables-for-obvious-errors",
    "title": "Preparing a dataset for analysis",
    "section": "Exercise 3: check numerical variables for obvious errors",
    "text": "Exercise 3: check numerical variables for obvious errors\nVideo instructions: check numerical variables for obvious errors\nWritten instructions: check numerical variables for obvious errors\n\n\nRead/hide\n\n\n\nAll we can really hope to find are clearly erroneous values, such as values that are outside the plausible range of values for the variable based on what it is measuring or what we restricted it to via study design, but more modest errors won’t be detectable.\nIf you have a numerical variable with relatively few distinct/unique values, such as age, then probably the easiest check is another frequency table. Produce a frequency table for age and what do you notice about the extreme values (i.e. the lower and upper ends)? One person has an age of 4. This is clearly an error, as the study only recruited participants aged 18+. What to do? There are three choices: 1) leave it as it is, 2) update it or 3) delete it. Option 1 is not a good idea because we know it must be an error. Option 2 would be ideal, but you can’t always do this. In this case an age of 4 could actually be an age of 24, 34, 40, 41 etc, so we would have to be able to go back and contact the participant to check their age if we wanted to update this. Where this is not possible you could conduct a sensitivity analysis by running your analysis with this age value as 40 and then again with the value updated to 49. That way you see what happens when you assume it was actually either end of the possible value range of the likely true value. Or you may take option is 3 and delete the value (but this should be reported in your methods). However, here we’ll assume we were able to check and the correct age should have been 40. To update this go to the Data View, right click on the age variable name and select Sort Ascending. The data should now be ordered with the participant with age 4 at the top (or you could use the find and replace process detailed above). Update their age to 40.\nWhen numeric variables have lots of different unique values, such as when they include decimals, using a frequency table becomes less easy. A better option is to produce a dotplot (also called a dotchart or Cleveland dotplot/dotchart), which is like a histogram but each value is displayed as a dot, with dots from observations of values in a similar range (i.e. in the same “bin”) being stacked like a histogram. As BMI has very few (maybe no) repeated values a frequency table will be huge. Let’s produce a dotplot instead. From the main menu go: Graphs &gt; Scatter/Dot. Then click Simple Dot &gt; Define. Then in the Simple Dot graph tool add the variable bmi into the X-Axis Variable box. Then click OK. Looking at the graph you’ll see a huge stack of values of similar values, and then far out to the right on the x-axis you should clearly see an anomalous value (a circle) that must be an error because it’s so high. Double click on the graph which will open it in a new window where you can edit the graph. Now click on the error value, which should highlight all circles in yellow, and then click on it again and only the error circle should be highlighted. Then right click on the circle and choose Go to Case. This will take you to the relevant observation in the Data View where you can update it. Again we have three choices, and let’s assume we could go back and get the correct value either from the participant or an earlier dataset (e.g. if weight and height were measured separately), and it was actually 28.175.\nNow re-run the dotplot for BMI and what do you see? Again there is a value to the right of the distribution that is a far way to the right (higher) than any other BMI value. This may be considered an “outlier”. Outliers are extreme values that are far greater/smaller than any other values in a numerical variable. There are no clear thresholds beyond which a value is considered an outlier, but as you can see probably the easiest way to spot possible outliers is via a dotplot. Once you’ve identified any possible outliers such as this very high BMI value you should check to see if they are obvious errors and correct them if possible. However, if they appear to be genuine values the recommended approach is to conduct any analyses that involve the relevant variable(s) with and without the outliers present, i.e. repeat your analysis but delete the observations (e.g. individuals) who have the outliers and see what changes. Then simply clearly report the results of both of these analyses and any implications for the interpretation of your results. This is known as a sensitivity analysis. Broadly speaking, if removing outliers makes an important difference for your results then your results are probably not very robust to start with. Where you have plenty of good quality data removing the odd outlier from an analysis will rarely make much difference unless it’s a relatively hugely larger/smaller value. As you’ll have seen from the dotplot there are no outliers in this dataset.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.2. Preparing a dataset"
    ]
  },
  {
    "objectID": "preparing-a-dataset.html#a-final-note-on-data-cleaning",
    "href": "preparing-a-dataset.html#a-final-note-on-data-cleaning",
    "title": "Preparing a dataset for analysis",
    "section": "A final note on data cleaning",
    "text": "A final note on data cleaning\nWe will end our data cleaning there having seen how to clean and prepare all variables as best as we can in isolation. However, with a real dataset these steps would just be the basics, and we would want to look further to check for errors and anomalous values by also looking at associations between variables. For example, if we looked at a scatter plot of sbp vs age we may see a suspiciously high value for sbp for a very young participant, i.e. one that would be very unlikely in such a young individual. We could then check this with any prior data or the participant if we were able to.\nNote: generally speaking you should not delete any observations (e.g. patients) from any variables when preparing your data. When it comes to analysing your data the presence of missing values in some variables may be a problem, and in the most simplistic approach you may have to conduct “complete case” analyses by removing any observations with missing values for variables in the analyses. However, this is not without implications for possible bias (https://stefvanbuuren.name/fimd/ch-introduction.html), and you should always compute and present the frequency of missing data if present.\nNote: if you are concerned that one or more observations for one or more variables are errors but you cannot confirm this then a good approach is to run your analyses with and without those data points, and then in your results explain if this made any difference to your conclusions and discuss the implications accordingly. This is known as a sensitivity analysis.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.2. Preparing a dataset"
    ]
  },
  {
    "objectID": "paired-t-test.html",
    "href": "paired-t-test.html",
    "title": "The paired t-test",
    "section": "",
    "text": "In this practical we’ll practice using the paired t-test to analyse an outcome where the observations are paired.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "paired-t-test.html#scenario",
    "href": "paired-t-test.html#scenario",
    "title": "The paired t-test",
    "section": "Scenario",
    "text": "Scenario\nThis contains real data from a cluster randomised controlled trial done by Chinese partners (although they are based in Canada). The research question was whether a multi-component complex intervention could reduce inappropriate prescribing of antibiotics by primary care providers to children (aged 2-14) in Chinese primary care facilities. However, this dataset just contains data from the intervention arm before and after the intervention. There are two variables: apr_base and apr_end. apr_base contains the antibiotic prescription rates (actually proportions not true rates) for primary care facilities before the intervention was applied in the intervention arm, and the apr_end contains the antibiotic prescription rates for the same primary care facilities after the intervention had run for six months in the intervention arm. Hence, each row contains data from the same primary care facility, and therefore the outcomes are correlated: they are pre and post measurements from the same facilities.\nThe antibiotic prescription rates are more specifically the facility-level proportion of prescriptions issued to children (aged 2-14) for upper respiratory tract infections that contain one or more antibiotics. Therefore, broadly speaking these will be inappropriate and a lower rate is more desirable. Therefore, we’ll use a paired t-test to explore the association between the antibiotic prescription rate and the intervention period (pre or post intervention) just within the intervention arm. This is equivalent to if we had actually run a uncontrolled before-after study rather than an RCT. Of course, given we had a control arm we would not (and did not) ignore it in reality when we analysed the study, which you can read if you are interested: https://www.thelancet.com/journals/langlo/article/PIIS2214-109X(17)30383-2/fulltext",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "paired-t-test.html#exercise-1-use-the-paired-t-test-to-compare-the-association-between-the-facility-level-proportion-of-appropriately-prescribed-antibiotics-before-and-after-an-intervention-to-reduce-inappropriate-antibiotic-prescribing-was-applied",
    "href": "paired-t-test.html#exercise-1-use-the-paired-t-test-to-compare-the-association-between-the-facility-level-proportion-of-appropriately-prescribed-antibiotics-before-and-after-an-intervention-to-reduce-inappropriate-antibiotic-prescribing-was-applied",
    "title": "The paired t-test",
    "section": "Exercise 1: use the paired t-test to compare the association between the facility-level proportion of appropriately prescribed antibiotics before and after an intervention to reduce inappropriate antibiotic prescribing was applied",
    "text": "Exercise 1: use the paired t-test to compare the association between the facility-level proportion of appropriately prescribed antibiotics before and after an intervention to reduce inappropriate antibiotic prescribing was applied\n\nLoad the “AB paired.sav” SPSS dataset.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "paired-t-test.html#step-1-check-the-assumptions-of-the-paired-t-test",
    "href": "paired-t-test.html#step-1-check-the-assumptions-of-the-paired-t-test",
    "title": "The paired t-test",
    "section": "Step 1: check the assumptions of the paired t-test",
    "text": "Step 1: check the assumptions of the paired t-test\n\n1. Continuous outcome\nTechnically a paired t-test assumes a continuous outcome variable, but as long as the following two assumptions are satisfied it’s fine to use a discrete outcome with a paired t-test.\n\n\n2. Paired outcome data/observations, but independent observations between pairs\nSee above for a discussion of how to understand if your data are suitable for the paired t-test. However, the paired t-test also assumes that pairs of observations are independent from each other. For example, if you recorded blood pressure measurements from individuals before and after an intervention the observations would be paired or correlated within individuals, but if individuals were also clustered within family grouping then there would also be correlations between pairs of individuals (as family members would likely have correlated before and after values), and the data would violate this assumption of the paired t-test.\n\n\n3. Approximately normally distributed differences between paired observations\nCompute the difference between each pair of observations and plot them on a histogram to check for approximate normality. In SPSS you can use the Compute Variable tool that we used earlier in the data preparation section to do this easily. You just need to enter the command: “var1 - var2” (without quotes) or vice versa, where var1 and var2 are your paired outcome variables. Or you can also do it in Excel by creating a new column from the difference between your paired outcomes. You should know how to plot a histogram now, but refer back to the “Step 1: check the assumptions of the independent t-test” section earlier for a reminder if needed.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "paired-t-test.html#step-2-run-the-paired-t-test",
    "href": "paired-t-test.html#step-2-run-the-paired-t-test",
    "title": "The paired t-test",
    "section": "Step 2: run the paired t-test",
    "text": "Step 2: run the paired t-test\n\nFrom the main menu go: Analyse &gt; Compare Means &gt; Paired Samples T Test.\nAdd apr_base as Variable1 and apr_end as Variable2 then click OK.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "paired-t-test.html#step-3-understand-the-results-tables-and-extract-the-key-results",
    "href": "paired-t-test.html#step-3-understand-the-results-tables-and-extract-the-key-results",
    "title": "The paired t-test",
    "section": "Step 3: understand the results tables and extract the key results",
    "text": "Step 3: understand the results tables and extract the key results\n\nThe results are largely the same as for the independent t-test, so refer back to the “Step 4: understand the results tables and extract the key results” section in the “Inferential analysis 1: the independent t-test” section to review what the results in the tables mean if you need reminding. However, a few things are a little different. Your descriptive statistics for each group are in the Paired Samples Statistics table, and you get the (presumably) Pearson correlation between the two groups in the Paired Samples Correlations table. Then your inferential results are in the Paired Samples Test table. There are now no equal variances assumed/not assumed version of the results, and we can see the key results are the Mean (i.e. mean difference between the reference group [entered into Variable1] and the comparison group [entered into Variable2], the associated 95% confidence intervals of this mean difference, and if you want it the two-tailed p-value (Sig. (2-tailed)), which tests the hypothesis that the mean difference = 0. You can ignore any “effect size” table.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "paired-t-test.html#step-4-report-and-interpret-the-results",
    "href": "paired-t-test.html#step-4-report-and-interpret-the-results",
    "title": "The paired t-test",
    "section": "Step 4: report and interpret the results",
    "text": "Step 4: report and interpret the results\nFor a report we would say something like:\n\nMethods: explain that you used a paired t-test and justify why.\n\n\nResults: at baseline the mean facility-level antibiotic prescription rate was 0.82. After the six-month post-intervention follow-up the mean facility-level antibiotic prescription rate was 0.4. There was therefore a mean change in the antibiotic prescription rate from baseline to six-month follow-up of 0.42 (95% CI: 0.3, 0.55).\n\n\nNote: take care interpreting the direction of change. If we compared the six-month follow-up group to the baseline group the change would be -0.4, which could be easily mistaken for a reduction when it’s actually an increase. Just be clear on the mean of each group and what comparison is being made, i.e. which group mean is being subtracted from which.\n\n\nDiscussion: interpret the direction and size of the difference in terms of the implications for practice and policy. Is the difference “statistically significant”, i.e. can we make a clear inference that there even is a difference on average between the groups, and if so is it a small difference, a medium difference, a large difference etc in terms of what is being measured and the implications for practice and policy?",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "paired-t-test.html#additional-exercise-use-the-paired-t-test-to-analyse-whether-the-number-of-mdr-tb-patients-lost-to-follow-up-changed-from-before-to-after-an-intervention-to-reduce-loss-to-follow-up-was-applied",
    "href": "paired-t-test.html#additional-exercise-use-the-paired-t-test-to-analyse-whether-the-number-of-mdr-tb-patients-lost-to-follow-up-changed-from-before-to-after-an-intervention-to-reduce-loss-to-follow-up-was-applied",
    "title": "The paired t-test",
    "section": "Additional exercise: use the paired t-test to analyse whether the number of MDR-TB patients lost to follow-up changed from before to after an intervention to reduce loss to follow-up was applied",
    "text": "Additional exercise: use the paired t-test to analyse whether the number of MDR-TB patients lost to follow-up changed from before to after an intervention to reduce loss to follow-up was applied\nOpen the simulated “MDR-TB LTFU.sav” dataset. It represents data collected from an uncontrolled pre-post (or before-after) study comparing the number of patients being treated for multiple drug resistant Tuberculosis (MDR-TB) who were lost to follow-up during a month before and after an intervention was implemented that aimed to reduce loss to follow-up. The unit of observation is the health facility, and the dataset contains three variables:\n\nfacility_id = generic facility ID.\nltfu = the number of patients lost to follow-up during the pre or post intervention period.\ntreatment_period = the treatment period (pre or post).\n\nNote: pre-post data are often stored like this in “long” format, with the outcome variable containing repeated measures for each unit of observation and a variable indicating which time period those observations come from. Therefore, you will need to rearrange the dataset into two outcome columns (pre and post) where the observations from each separate facility are on the same row (this should be easy enough to do once you look at the data).\n\nThen via the process outlined above use a paired t-test to analyse the association between the treatment period (i.e. in theory the effect of the treatment) and the number of MDR-TB patients lost to follow-up per facility.\nExtract the mean difference and confidence intervals around this estimate.\nIn the “Exercises” folder open the “Exercises.docx” Word document and scroll down to Paired t-test: the association between an intervention and MDR-TB patient loss to follow-up.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: sample size and group outcome means. Also be sure to include sufficient details about the outcome variable and the comparison made, including the type of analysis used, and of course the key inferential results. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\nExample results reporting text\n\n\nRead/hide\n\nUsing a paired t-test I analysed the change in the number of MDR-TB patients lost to follow-up during the month before and after an intervention to reduce loss to follow-up in MDR-TB health facilities. Out of a total sample size of 10 health facilities the mean number of patients lost to follow-up before the intervention was 7.9 and after the intervention was 5.3. This represented a mean difference of 2.6 (95% CI: 1, 4.2) more patients lost to follow-up in the month before the intervention was implemented. Therefore, the intervention was associated with a clear and statistically significant reduction in the number of patients lost to follow-up in the month following its implementation compared to the previous month. However, the key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an uncontrolled pre-post study like this), particularly unmeasured time-varying confounding variables that may have impacted on patient loss to follow-up (such as performance bias [https://catalogofbias.org/biases/performance-bias/]). Therefore, this is likely to represent a biased estimate of the independent association between the intervention and the number of patients lost to follow-up.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.3. Paired t-test"
    ]
  },
  {
    "objectID": "logistic-regression.html",
    "href": "logistic-regression.html",
    "title": "Multiple binary logistic regression",
    "section": "",
    "text": "In this practical we will look at using linear regression to estimate associations between any type of covariate and a continuous outcome.",
    "crumbs": [
      "8. Regresson modelling",
      "8.3. Logistic regression"
    ]
  },
  {
    "objectID": "logistic-regression.html#overview",
    "href": "logistic-regression.html#overview",
    "title": "Multiple binary logistic regression",
    "section": "Overview",
    "text": "Overview\nLike multiple linear regression multiple binary logistic regression allows us to analyse the associations between one or more continuous and/or categorical covariates and an outcome, but unlike multiple linear regression the outcome in multiple binary logistic regression must be binary. Like with linear regression “simple logistic regression” may be a term used to refer to logistic regression with one covariate only, while “multiple linear regression” refers to including more than one covariate, as would be the case for any analysis aiming at causal inference by adjusting for confounding, but there’s no qualitative difference, and we’ll just refer to “logistic regression” from here on.\nAlso, as touched on in the lecture there are different versions or extension of binary logistic regression like multinomial logistic regression for categorical outcomes with any number of category levels, but as these are more advanced we will not be looking at them further. Therefore, because “binary logistic regression” is far more commonly used/seen than any other form of logistic regression we’ll just refer to it as logistic regression from here on, which is common practice (i.e. if you see “logistic regression” in the literature you can assume it’s referring to binary logistic regression).\nYou can actually analyse binary outcomes using linear regression, and you will sometimes see such analyses in the literature (sometimes called a “linear probability model”) and you can often get reasonably useful (i.e. reasonably accurate/unbiased) results. However, generally this is not recommended because of the often substantial disadvantages and problems with this approach. For example, the model predictions may range outside 0-1 (the only values your outcome can take in reality when viewed as a probability of the event occurring), and your inferences are likely to be biased because the assumptions of linear regression cannot be met in such a situation, producing biased confidence intervals and p-values. Although logistic regression is more complex mathematically the basic idea of creating a linear model (where the terms are additive) with numerical or categorical covariates is the same, although the interpretation of the terms differs substantially.",
    "crumbs": [
      "8. Regresson modelling",
      "8.3. Logistic regression"
    ]
  },
  {
    "objectID": "logistic-regression.html#scenario",
    "href": "logistic-regression.html#scenario",
    "title": "Multiple binary logistic regression",
    "section": "Scenario",
    "text": "Scenario\nWe wish to describe important associations between key socio-demographic and relevant health related characteristics and the probability that an individuals has hypertension using the data collected in the SBP final dataset. As these are descriptive research questions we will use repeated logistic regression models where we model the association between each characteristic (covariate) of interest without adjusting for any other covariates. This will produce unadjusted or crude associations that reflect the associations as they appear without any assumption that they may reflect causal associations. One or more of these associations may of course reflect causal associations, but it’s unlikely they will be good (accurate) estimates of causal associations because this is an observational study so adjusting for the inevitable confounding effectively, and without making potentially worse mistakes like introducing collider bias, is a huge challenge that is beyond the scope of this module, but see the materials in the linear regression lecture on Minerva for an introduction to the complex world of causal inference using observational studies/data.",
    "crumbs": [
      "8. Regresson modelling",
      "8.3. Logistic regression"
    ]
  },
  {
    "objectID": "logistic-regression.html#exercise-1-describe-the-population-level-association-between-socio-economic-status-and-the-probability-of-having-hypertension-using-logistic-regression",
    "href": "logistic-regression.html#exercise-1-describe-the-population-level-association-between-socio-economic-status-and-the-probability-of-having-hypertension-using-logistic-regression",
    "title": "Multiple binary logistic regression",
    "section": "Exercise 1: describe the population-level association between socio-economic status and the probability of having hypertension using logistic regression",
    "text": "Exercise 1: describe the population-level association between socio-economic status and the probability of having hypertension using logistic regression\n\nLoad the “SBP data final.sav” dataset.\n\n\nStep 1: explore the data\nWritten instructions: explore the data for a logistic regression\n\n\nRead/hide\n\nThe same basic reasoning applies to our data exploration process as with the linear regression modelling process, so refer back to the “Step 1: explore the data” section in the linear regression practical if you want a reminder of the theory/justification for the following data exploration process.\nIn practical terms though we would examine the distribution of each variable on its own using the same methods discussed in the linear modelling practical, but when examining the associations between the outcome and each covariate we would have to use different graphical approaches.\n\nCategorical covariates\nFor exploring the association between categorical covariates like socio-economic status and binary outcomes we can just use a normal barchart where the Y-axis is the proportion of the outcome in each category level on the X-axis. Let’s see how to do this for socio-economic status:\n\nFrom the main menu go: Graphs &gt; Chart Builder. If an information box appears saying “Before you use this dialogue…” just click OK. Now in the Gallery tab at the middle left click on Bar and then double click on the top leftmost picture of a barchart (Simple Bar) to add it to the plot. Next drag the htn variable from the Variables: box onto the Y-Axis? dotted box on the chart image. Similarly, drag the ses variable onto the X-Axis? box. Look to the right of the tool window at the Element Properties tab in the Statistics box. You should see a drop down menu under Statistic:. This should already read “Mean”, but if not click on it and select mean. This tells SPSS to calculate the mean of the Y-axis variable, htn, for each value of the X-axis variable, which are now repeated when in each bin. As htn is binary and coded 1 and 0 it the mean will be the proportion of 1s or proportion of individuals with hypertension. Now just click OK.\n\nWhat can you see?\nWhat does the barchart of socio-economic status vs proportion of individuals with hypertension show?\nIn a real analysis you would explore all bivariate associations this way first but we’ll move onto quantifying the association via logistic regression now.\n\n\nCell frequencies\nUnlike with linear regression we have one more data exploration process that we must do. When looking at associations for a logistic regression model an important issue to look out for is so-called “sparse data”, which means we have small (or empty) “cell sizes”, which really means few (or no) observations of one of the levels of our binary outcome (e.g. either htn = yes or htn = no) in one or more levels of one or more categorical variables (e.g. if say all socio-economic status = low individuals were classed as htn = yes). This is explained in more detail during the “Additional potential problems” section below, but briefly if there are zero observations of one outcome level within one or more independent categorical variable levels that will cause the model to fail. If there are just a few you may see very large biased coefficients and confidence intervals. How few is few? There’s not set number, but &lt;5 is often cited as a cause for concern.\nWe can explore this issue using cross tabulations of the outcome variable against the categorical covariates.\n\nGo: Analyze &gt; Descriptive Statistics &gt; Crosstabs. Add htn to the Row(s): box and ses to the Col(s): box then click OK.\n\nIn the table that appears look at each cross-tabulation “cell” to see the number of observations in that cell, i.e. each possible combination of the independent categorical variable’s level and the outcome’s level, which for ses vs htn would include 1) low-yes, 2) low-no, 3) medium-yes, and 4) medium-no, 5) high-yes and 6) high-low. What can you see?\nWhat does the cross-tabulation show?\n\n\nRead/hide\n\nThere appear to be reasonable numbers of observations of both levels of the outcome for all levels of ses. See the “Additional potential problems” for more discussion of the possible problems you can face if the number of observations in a cell is too low.",
    "crumbs": [
      "8. Regresson modelling",
      "8.3. Logistic regression"
    ]
  },
  {
    "objectID": "logistic-regression.html#exercise-2-describe-the-population-level-association-between-bmi-and-the-probability-of-having-hypertension-using-logistic-regression",
    "href": "logistic-regression.html#exercise-2-describe-the-population-level-association-between-bmi-and-the-probability-of-having-hypertension-using-logistic-regression",
    "title": "Multiple binary logistic regression",
    "section": "Exercise 2: describe the population-level association between bmi and the probability of having hypertension using logistic regression",
    "text": "Exercise 2: describe the population-level association between bmi and the probability of having hypertension using logistic regression\n\nStep 1: explore the data\n\nNumerical covariates\nSpecifically, as we have a binary outcome when exploring the association between numerical covariates and our binary outcome we have to use a form of barchart, where we first create a “binned” or grouped version of our numerical covariate (i.e. a new variable which takes a single value for a range of values of the original covariate).\nLet’s see how to do this for bmi. First create the binned covariate.\n\nFrom the main menu go: Transform &gt; Visual Binning. Add bmi to the Variables to Bin: box and click Continue. At the top of the window that apppears look for the Name: fields and in the editable field called Binned Variable: give your new binned variable a name, e.g. bmi_bin. Then at the bottom right click on the Make Cutpoints… button. In the middle of the tool window click the Equal Percentiles Based on Scanned Cases option and in the Intervals - fill in either field are click on the Number of Cutpoints: box and enter a suitable number of cutpoints. It’s hard to know what is a suitable number but typically the more bins the better, as you can see associations at a finer scale, but for more bins you need more data (sample size). For bmi let’s choose 15, but you can always remake this variable with more/fewer bins as needed. Therefore, enter 15 in this box and then click Apply at the bottom. Then click the Make Labels button just below the Make Cutpoints… button and then click OK. Click OK on the information box that appears.\n\nNow we can plot the mean of our binary outcome, which is equivalent to the proportion/percentage, for every bin we’ve just created, using a type of barchart, although we’ll use a “histogram” tool to create it.\n\nFrom the main menu go: Graphs &gt; Chart Builder. If an information box appears saying “Before you use this dialogue…” just click OK. Now in the Gallery tab at the middle left click on Bar and then double click on the leftmost picture of a graph (Simple Bar) to add it to the plot. Next drag the htn variable from the Variables: box onto the Y-Axis? dotted box on the chart image. Similarly, drag the bmi_bin variable onto the X-Axis? box. Look to the right of the tool window at the Element Properties tab in the Statistics box. You should see a drop down menu under Statistic:. This should already read “Mean”, but if not click on it and select mean. This tells SPSS to calculate the mean of the Y-axis variable, htn, for each value of the X-axis variable, which are now repeated when in each bin. Now just click OK.\n\nWhat can you see?\nWhat does the barchart of BMI vs hypertension show?\n\n\nRead/hide\n\nThere appears to be a fairly linear increase in the proportion of individuals with hypertension as BMI values increase.\n\n\n\n\nStep 2: run the logistic regression\n\nRepeat the instructions from exercise 1, but replace the ses covariate with the bmi variable and make sure you add it to the Covariates: box not the Factors: box, otherwise you’ll get estimated differences in the probability of hypertension between each distinct value of bmi and a reference value!\n\n\n\nStep 3: check the assumptions\n\nSee exercise 1, but now we don’t have to worry about cell sizes as there are no cells: we’re working with a continuous covariate.\n\n\n\nStep 4: consider additional problems\n\nSee exercise 1.\n\n\n\nStep 5 and 6: understand the results tables and extract the key results and report the results\n\nSee exercise 1 for full details. In brief though, we can see our key result in the “Parameter estimates” table “Exp(B)” and associated 95% confidence intervals columns.\n\nSpecifically, we can see that the odds ratio for the slope between bmi and hypertension is 1.7. Therefore, the model is indicating that for every 1-unit increase in the value of bmi the mean odds of having hypertension are expected to increase 1.7 times, and the 95% confidence intervals indicate that this relative increase is likely to be between 1.5 and 1.9 times in the target population.",
    "crumbs": [
      "8. Regresson modelling",
      "8.3. Logistic regression"
    ]
  },
  {
    "objectID": "linear-regression-further.html",
    "href": "linear-regression-further.html",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "",
    "text": "In this practical we will look at how we can use linear regression with interactions to describe more complicated associations between two covariates and a continuous outcome.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "linear-regression-further.html#uses",
    "href": "linear-regression-further.html#uses",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "Uses",
    "text": "Uses\nPreviously we saw how we can use linear regression to describe associations between continuous, discrete or categorical covariates and continuous outcomes (and sometimes also discrete outcomes, if the modelling assumptions are not too badly violated), but we only looked at describing bivariate associations. Here, we look at how we can describe interactions between two covariates or non-linear associations to describe more complicated associations that often exist.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "linear-regression-further.html#key-terminology-and-concepts",
    "href": "linear-regression-further.html#key-terminology-and-concepts",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "Key terminology and concepts",
    "text": "Key terminology and concepts\n\nInteraction = When the size and/or direction of a association between one covariate and an outcome depends on the value of another covariate (or vice versa).\nNon-linear association = A association between a continuous/discrete covariate and a continuous/potentially discrete outcome that is not linear (i.e. not a straight line), or that you do not want to assume is linear. Therefore, within a linear regression modelling context such a association cannot be modelled using a single coefficient and requires polynomial terms for the covariate of interest.\nPolynomial terms = A continuous or discrete covariate with an exponent/power, e.g. age-squared, age-cubed etc.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "linear-regression-further.html#scenario",
    "href": "linear-regression-further.html#scenario",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "Scenario",
    "text": "Scenario\nWe wish to describe important and potentially complex associations between key socio-demographic and relevant health related characteristics and individuals’ systolic blood pressure using the data collected in the “SBP int & nl” dataset. Interactions and non-linear associations are often subtle/weak so the interactions and non-linear associations in this dataset have been created to clearly illustrate these types of associations.\nAs these are descriptive research questions and we want to describe the associations of interest as they exist (to inform policy and practice), we will use repeated linear regression models where we model the associations of interest without adjusting for any other covariates.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "linear-regression-further.html#exercise-1-to-explore-how-we-can-describe-categorical-by-categorical-interactions-we-will-describe-the-population-level-association-between-sex-femalemale-and-smoking-status-noyes-combined-subgroups-our-covariates-and-systolic-blood-pressure-mmhg---our-outcome-using-linear-regression.",
    "href": "linear-regression-further.html#exercise-1-to-explore-how-we-can-describe-categorical-by-categorical-interactions-we-will-describe-the-population-level-association-between-sex-femalemale-and-smoking-status-noyes-combined-subgroups-our-covariates-and-systolic-blood-pressure-mmhg---our-outcome-using-linear-regression.",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "Exercise 1: to explore how we can describe categorical by categorical interactions, we will describe the population-level association between sex (female/male) and smoking status (no/yes) combined subgroups (our covariates) and systolic blood pressure (mmHg - our outcome) using linear regression.",
    "text": "Exercise 1: to explore how we can describe categorical by categorical interactions, we will describe the population-level association between sex (female/male) and smoking status (no/yes) combined subgroups (our covariates) and systolic blood pressure (mmHg - our outcome) using linear regression.\n\nLoad the “SBP int & nl.sav” dataset.\n\n\nStep 1: explore the data\nWritten instructions: explore the data\n\n\nRead/hide\n\nAgain, it recommended to thoroughly explore you data in terms of: 1) the distribution of your outcome and covariates, to ensure you understand them well and are taking a suitable modelling approach (e.g. linear regression rather than another type of regression), and 2) what functional form of model makes good sense given the associations between your covariate(s) and outcome variable(s) - particularly whether there are any clear/strong interactions or non-linear associations, which we will look at here.\n\nUnivariate exploration\nTo understand the distribution of your variables you can use histograms and boxplots for numerical variables and barcharts for categorical variables. Let’s run a histogram for our outcome variable.\n\nFrom the main menu go: Graphs &gt; Histograms. Add sbp_sex_smoke into the Variable: box, tick the Display normal curve box and click OK.\n\nThe overall distribution appears to be bimodal - i.e. there are two peaks. This is often indicative of there being two separate and distinct groups within the data that systematically differ in terms of their outcome values.\nNext, let’s look at a bar chart for our sex and smoke variables.\n\nFrom the main menu go: Graphs &gt; Bar. Then click the Simple option and Define. Add the variable sex to the Category axis: box, and within the Bars represent area tick the % of cases option and click OK. What do you see? Repeat for the smoke variable.\n\nSex is perfectly equally distributed, i.e. 50:50, but only 20% of participants are smokers.\n\n\nBivariate exploration\nFirst, let’s explore the distribution of sex and smoking status together. This is visualising a cross-tabulation of the two variables.\n\nFrom the main menu go: Graphs &gt; Bar. Then click the Simple option and Define. Add the variable sex_smoke to the Category axis: box, and within the Bars represent area tick the % of cases option and click OK. Note: it seems to be pretty tricky to create such combined variables in SPSS so I actually just did this in another software package, but you could do it easily enough in Excel too using the CONCAT function.\n\nThe female smoker group is half the size of the male smoker group.\nNext let’s look at the distribution of the outcome in relation to the four subgroups of interest (female-non-smoke, female-smoker, male-non-smoker, male-smoker). We will use a boxplot to display the median, interquartile range and outliers for systolic blood pressure values in each subgroup.\n\nFrom the main menu go: Graphs &gt; Boxplot. Then select the Simple option and click Define. Add the sbp_sex_smoke variable into the Variable: box and the sex_smoke variable into the Category Axis: box and then click OK.\n\nIt looks like median systolic blood pressure is lowest and roughly equal for female and male non-smokers and higher for both female and male smokers, but with male smokers having slightly higher median systolic blood pressure than female smokers.\n\n\n\nStep 2: run the linear regression model\nSorry there are no video-based instructions.\nWritten instructions: run the linear regression model\n\n\nRead/hide\n\nRemember linear regression allows you to look at associations between a numerical outcome variable and any number of numerical or categorical covariates, assuming all the assumptions of the method are met (we’ll check these out shortly). So let’s see how we build, run and estimate our linear regression model in SPSS.\n\nFrom the main menu go: Analyze &gt; General Linear Model &gt; Univariate.\nNext, in the Univariate tool we add our outcome variable sbp_sex_smoke to the Dependent Variable: box. Then we add our covariate of sex_smoke to the Fixed factors: box. We can ignore everything else.\nNext, click the Contrasts button and in the Change Contrast area set Contrast: to “Repeated”.\nNext, click the Save button and under Predicted Values tick the Unstandardized box, and then under Residuals tick the Unstandardized box, and then under Diagnostics tick the box for Cook’s distance.\nLastly, click the Options button, and then under Display tick the Parameter estimates box and click Continue and then OK. The output window will then pop up with the results, but first…\n\n\n\n\nStep 3: check the assumptions of linear regression\nBefore we look at the results that appear in the output window we must first check whether we can treat the results as robust and valid. Our results are only potentially valid if the assumptions of the linear regression model have been met/hold, i.e. if they have not been violated. When it comes to interpretation we would of course have to consider all other potential sources of bias. Below we’ll just list the modelling assumptions (see the previous linear regression practical for more details) and how to check them.\n1. Continuous outcome\nWe know by definition the outcome is continuous.\n2. Independent observations\nWe know based on the study design, specifically the sampling design, that we have a simple random sample.\n3. Normally distributed residuals\nWe can again plot a histogram of our residuals to assess the approximately.\n\nFrom the main menu go: Graphs &gt; Histogram. Then add the RES_1 variable into the Variable: box (note: if you run further models the subsequent residual variables will be automatically named RES_2, RES_3 and so on - we will therefore refer to subsequent residual variables as RES_X, with X being an unknown number depending on how many models you have run), tick the Display normal curve box and click OK.\n\nThe residuals appear to be very reasonably approximately normal so we can safely assume this assumption has not been violated in our model.\n4. Linearity of the associations between the residuals and the numerical variable(s)\nThis assumption does not apply to this model as we only have a single categorical covariate. It would only apply when continuous/discrete covariates are involved.\n5. Homoscedasticity: constant variance of the residuals across model predicted values\nWe can again plot the residuals against predicted/fitted values, but as we just have one covariate with four groups we won’t see a cloud of points but instead four separate lines of points. This is because the model only includes one categorical covariate with four groups and so the model can only predict four possible distinct values depending on the group we are predicting a mean outcome for. However, we can still compare whether the spread of the residuals is approximately equal for each group.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot, then select Simple Scatter and click Define. Add the RES_1 variable into the Y Axis: box and the PRE_1 variable into the X Axis: box and click OK.\n\nThere does not appear to be any clear and substantial difference in the spread of the residuals at each predicted value.\n\n\nStep 4: consider additional possible issues (before interpreting the results)\nNo extremely influential observations\nAgain, we will create a scatterplot of Cook’s D against the observation ID variable (which is just a simple count from the first to the last observation).\n\nRemember in the main menu we go: Graphs &gt; Scatter/Dot. Then select the Simple Scatter option and click Define. Then add the Cook’s D variable COO_1 to the Y Axis: box and the id variable to the X Axis: box and click OK.\n\nThere are no clear outliers, i.e. observations that are much more influential on the results than most observations.\n\n\nStep 5: understand the results tables and extract the key results\nSo now that we have verified that the assumptions of the model are not clearly violated, we can finally interpret our results.\nThe “Between-Subjects Factors” just shows us the sample size per group for our covariate and we we will again ignore the “Tests of Between-Subjects Effects” and focus just on the final table called “Parameter Estimates”. See the first exercise for the previous linear regression practical for more information on what this table contains.\nFirst we will consider the point estimates for the various coefficients. The name/categorical level is given in the first column called “Parameter” and the point estimates of the coefficients are given in the second column called “B”.\nRemember that the intercept is the expected mean of the outcome when all covariates = 0. We just have one categorical covariate, and each category group will be represented within the model via separate dummy coded binary (0/1) variables. Therefore, the intercept represents the expected mean of the outcome when the sex_smoke variable = 0. We can work out which level has been coded as 0 (or the reference) easily. Look at the very bottom of the table and it says “a. This parameter is set to zero because it is redundant.” We can then look for the covariate in the “B” column that is 0 and has a superscript “a” next to it. Under the “Parameter” column this is the covariate level m_yes (listed as “sex_smoke=m_yes”). Therefore, the intercept represents the expected mean of the outcome (i.e. systolic blood pressure in units of mmHg) for male smokers.\nNote, for string-coded categorical variables, by default SPSS sets the reference category based on the names of the levels in reverse alphabetical order. So if you have levels called “a” and “b” then “b” would be the reference. As we just have the one categorical covariate, the other coefficients therefore represent the difference between the expected mean of the outcome for each group compared to the reference group, which again is set to m_yes, i.e. male smokers.\nFor example, we can see that male non-smokers have an expected mean systolic blood pressure that is -19.4 mmHg lower than the expected mean systolic blood pressure for male smokers (best point estimate), and the 95% confidence intervals tell us that the likely value for this mean difference in the target population is between -21.3 and -17.5 mmHg (and if we want to frame this in terms of a null-hypothesis significance test then we can see the two-sided p-value for the null hypothesis that the coefficient = 0 under the “Sig.” column, which is &lt;0.001).\nThen if we want to explore comparisons to groups other than the reference group we can look below at the “Contrast Results (K Matrix)” table. Here the first colum refers to the contrast or comparison of interest in terms of levels. These are as per the “Parameter estimates” table, so level 1 = fm_no, level 2 = fm_yes, level 3 = m_no and level 4 = m_yes (the reference level for the “Parameter estimates” table). So we can see that the final comparison of level 3 to level 4 matches that seen in the “Parameter estimates” table for level 3 (m_no). As always, if you want to swap any comparisons to the other way around just flip the signs and swap the confidence intervals.\nPractical importance\nAs always, when it comes to making practical interpretations we need to consider whether the size and direction of plausible associations would be important clinically or for public health considerations.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "linear-regression-further.html#exercise-2-to-explore-how-we-can-describe-continuous-by-categorical-interactions-we-will-describe-how-the-population-level-association-between-age-years-and-systolic-blood-pressure-mmhg---our-outcome-varies-by-sex-femalemale-using-linear-regression.-note-you-could-equally-frame-this-question-as-looking-how-the-association-between-sex-and-systolic-blood-pressure-varies-by-age.-its-two-sides-of-the-same-coin.",
    "href": "linear-regression-further.html#exercise-2-to-explore-how-we-can-describe-continuous-by-categorical-interactions-we-will-describe-how-the-population-level-association-between-age-years-and-systolic-blood-pressure-mmhg---our-outcome-varies-by-sex-femalemale-using-linear-regression.-note-you-could-equally-frame-this-question-as-looking-how-the-association-between-sex-and-systolic-blood-pressure-varies-by-age.-its-two-sides-of-the-same-coin.",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "Exercise 2: to explore how we can describe continuous by categorical interactions, we will describe how the population-level association between age (years) and systolic blood pressure (mmHg - our outcome) varies by sex (female/male) using linear regression. Note, you could equally frame this question as looking how the association between sex and systolic blood pressure varies by age. It’s “two sides of the same coin”.",
    "text": "Exercise 2: to explore how we can describe continuous by categorical interactions, we will describe how the population-level association between age (years) and systolic blood pressure (mmHg - our outcome) varies by sex (female/male) using linear regression. Note, you could equally frame this question as looking how the association between sex and systolic blood pressure varies by age. It’s “two sides of the same coin”.\n\nStep 1: explore the data\n\nUnivariate\nFirst, let’s explore the distribution of the outcome via a histogram.\n\nFrom the main menu go: Graphs &gt; Histograms. Add sbp_age_sex into the Variable: box, tick the Display normal curve box and click OK.\n\nThe overall distribution appears to be approximately normal but potentially there is some multi-modality, i.e. more than one peak/mean.\nNext, let’s explore the distribution of age via a histogram.\n\nFrom the main menu go: Graphs &gt; Histograms. Add age into the Variable: box, tick the Display normal curve box and click OK.\n\nIt’s hard to interpret but age appears more uniformly distributed, or maybe a complex mix of a few peaks. Remember that linear regression can model continuous/discrete covariates of any distribution. It’s the distribution of the model residuals that must be approximately normal. It’s always good if the same size is reasonably equally distributed across the range of any continuous/discrete covariates, as this gives the model more information to estimate the association more accurately.\nWe’ve already explored the distribution of the sex variable, so need to do so again.\n\n\nBivariate\nLet’s look visualise the distribution of systolic blood pressure values by age and sex.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot. Then ensure Simple Scatter is selected and click OK. Then add the sbp_age_sex variable into the Y Axis: box, the age variable into the X Axis: box, and add the sex variable into the Set Markers By: box (which will colour points by sex), and then click OK.\n\nIt looks like there is a linear association between age and systolic blood pressure for both sexes, but the association is steeper for men.\n\n\n\nStep 2: run the regression model\nSorry there are no video-based instructions.\nWritten instructions: run the linear regression model\n\n\nRead/hide\n\n\nFrom the main menu go: Analyze &gt; General Linear Model &gt; Univariate.\nNext, in the Univariate tool we add our outcome variable sbp_age_sex to the Dependent Variable: box. Then this time we add our covariate of age to the Covariate(s): box and our covariate of sex to the Fixed factors: box.\nNext, click the Model button and in the Specify Model area click the Build terms: button. Then in the Build Term(s): area under Type: click on the drop-down menu and choose Main effects. Then click on sex and drag it into the Model: area on the right and release the click (or you can also click the blue, right-pointing arrow below the drop-down menu to add them). Repeat for the sex covariate. Both covariates should now be in the Model: area. Now click on the same drop-down menu again and choose Interaction. Then click on sex, hold shift, then click on age (so they are both highlighted) and, while still holding the left-button on the mouse down, drag them both into the Model: area and release. Under the age and sex covariates you should now see the interaction term “age*sex”. Now click Continue at the bottom of the tool.\nNext, click the Save button and under Predicted Values tick the Unstandardized box, and then under Residuals tick the Unstandardized box, and then under Diagnostics tick the box for Cook’s distance.\nLastly, click the Options button, and then under Display tick the Parameter estimates box and click Continue and then OK. The output window will then pop up with the results, but first…\n\n\n\n\nStep 3: check the assumptions\nWe would again need to verify all the assumptions. We only provide details where things are slightly different from the last exercise.\n1. Continuous outcome 2. Independent observations 3. Normally distributed residuals 4. Linearity of the associations between the residuals and the numerical variable(s)\nAs we now have a continuous/discrete covariate we would need to plot the residuals against the age covariate to check there are no trends. We can do this using a scatter plot.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot, then select Simple Scatter and click Define. Add the RES_X variable into the Y Axis: box and the age variable into the X Axis: box and click OK.\n\nThere are no clear trends.\n5. Homoscedasticity: constant variance of the residuals across model predicted values\n\n\nStep 4: consider additional possible issues\nNo extremely influential observations\nThese can be checked as before, but there are again no concerns.\n\n\nStep 5: understand the results tables and extract the key results\nAgain, we will focus on the “Parameter estimates” table. For our categorical covariate of sex we can see that the male level has been set as the reference level. Again, we can determine this by looking for which level = 0 and has the superscript “a” next to it, which links to the statement “The parameter is set to zero because it is redundant” at the bottom of the table.\nTherefore, the intercept now has no real-world meaning because it represents the expected mean outcome when all covariates are 0, which here is for males aged 0. We could “centre” age so it is the difference from the mean of age and then the intercept would represent the expected mean outcome for males at the mean age (across all participants), but we are interested in the association between age and systolic blood pressure and how it varies by sex, so there is no benefit.\nAs we included an interaction between age and sex in the model the coefficient for age represents the slope (or more precisely the expected change in the mean of the outcome for every 1-unit increase in the covariate, i.e. for every year older) when sex = 0, which here represents males. We can therefore see that the model predicts that for every year older a male is their mean systolic blood pressure is expected to increase by 0.51 mmHg. The coefficient named “[sex=fm]*age” can then be interpreted as the model-predicted difference in the association (slope) between age and systolic blood pressure in females compared to the corresponding association (slope) in men. We can therefore see that the association (slope) between age and systolic blood pressure in females is 0.29 mmHg lower (because the coefficient is negative - the best point estimate) than the corresponding association (slope) in males, and the likely value for this difference in the target population is between -0.34 and -0.24 (with the p-value for the null-hypothesis that this difference in slopes = 0 given in the “Sig.” column).\nAnother way to understand this is that the model predicts that the association (slope) between age and systolic blood pressure in males is 0.51 while in females it is 0.51 + (-0.29) = 0.22, i.e. very roughly half as steep. If we wanted to get the predicted association (slope) for females and its 95% confidence intervals we could just recode the sex variable so that the female level was set to the reference level and re-run the model. Again, SPSS chooses the level where the name of that level starts/is made of letters that are lower in the alphabet. So you could e.g. recode it as “z_female” to ensure that female becomes the reference level. Then the age coefficient would represent the association (slope) for females.\nPractical importance\nAs always, when it comes to making practical interpretations we need to consider whether the size and direction of plausible associations would be important clinically or for public health considerations.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "linear-regression-further.html#exercise-3-to-explore-how-we-can-describe-continuous-by-continuous-interactions-we-will-describe-how-the-population-level-association-between-age-years-and-systolic-blood-pressure-mmhg---our-outcome-varies-by-bmi-kgm2-or-equivalently-how-how-the-population-level-association-between-bmi-kgm2-and-systolic-blood-pressure-mmhg---our-outcome-varies-by-age-years-using-linear-regression.-again-this-is-two-sides-of-the-same-coin.",
    "href": "linear-regression-further.html#exercise-3-to-explore-how-we-can-describe-continuous-by-continuous-interactions-we-will-describe-how-the-population-level-association-between-age-years-and-systolic-blood-pressure-mmhg---our-outcome-varies-by-bmi-kgm2-or-equivalently-how-how-the-population-level-association-between-bmi-kgm2-and-systolic-blood-pressure-mmhg---our-outcome-varies-by-age-years-using-linear-regression.-again-this-is-two-sides-of-the-same-coin.",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "Exercise 3: to explore how we can describe continuous by continuous interactions, we will describe how the population-level association between age (years) and systolic blood pressure (mmHg - our outcome) varies by BMI (kg/m2), or equivalently how how the population-level association between BMI (kg/m2) and systolic blood pressure (mmHg - our outcome) varies by age (years), using linear regression. Again, this is “two sides of the same coin”.",
    "text": "Exercise 3: to explore how we can describe continuous by continuous interactions, we will describe how the population-level association between age (years) and systolic blood pressure (mmHg - our outcome) varies by BMI (kg/m2), or equivalently how how the population-level association between BMI (kg/m2) and systolic blood pressure (mmHg - our outcome) varies by age (years), using linear regression. Again, this is “two sides of the same coin”.\n\nStep 1: explore the data\n\nUnivariate\nFirst, let’s explore the distribution of the outcome via a histogram.\n\nFrom the main menu go: Graphs &gt; Histograms. Add sbp_age_bmi into the Variable: box, tick the Display normal curve box and click OK.\n\nThe overall distribution appears to be approximately normal but with a little right-skew.\nNext, let’s explore the distribution of age and BMI via a histogram.\n\nFrom the main menu go: Graphs &gt; Histograms. Add bmi into the Variable: box, tick the Display normal curve box and click OK.\n\nBMI appears fairly uniformly distributed.\nWe’ve already explored the distribution of the age variable, so need to do so again.\n\n\nBivariate\nLet’s look visualise the distribution of systolic blood pressure values by BMI. We’ve already explored the distribution of systolic blood pressure by age, so need to do so again.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot. Then ensure Simple Scatter is selected and click OK. Then add the sbp_age_bmi variable into the Y Axis: box, the bmi variable into the X Axis: box and then click OK.\n\nIt looks like there is a linear association between BMI and systolic blood pressure.\n\n\n\nStep 2: run the regression model\nSorry there are no video-based instructions.\nWritten instructions: run the linear regression model\n\n\nRead/hide\n\n\nFrom the main menu go: Analyze &gt; General Linear Model &gt; Univariate.\nNext, in the Univariate tool we add our outcome variable sbp_age_bmi to the Dependent Variable: box. Then this time we add our covariates of age and bmi to the Covariate(s): box.\nNext, click the Model button and in the Specify Model area click the Build terms: button. Then in the Build Term(s): area under Type: click on the drop-down menu and choose Main effects. Then click on age and drag it into the Model: area on the right and release the click (or you can also click the blue, right-pointing arrow below the drop-down menu to add them). Repeat for the bmi covariate. Both covariates should now be in the Model: area. Now click on the same drop-down menu again and choose Interaction. Then click on age, hold shift, then click on bmi (so they are both highlighted) and, while still holding the left-button on the mouse down, drag them both into the Model: area and release. Under the age and bmi covariates you should now see the interaction term “age*bmi”. Now click Continue at the bottom of the tool.\nNext, click the Save button and under Predicted Values tick the Unstandardized box, and then under Residuals tick the Unstandardized box, and then under Diagnostics tick the box for Cook’s distance.\nLastly, click the Options button, and then under Display tick the Parameter estimates box and click Continue and then OK. The output window will then pop up with the results, but first…\n\n\n\n\nStep 3: check the assumptions\nWe would again need to verify all the assumptions. We only provide details where things are slightly different from the last exercise.\n1. Continuous outcome 2. Independent observations 3. Normally distributed residuals 4. Linearity of the associations between the residuals and the numerical variable(s)\nAs we now have two continuous/discrete covariate we would need to plot the residuals against both to check there are no trends. We can do this using a scatter plot.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot, then select Simple Scatter and click Define. Add the RES_X variable into the Y Axis: box and the age variable into the X Axis: box and click OK. Repeat for bmi.\n\nThere are no clear trends.\n5. Homoscedasticity: constant variance of the residuals across model predicted values\n\n\nStep 4: consider additional possible issues\nNo extremely influential observations\nThese can be checked as before, but there are again no concerns.\n\n\nStep 5: understand the results tables and extract the key results\nAgain, we will focus on the “Parameter estimates” table. The intercept now represents the expected mean of the outcome when both age and BMI = 0, so clearly it has no real-world interpretation (but again, we could centre both covariates and then it would be interpretable).\nThe bmi and age coefficients then represent the model-predicted association (slope) between BMI and systolic blood pressure when age = 0 and between age and systolic blood pressure when BMI = 0, so similarly they are not very meaningful.\nThe “bmi * age” coefficient represents either the difference in the association (slope) between BMI and systolic blood pressure for a 1-unit increase in age, or equivalently the difference in the association (slope) between age and systolic blood pressure for a 1-unit increase in BMI. Therefore, we can see that the association (slope) between age and systolic blood pressure increases (becomes more steeply positive) for every 1-unit increase in BMI by 0.031 mmHg (best point estimate), with the likely value for the increase in the target population is between 0.027 and 0.035, and equivalently the association (slope) between BMI and systolic blood pressure increases (becomes more steeply positive) for every 1-unit increase in age by 0.031 mmHg (best point estimate), with the likely value for the increase in the target population is between 0.027 and 0.035.\nIt is usually most helpful to interpret one or both associations at key values of the other covariate. For example, the association (slope) between age and systolic blood pressure at key values of BMI. This can be done by using the model to predict expected mean values of the outcome (and confidence intervals for those values) for representative values of age and key values of BMI (or vice versa). However, in SPSS this is quite tricky and time consuming, so we won’t look at this further in this session.\nPractical importance\nAs always, when it comes to making practical interpretations we need to consider whether the size and direction of plausible associations would be important clinically or for public health considerations.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "linear-regression-further.html#exercise-4-to-explore-how-we-can-describe-non-linear-associations-between-continuousdiscrete-covariates-and-continuous-outcomes-we-will-describe-the-population-level-non-linear-association-between-age-years-and-systolic-blood-pressure-mmhg---our-outcome.",
    "href": "linear-regression-further.html#exercise-4-to-explore-how-we-can-describe-non-linear-associations-between-continuousdiscrete-covariates-and-continuous-outcomes-we-will-describe-the-population-level-non-linear-association-between-age-years-and-systolic-blood-pressure-mmhg---our-outcome.",
    "title": "Multiple linear regression interactions & non-linear associations",
    "section": "Exercise 4: to explore how we can describe non-linear associations between continuous/discrete covariates and continuous outcomes, we will describe the population-level non-linear association between age (years) and systolic blood pressure (mmHg - our outcome).",
    "text": "Exercise 4: to explore how we can describe non-linear associations between continuous/discrete covariates and continuous outcomes, we will describe the population-level non-linear association between age (years) and systolic blood pressure (mmHg - our outcome).\n\nStep 1: explore the data\n\nUnivariate\nFirst, let’s explore the distribution of the outcome via a histogram.\n\nFrom the main menu go: Graphs &gt; Histograms. Add sbp_age2 into the Variable: box, tick the Display normal curve box and click OK.\n\nThe overall distribution appears to be approximately normal.\nWe’ve already explored the distribution of age previously, which is approximately uniform.\n\n\nBivariate\nLet’s look visualise the distribution of systolic blood pressure values by age, which will now be non-linear by design for this practical.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot. Then ensure Simple Scatter is selected and click OK. Then add the sbp_age2 variable into the Y Axis: box, the age variable into the X Axis: box and then click OK.\n\nIt’s hard to see but there is a hint of a slight inverted-u shape to the trend in the points.\n\n\n\nStep 2: run the regression model\nSorry there are no video-based instructions.\nWritten instructions: run the linear regression model\n\n\nRead/hide\n\n\nFirst we need to create our age squared variable. From the main menu go: Transform &gt; Compute Variable. Then in the Target Variable: box add a name for our age squared variable. I will use age2. Then in the Numeric Expression: box add age ** 2. The “**” indicates you are raising the variable age to a power, in this case 2, i.e. squaring it. Then click OK.\n\nNow we can run the model.\n\nFrom the main menu go: Analyze &gt; General Linear Model &gt; Univariate.\nNext, in the Univariate tool we add our outcome variable sbp_age2 to the Dependent Variable: box. Then this time we add our covariates of age and age2 to the Covariate(s): box.\nNext, click the Model button and in the Specify Model area click the Build terms: button. Then in the Build Term(s): area under Type: click on the drop-down menu and choose Main effects. Then click on age and drag it into the Model: area on the right and release the click (or you can also click the blue, right-pointing arrow below the drop-down menu to add them). Repeat for the age2 covariate.\nNext, click the Save button and under Predicted Values tick the Unstandardized and Standard Error boxes, and then under Residuals tick the Unstandardized box, and then under Diagnostics tick the box for Cook’s distance.\nLastly, click the Options button, and then under Display tick the Parameter estimates box and click Continue and then OK. The output window will then pop up with the results, but first…\n\n\n\n\nStep 3: check the assumptions\nWe would again need to verify all the assumptions. We only provide details where things are slightly different from the last exercise.\n1. Continuous outcome 2. Independent observations 3. Normally distributed residuals 4. Linearity of the associations between the residuals and the numerical variable(s)\nNote: if there is a non-linear trend in the association between age and systolic blood pressure if our model adequately models this via our quadratic term then the residuals should show no trends as they should be approximately evenly distributed around the model-predicted mean, which itself will have a non-linear association with age. Let’s check.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot, then select Simple Scatter and click Define. Add the RES_X variable into the Y Axis: box and the age variable into the X Axis: box and click OK.\n\nThere are no clear trends.\n5. Homoscedasticity: constant variance of the residuals across model predicted values\n\n\nStep 4: consider additional possible issues\nNo extremely influential observations\nThese can be checked as before, but there are again no concerns.\n\n\nStep 5: understand the results tables and extract the key results\nAgain, we will focus on the “Parameter estimates” table. The intercept now represents the expected mean of the outcome when both age and age2 are 0, so clearly it has no real-world interpretation (but again, we could centre both covariates and then it would be interpretable ).\nWe cannot usefully interpret the coefficient for age because it depends on age2, i.e. the association is non-linear and cannot be described by a single slope value. Therefore, typically we would compute predicted values for a range of values of our covariate using the model (along with and their confidence intervals) and plot those to visualise the non-linear association and allow inference for practical interpretation.\nIt is again not straight forward to do this in SPSS, but it’s not too complicated to at least plot the predict outcome values from the model against the observed covariate values, so let’s do that.\n\nFrom the main menu go: Graphs &gt; Scatter/Dot, then select Simple Scatter and click Define. Add the PRE_X variable into the Y Axis: box (where _X may be 4 if you’ve just made one model per exercise but it may be higher - used the highest value as that will be the most recent predicted values variable) and the age variable into the X Axis: box and click OK.\n\nRemember to pay attention to the axes. Without further editing these are the defaults, and a restricted y-axis range will make any curve appear more curved than a more honest perspective when looking across a wider range of possible outcome values.\nPractical importance\nAs always, when it comes to making practical interpretations we need to consider whether the size and direction of plausible associations would be important clinically or for public health considerations.",
    "crumbs": [
      "8. Regresson modelling",
      "8.2. Linear regression II"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Quantitative Research Methods in International Health Computer Practical Session Materials",
    "section": "",
    "text": "This website hosts the materials for the Introduction to Quantitative Research Methods in International Health module computer practical sessions.\nIf you need an introduction to the website and information on how to use it during the computer practical sessions please read here: how to use this website.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html",
    "href": "independent-t-test-skewed.html",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "",
    "text": "In this practical we’ll practice using the independent t-test to analyse an outcome that is strongly skewed.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#scenario",
    "href": "independent-t-test-skewed.html#scenario",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Scenario",
    "text": "Scenario\nYou and your colleagues have been tasked by the your district’s health authorities to help them deal with an outbreak of Ebola. Many villages are affected and case counts per village over the last month have been collected. The health authorities want to understand where best to focus their limited resources, so they want you to help them address a descriptive research question: what is the association between village density, specifically villages with &lt;10 individuals per 100m² compared to those with ≥10 per 100m², and the mean number of Ebola cases.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#exercise-1-use-the-independent-t-test-to-analyse-how-the-mean-number-of-ebola-cases-per-village-differs-between-villages-with-10-individuals-per-100m²-and-those-with-10-per-100m²",
    "href": "independent-t-test-skewed.html#exercise-1-use-the-independent-t-test-to-analyse-how-the-mean-number-of-ebola-cases-per-village-differs-between-villages-with-10-individuals-per-100m²-and-those-with-10-per-100m²",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Exercise 1: use the independent t-test to analyse how the mean number of Ebola cases per village differs between villages with <10 individuals per 100m² and those with ≥10 per 100m²",
    "text": "Exercise 1: use the independent t-test to analyse how the mean number of Ebola cases per village differs between villages with &lt;10 individuals per 100m² and those with ≥10 per 100m²\n\nLoad the “Ebola data.sav” SPSS dataset.\n\nAgain this is a simulated dataset. There are three variables and every observation is meant to represent a record from a separate village in a hypothetical region suffering from an Ebola outbreak. The variables contain the following data:\n\nn_ebola_cases = number of Ebola cases per village over the last month.\nn_chw = number of community health workers per village.\npop_density = relative population density per village (1 = &lt;10 per 100m², 2 = ≥ 10 per 100m²).\n\nUsing an independent t-test we’ll analyse whether there is a difference in the mean number of Ebola cases per village over the last month between villages with &lt;10 individuals per 100m² and those with ≥10 per 100m², i.e. between lower and higher density villages.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#step-1-check-the-assumptions-of-the-independent-t-test.",
    "href": "independent-t-test-skewed.html#step-1-check-the-assumptions-of-the-independent-t-test.",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Step 1: check the assumptions of the independent t-test.",
    "text": "Step 1: check the assumptions of the independent t-test.\n\n1. Continuous outcome\nTechnically an independent t-test assumes a continuous outcome variable, but as long as the following two assumptions are satisfied it’s fine to use a discrete outcome with an independent t-test. Here our outcome is a discrete numerical variable.\n\n\n2. Independent groups\nWe will assume this holds here as we didn’t build non-independence into the data, but it may well not hold if this was real data. Can you think why? It’s likely that villages that are closer together have correlated Ebola rates, given spread between villages that are closer together is more likely than between villages that are farther apart. If this was a big issue we could most simply aggregate or pool villages that are closer together to create cluster summary values of the outcome, if these clusters were large enough that there was negligible transmission between clusters, or we could use a more sophisticated technique like a multi-level regression model.\n\n\n3. The outcome is approximately normally distributed within each group\nSee the “Step 1: check the assumptions of the independent t-test” section in the Inferential analysis 1: the independent t-test” section above for a reminder of how to do this using histograms, but this time use the n_ebola_cases variable as the outcome and the pop_density variable as the rows grouping variable in your histograms. You’ll see that the distribution of Ebola cases per population density group is right skewed, particularly for the lower density group. Ideally we’d analyse such data using a special model for count data like a Poisson or negative binomial model, but those approaches are beyond the scope of this introductory course.\nInstead, we can try and transform the outcome to address the skew and then use an independent t-test that assumes normal data. There are many possible ways to transform data, and many are quite complicated and again beyond this course. However, for right skewed data two common and simple transforms that are often sufficient are to:\n\nTake the square root of every value.\nTake the logarithm (usually the natural log) of every value.\n\nThese transformations will hopefully “pull in” the right skewed values so the distribution becomes more symmetrical and approximately normal (although it will never be truly normal). Log transforms will pull any skew in more strongly than square root transforms and are often preferred and probably best to try first. Note: as you cannot take the square root or logarithm of negative numbers you can only apply these transformations to positive values. Also, while you can take the square root of 0 (it’s 0) you cannot take the logarithm of 0 (it’s “undefined”). Therefore, if your outcome has any values of 0 then you cannot take its logarithm unless you first add a constant to every value. This is a bit of a fudge type approach and is really not ideal, but it is done and people usually add a small value, maybe most often 1 (or sometimes 0.1), but there’s no right or wrong number really.\n\n\n4. Equal variances in each group\nWe can either use the histograms we produce to compare variances in each group or use the result of the Levene’s test once we run the independent t-test.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#step-2-transform-the-outcome",
    "href": "independent-t-test-skewed.html#step-2-transform-the-outcome",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Step 2: transform the outcome",
    "text": "Step 2: transform the outcome\nLet’s create a ln (natural log) transformed variable using the Compute tool to deal with the right-skew in the outcome’s distribution within each group.\n\nFirst explore the variable by creating a frequency table. See the “Categorical variables” sub-section in the “Exercise: create a”Table 1” summarising the key characteristics of the SBP data study sample” section above if you need a reminder of how to create a frequency table. Note: these instructions were for categorical variables but you can use numerical variables too.\nWhat do you notice about the range of the outcome? It includes 0. Therefore, we must add a constant before transforming via the logarithm. We’ll add 1 as it’s probably the most common value used in such a situation.\nFrom the main menu go: Transform &gt; Compute Variable. Then in the Compute Variable tool call the new variable ln_n_ebola_cases, but this time enter the transform command “LN(n_ebola_cases+1)” (without quotes) and then click OK. This command first adds 1 to every outcome value before taking the natural log.\nOnce you’ve computed your log-transformed variable check the distribution of values for the transformed outcome in each group using histograms again. You should see that they look more normal now.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#step-3-run-the-independent-t-test",
    "href": "independent-t-test-skewed.html#step-3-run-the-independent-t-test",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Step 3: run the independent t-test",
    "text": "Step 3: run the independent t-test\n\nRun an independent t-test comparing the mean of ln_n_ebola_cases between the lower and higher groups of the pop_density variable. If you can’t remember how to refer back to the previous “Step 3: run the independent t-test” section above. When defining the groups enter Group 1 as “1” (the &lt;10 individuals per 100m² group) and Group 2 as “2” to replicate the results I present, but you could of course compare them the other way round.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#step-4-understand-the-results-tables-and-extract-the-key-results",
    "href": "independent-t-test-skewed.html#step-4-understand-the-results-tables-and-extract-the-key-results",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Step 4: understand the results tables and extract the key results",
    "text": "Step 4: understand the results tables and extract the key results\n\nAgain, refer back to the previous “Step 4: understand the results tables and extract the key results” section above if you need a refresher, but we are just extracting the same results here.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#step-5-report-and-interpret-the-results",
    "href": "independent-t-test-skewed.html#step-5-report-and-interpret-the-results",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Step 5: report and interpret the results",
    "text": "Step 5: report and interpret the results\nYou should get the following result using the equal variances not assumed set of results, given the Levene’s test is significant and the histograms indicate non-equal variances, which could be reported as follows (remember to explain your analysis process and justify it in your methods):\n\nComparing the &lt;10 per 100m² group (mean Ebola cases per village = 1.94) to the ≥ 10 per 100m² group (Ebola mean cases per village = 6.63) there is a mean difference in the natural-log number of Ebola cases per village of -1 (95% CI: -1.1, -0.9).\n\n\nNote: I’ve presented the group means on the original scale so they can be interpreted easily, and I’ve not bothered presenting the associated p-value above. Again it tells us nothing more and far less than the effect size or mean difference and the associated 95% confidence intervals.\nHowever, this mean difference and its 95% CI is for our outcome but on the natural log scale (i.e. how we transformed it), so it isn’t easy to interpret: what does a mean difference of -1 ln Ebola cases mean? Luckily we can transform (back transform) this mean difference back onto the original scale by using exponentiation with the base e applied to each value (https://en.wikipedia.org/wiki/Exponentiation). This is actually maybe most easily and quickly done just using the Google search engine’s calculator functions. Just type “exp(X)” into Google, where X is either the value of the mean difference or the upper or lower confidence interval value, and it will back-transform those values back to their original scales.\nDoing this for the mean difference and each 95% CI and you should get the following result:\n\n\nMean difference = 0.36 (95% CI: 0.32, 0.4).\n\n\nSo how do we interpret this now? We must take care because we have calculated a difference on natural-log transformed data (via the independent t-test) and then back-transformed that mean difference of ln-transformed values. What we actually then ultimately get is a ratio between the geometric mean (https://en.wikipedia.org/wiki/Geometric_mean) of the outcome in the two comparison groups, rather than a difference in the arithmetic mean of the outcome in the two comparison groups, as we get with an independent t-test where we do not ln-transform the data and then back-transform the results.\nFor example, ln(2) – ln(4) is -0.6931472, and if you calculate the exponential (with base e) of -0.6931472 you get 0.5, and the ratio of 2:4, i.e. 2/4 = 0.5. Or vice versa: if you calculate the exponential of ln(4) – ln(2) you get 2, and the ratio of 4/2 is 2! Therefore, the exponential back-transformed result now represents a ratio, but as we took the mean of the log-transformed values when we back-transform these would become geometric means, so the ratio is between the geometric mean of the outcome for each group. Therefore, like with risk/odds ratios as this result is now on a ratio or multiplicative scale the null value (i.e. the value of no difference between the two groups) is now not 0 but 1, because any number divided by itself = 1. And just like with risk/odds ratios we interpret the result in terms of the number of “times” our reference group’s mean value is compared to the comparison group.\nTherefore, in a results section we can say that “in villages with &lt;10 individuals per 100m² the geometric mean number of Ebola cases was 0.36 (95% CI: 0.32, 0.4) times the geometric mean number of Ebola cases found in villages ≥ 10 individuals per 100m².”\nYou can also view this ratio of means in percentage terms by converting the result using one of the following simple sums, which you may find easier to interpret:\n\n\nWhen the exponentiated difference (D) is &lt;1 the % decrease = (1 - D) x 100. When the exponentiated difference (D) is &gt;1 the % increase = (D - 1) x 100.\n\n\nSo for our result we can calculate that the geometric mean number of Ebola cases was (1 - 0.36) x 100 = 64% lower in low density areas compared to high density areas (you should also transform each confidence intervals range value onto the percentage scale and present them along with the point estimate in any results section).\nNote: geometric means are typically very similar to arithmetic means for outcomes that don’t have a huge range, i.e. that don’t span a number of orders of magnitude. Therefore, for many outcomes you can think of the geometric mean as being approximately equivalent to the arithmetic mean, but this won’t be the case for outcomes with big ranges spanning orders of magnitude from the smallest to the largest value.\nLastly, if we were comparing a numerical variable between two groups what if our data are still badly skewed despite transformation? Then we can use a non-parametric test, such as the Mann-Whitney U test (the most common fall back if the independent t-test cannot be used). We will not cover that in this class so we have more time for more sophisticated tests, but you should have no serious difficulties running and interpreting such a test now using one of the many online or text book guides available (see the MWU SPSS.pdf files in the “Computer Practical sessions” “Additional materials” folder on Minerva). The two big limitations of this test are the reduced power and the fact it only gives you a p-value to accompany your difference (which given the skewed data should arguably be summarised via the median) but no confidence intervals.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "independent-t-test-skewed.html#additional-exercise-use-the-independent-t-test-to-analyse-how-the-mean-number-of-ebola-cases-per-village-differs-between-villages-with-5-community-healthcare-workers-and-those-5",
    "href": "independent-t-test-skewed.html#additional-exercise-use-the-independent-t-test-to-analyse-how-the-mean-number-of-ebola-cases-per-village-differs-between-villages-with-5-community-healthcare-workers-and-those-5",
    "title": "The independent t-test: usage when the outcome is skewed",
    "section": "Additional exercise: use the independent t-test to analyse how the mean number of Ebola cases per village differs between villages with <5 community healthcare workers and those ≥5",
    "text": "Additional exercise: use the independent t-test to analyse how the mean number of Ebola cases per village differs between villages with &lt;5 community healthcare workers and those ≥5\nUsing the “Ebola.sav” dataset and the process outlined above use an independent t-test to analyse the association between community health worker number and Ebola case number.\n\nDivide n_chw into two groups based on n_chw values &lt;5 and those ≥5.\nAs we know the outcome n_ebola_cases is right skewed we must first transform it. We know a natural log (ln) transform works reasonably well, so transform this variable (or use the already transformed version you’ll have created earlier). Remember as there are 0s in the outcome we must also first add a constant before transforming. When I did the analysis I used a constant of 1, so I suggest you use this to avoid any differences, although they should be very minor.\nCompare the number of Ebola cases per village for villages with &lt;5 community health workers to those with ≥5 community health workers using the independent t-test.\nExtract the mean difference and confidence intervals around this estimate and back transform them by exponentiation onto their original scale. Remember you can do this quickly via Google by Googling exp(x) where x is the ln-transformed mean difference/upper or lower confidence interval of the ln-transformed mean difference.\nIn the “Exercises” folder open the “Exercises.docx” Word document and scroll down to Independent t-test with a skewed outcome: the relationship between community health worker number and Ebola case rate within villages.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: sample size, group sizes and group outcome means (on the original scale). Also be sure to include sufficient details about the outcome variable and the comparison made, including how the two independent groups were defined, as well as the type of analysis used, and of course the key inferential results, but you also need to mention the fact that the data were ln-transformed prior to analysis to deal with the right skew/non-normality and that the mean difference and confidence intervals presented were then back-transformed. Remember you need to interpret the result carefully because the transformation and back-transformation mean that the mean difference is now no longer a simple mean difference in reality. See “Step 5: report and interpret the results” if you need reminding. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\nExample results reporting text\n\n\nRead/hide\n\nUsing an independent t-test I analysed the association between the number of community health workers per village (&lt;5 compared to ≥5) and the number of Ebola cases per village. Out of a total sample size of 250 villages, 107 had &lt;5 community health workers (arithmetic mean Ebola cases = 3.1) and 143 had ≥5 community health workers (arithmetic Ebola cases = 3.4). Due to a strongly right-skewed outcome I first transformed the outcome by taking the natural log of each outcome value (first adding a constant of 1 due to the presence of 0s) before back-transforming the resulting independent t-test results via exponentiation with the base e. This indicated that villages with &lt;5 community health workers had a geometric mean number of Ebola cases that was 0.9 times (95% CI: 0.7, 1.04) the geometric mean number of Ebola cases among villages with ≥5 community health workers, or equivalently the geometric number of Ebola cases among villages with &lt;5 community health workers was 10% lower than the geometric mean number of Ebola cases among villages with ≥5 community health workers.\nTherefore, based on the confidence intervals there was no clear or statistically significant association between the number of community health workers in a village and the number of Ebola cases. The key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an observational cross-sectional study like this). Therefore, this is likely to represent a biased estimate of the association between community health worker number per village and the number of Ebola cases per village in the target population.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.2. Independent t-test with skewed data"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html",
    "href": "complex-sampling-analysis.html",
    "title": "Complex sample/survey design analysis",
    "section": "",
    "text": "In this practical we will look at how we can appropriately analyse data that has been sampled using sampling methods including “complex” features. However, we will be using the same broad methods of analysis that we have already looked at, such as estimating population means/percentages, and linear and logistic regression.",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html#overview",
    "href": "complex-sampling-analysis.html#overview",
    "title": "Complex sample/survey design analysis",
    "section": "Overview",
    "text": "Overview\nComplex sample design data are datasets (i.e. sample) that have come from a probability sampling method that has any combination of the following three “complex design features”, with all three often being present together:\n\nStratification\nClustered sampling (often multi-stage clustered sampling)\nUnequal probabilities of selection for sampling units\n\nIt’s also often referred to as “complex survey design” data, but really a survey is not a study design and you could sample data for any study design using sampling methods with complex features like this, so the more general term would be complex sample design data.\nSuch data may have come from a single cross-sectional sample, multiple cross-sectional samples (i.e. multiple samples across time of different sampling units), or be true longitudinal data (i.e. multiple samples across time of the same sampling units).\nThis type of data is most commonly encountered/analysed in global health research when working with large-scale, typically national-scale, cross-sectional household surveys, such as the Demographic and Health Surveys (DHS: https://dhsprogram.com/), the Multiple Indicator Cluster Surveys (MICS: https://mics.unicef.org/), the WHO STEPwise Approach to surveillance Surveys (STEPS: https://www.who.int/ncds/surveillance/steps/en/), and many countries’ own household surveys. This is because large-scale household surveys almost always use all three of these key complex sampling design features of stratification, cluster sampling, and unequal probabilities of selection of sampling units. The reason they do this is usually for both pragmatic, logistical, and statistical/research reasons. Specifically, stratified sampling is often employed to both allow strata (typically an administrative level such as the region, often further stratified into urban and rural areas) to be independently sampled to ensure that each strata has a sufficient sample size to allow inferences to be made at the strata-level. Cluster sampling is primarily employed to reduce the logistical costs of sampling across huge geographical areas. While stratification and the multi-stage cluster sampling methods employed result in unequal probabilities of selection for sampling units (typically individuals).\nStandard methods of analysis, or more technically the maths underlying those methods, such as the independent t-test and linear regression, assume that the data being analysed come from a simple random sample. However, any of the complex sampling design features mentioned above result in a violation of that assumption, which if not adjusted for would result in either biased point estimates (e.g. biased regression coefficients) and/or biased standard errors of those point estimates (i.e. biased variance estimates), which means biased confidence intervals and p-values. More specifically, not adjusting for clustering typically results in falsely narrow confidence intervals/falsely small p-values, while not adjusting for stratification can mean the analysis “misses out” of the benefits of a stratified sample, which can reduce the overall variation in the outcome and produce more precise results for a given sample size. And not adjusting for unequal probabilities of selection of sampling units typically results in both biased/inaccurate point estimates of characteristics/associations and falsely narrow confidence intervals/falsely small p-values. Therefore, it is critical that we account or adjust for any complex sampling design features in our analyses to obtain unbiased results.\nLuckily there are modifications to all commonly used analytical methods, including t-tests and regression models (i.e. linear and logistic and others that we’ve not looked at). And also fortunately we don’t have to worry about the underlying maths. With statistical software like SPSS primarily all we have to do is tell the software which variables code for, or identify, the different complex design features, and then select the appropriate complex sampling design version of the analysis we want to do. Then we proceed with our analysis like we would if we were analysing data from a simple random sample, i.e. using the processes outlined in the earlier practical sessions.\nIn any high quality complex sampling dataset that used stratification, cluster sampling, and unequal probabilities of selection of sampling units in its sampling design, there will be a variable that indicates which strata each observation (sampling unit) comes from and a variable that indicates which cluster sampling unit each observation comes from. There will also be a third variable that contains the survey weights, which are used to adjust for the unequal probabilities of selection of sampling units.",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html#a-very-brief-description-of-survey-weights",
    "href": "complex-sampling-analysis.html#a-very-brief-description-of-survey-weights",
    "title": "Complex sample/survey design analysis",
    "section": "A very brief description of survey weights",
    "text": "A very brief description of survey weights\n\n\nRead/hide\n\nSampling with unequal probabilities of selection of sampling units results in unrepresentative samples (i.e. unrepresentative of the target population). This may occur when using stratified sampling with disproportionate strata, or when using a multi-stage cluster sampling approach. To obtain representative results we must therefore “map” the survey sample back onto the target population. To do this survey data producers calculate/produce sampling weights. In the analysis these weights upweight (or increase the contribution) of observations from sampling units that were undersampled compared to proportion of the target population that they represent, and they downweight (or decrease the contribution) of observations from sampling units that were oversampled compared to proportion of the target population that they represent. These weights are then usually combined with two other sets of weights. One that attempts to correct for any missing observations, and one that uses any pre-existing, recent, robust data on characteristics of the target population (such as a recent census) to adjust for any remaining lack of representativeness still reflected in the sample. Once all three types of weight are combined this results in a final or ultimate survey weight variable.\n\nTherefore, once we’ve identified these three variables and “told” SPSS which variables they are we can analyse our data and largely forget/ignore the fact that we are doing a complex sampling design version of our chosen analysis. The type of results we get and their interpretation will, broadly speaking, not change apart from in a few areas that we will look at in this practical session.",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html#further-reading",
    "href": "complex-sampling-analysis.html#further-reading",
    "title": "Complex sample/survey design analysis",
    "section": "Further reading",
    "text": "Further reading\n\n\nRead/hide\n\nUnfortunately, the best and most beginner-friendly textbook on sampling design data analysis, which also included examples using SPSS (as well as other statistical software) is now no longer available from the library (it was previously available online via the library). This book is: Heeringa, S., West, B.T., Berglund, P.A. (2017). Applied Survey Data Analysis (2nd ed.). Boca Raton, FL, USA: CRC Press. If you do plan on conducting sampling design analyses in your later careers I would strongly recommend buying the book, as it’s well worth it. The accompanying website also has lots of practice examples of analyses in SPSS and other statistical software. Unfortunately there are not any similar textbooks that I am aware of for sampling design analysis.\nHowever, there is a nice overview/summary of the main issues around sampling design data analysis in the following paper, by one of the authors of the book I’ve just mentioned, which is freely available here:\nhttps://pubmed.ncbi.nlm.nih.gov/18956450/",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html#preparing-for-sampling-design-data-analysis-a-checklist",
    "href": "complex-sampling-analysis.html#preparing-for-sampling-design-data-analysis-a-checklist",
    "title": "Complex sample/survey design analysis",
    "section": "Preparing for sampling design data analysis: a checklist",
    "text": "Preparing for sampling design data analysis: a checklist\nThe following is an excellent future reference checklist of steps to undertake when preparing to analyse sampling design data in a real analysis. It is taken from the Applied Survey Data Analysis textbook mentioned previously. For this practical we will assume we have carried out steps 1-3 and step 5 to save time, and some of the issues mentioned are beyond the scope of this introductory course/session, but if you were to do sampling design analysis for a real study or later in your career you should educate yourself enough to understand all these terms and the issues they refer to (e.g. by buying and reading the recommended textbook).\n\n\nRead/hide\n\n\nReview the documentation for the data set provided by the data producer, specifically focusing on sections discussing the development of the final survey weights and sampling error (standard error) estimation. Contact the data producer if any questions arise.\nIdentify the correct weight variable for the analysis, keeping in mind that many survey data sets include separate weights for different types of analyses. Perform simple descriptive analyses of the weight variable, noting the general distribution of the weights, whether the weights have been normalized, and whether there are missing or 0 weight values for some cases. Select a few key variables from the survey data set and compare weighted and unweighted estimates of descriptive parameters (e.g., means, proportions) for these variables to understand the effect the weights have.\nIdentify the variables in the data set containing the “sampling error calculation codes” that define the sampling error calculation model [JPH: these are just the variables defining the strata and clusters]. Examine how many clusters were selected from each sampling stratum (according to the sampling error calculation model), and whether particular clusters have small sample sizes. If only a single sampling error cluster is identified in a sampling stratum, contact the data producer or consult the documentation for the data set for guidance on recommended variance estimation methods. Determine whether replicate sampling weights are present if sampling error calculation codes are not available, and make sure that the statistical software is capable of accommodating replicate weights (Section 4.2.1).\nCreate a final analysis data set containing only the analysis variables of interest (including the survey weights, sampling error calculation variables, and case identifiers) [JPH: this is optional. It’s nice to keep your dataset tidy and no bigger than necessary for speed and manageability, but it obviously doesn’t affect the analysis in anyway]. Examine univariate and bivariate summaries for the key analysis variables to determine possible problems with missing data or unusual values on the individual variables.\nReview the documentation provided by the data producer to understand the procedure (typically nonresponse adjustment) used to address unit nonresponse or nonresponse to a wave or phase of the survey data collection. Analyse the rates and patterns of item missing data for all variables that will be included in the analysis. Investigate the potential missing data mechanism by defining indicator variables flagging missing data for the analysis variables of interest. Use statistical tests (e.g., chi-square tests, two-sample t-tests) to see if there are any systematic differences between respondents providing complete responses and respondents failing to provide complete responses on important analysis variables (e.g., demographics). Choose an appropriate strategy for addressing missing data using the guidance provided in Section 4.4 and Chapter 12 [JPH: we will not look at missing data issues in this practical, but most high quality sampling design datasets, such as the DHS surveys, include weights that incorporate adjustments for missing data that, while not perfect by any means, go some way to addressing bias from missing data by simply including the weights in the analysis as you should be doing anyway].\nDefine indicator variables for important analysis subclasses. Do not delete cases that are not a part of the primary analysis subclass. Assess a cross-tabulation of the stratum and cluster sampling error calculation codes for the subclass cases to identify the distribution of the subclass across the strata and clusters defined by the sampling error calculation model. Consult a survey statistician prior to analysis of subclasses that exhibit the “mixed class” characteristic illustrated in Figure 4.4. Make sure to employ appropriate software options for unconditional subclass analyses if using TSL for variance estimation [JPH: we will look at this issue in more detail later in this practical session].",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html#scenario",
    "href": "complex-sampling-analysis.html#scenario",
    "title": "Complex sample/survey design analysis",
    "section": "Scenario",
    "text": "Scenario\nYou have been tasked by your ministry of health to use your country’s recent DHS survey data to explore associations between socio-demographic characteristics and the time women report it takes them to collect water.",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html#exercise-1-describe-the-association-between-location-and-time-taken-to-collect-water",
    "href": "complex-sampling-analysis.html#exercise-1-describe-the-association-between-location-and-time-taken-to-collect-water",
    "title": "Complex sample/survey design analysis",
    "section": "Exercise 1: describe the association between location and time taken to collect water",
    "text": "Exercise 1: describe the association between location and time taken to collect water\nThe dataset we will use here is one of the DHS’s “model datasets”, and as such it was produced by the DHS for practicing analysis of DHS survey data. It is therefore representative of the types of data and data features (e.g. sampling design features) found in DHS surveys, but it doesn’t contain any real data from any country. DHS surveys typically result in the production of a number of datasets, primarily a household-level dataset and datasets for women, men, and children, plus potentially others. You can read more about DHS datasets here: https://dhsprogram.com/data/\nHowever, we will just use the “individual recode” dataset in the SPSS .sav format (DHS helpfully makes their datasets available in other formats too). This contains data from the female respondents of the survey, who consist of all members of a selected household who are aged 15-49. This dataset is included in the “Datasets” folder of the MSc & MPH computer sessions practical files, and is called “ZZIR62FL.SAV”. The “odd” naming is due to DHS’s naming conventions, which you can read about here: https://www.dhsprogram.com/data/File-Types-and-Names.cfm\nMore specifically, we will use the dataset to answer the descriptive research question “what factors are associated the time women report it takes someone from their household to reach the household’s source of drinking water?”\n\nStep 1: prepare and explore the data\nFirst of all we will identify, explore, and edit as necessary the key complex design variables of the survey. We will then explore our independent and outcome variables, and then produce descriptive statistics to describe the sample’s characteristics in terms of the variables measured, as would commonly be found in the “Table 1” of a paper. Then we will produce our analytical model results.\n\nLoad the “ZZIR62FL.sav” SPSS dataset.\n\nVideo instructions: prepare and explore the data\nWritten instructions: prepare and explore the data\n\n\nRead/hide\n\n\nBasic variable details obtained from the documentation describing the dataset\n\nV005 = Women’s individual sample weight (6 decimals) - numerical\nV021 = Primary sampling unit - nominal\nV023 = Stratification used in sample design - nominal\nV025 = Type of place of residence - nominal\nV115 = Time to get to water source - numerical (minutes)\nV152 = Age of household head - numerical (years)\nV190 = Wealth index - ordinal\n\n\n\nPrepare and explore the data\nAs per the checklist steps 1-3 for any sampling design analysis you must first thoroughly review the survey technical/methodological documentation to understand the design of the survey and sampling process, and to understand which variables contain the stratification, clustering, and weighting information, and to understand any modifications of those variables that are required. To save time we will assume we have done this, and that we have removed all variables from the dataset other than these design variables and the independent and outcome variables.\nTherefore, open the ZZIR62FL.SAV dataset. If a window appears with a message starting “IBM SPSS Statistics is running in Unicode encoding mode…” just click Yes. We can assume that from reading the relevant technical/methodological literature on this survey we have identified the relevant complex design variables and our variables of interest and removed all other variables from the dataset. The information on the complex design variables can be found in the “DHS Guide to Statistics” (https://dhsprogram.com/data/Guide-to-DHS-Statistics/Analyzing_DHS_Data.htm), but you should also review the survey specific literature (each DHS survey has a final report with specific methodological details relevant to that survey, along with a full copy of the questionnaire used that you should always review).\n\nExplore and edit the complex design variables\nTherefore, V005 identifies the survey weights, V021 identifies the clusters (often called primary sampling units as they are the first stage of a multi-stage cluster sample), and V023 identifies the strata. However, and this shows why you must read survey methodological literature carefully, all DHS weights are stored multiplied by 1,000,000. This is for very technical computer science related reasons about floating point errors that we don’t need to go into here, but the point is that to use the weight variable we therefore need to first divide all values by 1,000,000.\nTo do this from the main menu go: Transform &gt; Compute Variable. Then in the Target Variable: box give our new variable a suitable name, say “survey_weight”, and in the Numeric Expression: box enter the following:\n\nV005 / 1000000\n\nThen click OK. You can now delete the old V005 variable if you wish.\nBefore we go any further let’s rename the other variables for ease of reference. We’ll use the following names:\n\nV021 = cluster\nV023 = strata\nV025 = location\nV115 = time_water\nV152 = age_hhh\nV190 = wealth\n\nAnd let’s also change the variables cluster, strata, location, and wealth to the variable type nominal so SPSS treats them as categorical variables (as they use numerical coding SPSS automatically treats them as numerical variables unless told otherwise).\nNow let’s explore the complex design variables. For a quick overview of their descriptive statistics remember you can just right click on each variable in either the Data view or Variable View and click on Descriptive Statistics. However, you should first change the strata (V023) and cluster variables. As our strata and cluster variables are both categorical this is probably sufficient, but it’s worth producing a histogram for the numerical survey_weight variable to visualise its distribution. What do the descriptive statistics and histogram show you for the three complex design variables?\nWhat do the descriptive statistics and histogram show you for the three complex design variables?\n\n\nRead/hide\n\nThere are no missing values for any of the variables. The observations are typically evenly distributed across clusters, but much less evenly distributed across strata. However, there are no concerning issues, such as a single observation in a cluster or strata, that might indicate data errors. The histogram of the weights shows a strongly right-skewed distribution with some potentially concerning extremely high weight values. If this was a real analysis it would be worth looking into these more closely, but we’ll assume they’re all fine.\n\n\n\nExplore and edit the data variables\nNext let’s explore each of the data variables in turn. Again for a quick univariate exploration we can just use the same process as we used to explore the complex design variables.\nWhat do the descriptive statistics and histogram show you for the four data variables?\n\n\nRead/hide\n\nThere are no missing values for location, and there is a ~40%:60% split between urban and rural located households of the women surveyed. There are 13 missing values of our outcome variable time_water, and a histogram indicates that the variable appears to have a strongly “bimodal” distribution with two distinct “peaks”, but we’ll come back this shortly. There are 6 missing values of age_hhh, and a histogram indicates that the variable is fairly normally distributed. There are no missing values of wealth, and observations are evenly distributed across it’s five levels. This is to be expected because in DHS surveys the numerical household wealth index is categorised into five wealth quintiles by evenly separating increasing values of the variable into five levels/groups.\nSo back to the outcome variable. This is a good example of why a careful data exploration is important prior to any analysis, and why understanding your survey data and methodological documentation is so important. You can access the questionnaire that the model dataset is based on here: https://dhsprogram.com/pubs/pdf/DHSQ6/DHS6_Questionnaires_5Nov2012_DHSQ6.pdf\nScroll down to question 104 “How long does it take to go there, get water, and come back?”, which is where our variable comes from.\nWhat can you see about the possible response values?\n\n\nRead/hide\n\nRespondents can provide the number of minutes it takes them to go and collect water and come back, but for those who don’t know their response is coded as 998:, which is the second peak we see on the histogram. Therefore, the distribution is not bimodal at all, we’re just not looking at only the true/non-missing values.\n\nTherefore, let’s just look at the distribution of non-missing values for our outcome. Instead of deleting all observations where the response was “don’t know” to our question of interest we can tell SPSS to treat the value 998 as a missing value for this variable. In the Variable View for the time_water row click on the cell under the column Missing and click on the box with three dots in that appears to the right of the cell. In the Missing Values box that appears click the Discrete missing values button and in the first box enter 998 and then click OK.\nNow re-run the histogram and what do you see?\n\n\nRead/hide\n\nThe distribution of values is clearly strongly right-skewed. Remember that linear regression assumes that the residuals are approximately normally distributed not the raw values of the outcome, and therefore once we adjust for all our covariates our model may be valid. However, with such strong right-skew it looks likely that we’ll have to transform the outcome (another option would be to use a regression model for count data, such as the Poisson or negative binomial regression model, but that’s beyond this course).\n\n\n\n\nExplore bivariate associations\nLet’s look at the associations between our covariates and our outcome to check for any clearly non-linear associations. In a real analysis we would also want to check for the presence of strong interactions, ideally that we would expect from theory, but that is beyond the scope of this practical and course.\nAs previously for numerical covariates we use scatter plots.\n\nFrom the menu go: Graphs &gt; Scatter/Dot, then select Simple scatter option and click Define. Then add age_hhh and water_time to the appropriate X Axis: and Y Axis: boxes respectively and click OK.\n\nIt’s hard to see much of an obvious association there, and certainly no clear non-linear association.\nThen for the two categorical covariates again we can use box plots.\n\nFrom the main menu go: Graphs &gt; Boxplot, then select Simple and click Define. Add the water_time variable into the Variable: box and location into the Category Axis: box and click OK.\n\nIt’s also hard to see much of a clear association here, but the median time for urban looks slightly higher than the median time for rural, but clearly there’s a lot of variation in both category levels. If you repeat the graph but for wealth what do you see? Again, personally I can’t see any clear associations here.",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "complex-sampling-analysis.html#exercise-2-describe-the-mean-time-in-minutes-women-report-it-takes-them-to-collect-water",
    "href": "complex-sampling-analysis.html#exercise-2-describe-the-mean-time-in-minutes-women-report-it-takes-them-to-collect-water",
    "title": "Complex sample/survey design analysis",
    "section": "Exercise 2: describe the mean time (in minutes) women report it takes them to collect water",
    "text": "Exercise 2: describe the mean time (in minutes) women report it takes them to collect water\n\nUse the “Descriptives” tool within the “Complex Samples” tool to estimate the mean time women report it takes them to collect water. It should be now be pretty straight forward what to do, but make sure to click on the “Statistics” button to ensure you tell SPSS to calculate the necessary statistics to address this descriptive inferential question.\n\nWhat are plausible values given the confidence intervals of the estimate?\n\n\nRead/hide\n\nIt looks like the likely population mean is between 23.6 and 25.6 minutes. So we appear to have an extremely precise estimate of the population mean, assuming we have no/minimal bias in the study!",
    "crumbs": [
      "9. Complex sampling design analysis"
    ]
  },
  {
    "objectID": "chi-sq-goodness.html",
    "href": "chi-sq-goodness.html",
    "title": "The chi-square goodness-of-fit test",
    "section": "",
    "text": "The chi-square goodness-of-fit test\nThere is another chi-square based test usually called the chi-square goodness-of-fit test. We will not practice this test here as it’s far less commonly required/used than the chi-square independence test. In brief though, the chi-square goodness-of-fit test is a nonparametric test for single variables that is used to analyse whether the distribution of cases (units of observation) in a single categorical variable follows a known or hypothesised distribution. For example, whether the proportion of males and females in a sample follows the distribution of males and females in a given target population, or a distribution that is “hypothesised”, such as the proportion of males compared to females that we expect to have a certain characteristic/disease etc.\nA good overview of the test with instructions of how to use it in SPSS can be found here: https://statistics.laerd.com/spss-tutorials/chi-square-goodness-of-fit-test-in-spss-statistics.php",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.2. Chi-square goodness of fit test"
    ]
  },
  {
    "objectID": "chi-sq-independence.html",
    "href": "chi-sq-independence.html",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "While the independent and paired t-tests allow us to analyse the association between a numerical outcome and a binary covariate, the chi-square (or chi-square) test of independence allows us to analyse whether there is an association between a categorical outcome (with any number of category levels) and a categorical covariate (again with any number of category levels), in terms of whether the relative frequency of observations across category levels in the outcome differs depending on the category level of the covariate. It’s probably called the chi-square test of independence because it uses the chi-square distribution and the inferential part of the test is a hypothesis test where the null hypothesis is that no association exists between the outcome and covariate, i.e. that they are independent.\nFor example, assume we are interested in whether there is an association between hypertension (a binary outcome) and socio-economic status, defined as having three levels: low, medium and high. Here for our purposes of illustration we don’t define these levels further, but in a real study they would be based on some, usually multivariate, computation/analysis. Statistically, we would be interested in whether the relative frequency (i.e. proportion/percentage) of hypertension differed depending on whether individuals were classed as being of low, medium, or high socio-economic status. This type of analysis could then be done using a chi-square test of independence (also called Pearson’s chi-square test after the inventor of the method, or the chi-square test of association), but note that the two categorical variables can have any number of levels.\nThe chi-square test of independence is still quite popular for such situations, which is why we are covering it, but it does have some big limitations and we’ll see later how to use a much more useful approach is logistic regression. Note: although the p-value calculated for the test is based on the chi-square distribution the test itself does not assume the underlying data follow any specific distribution and so it is technically a non-parametric test.\n\n\n\nUsing the “SBP data final.sav” dataset we’ll analyse whether there is an association between individuals’ hypertension status (hypertension/no hypertension), based on having a systolic blood pressure &lt;140 mmHg or ≥140 mmHg, and their sex (asumed to just be male/female), and then you can try a similar analysis by yourself.\n\nLoad the “SBP data final.sav” SPSS dataset.\n\n\n\n\n1. Both the outcome and the covariate should be categorical (either ordinal or nominal), each having at least two levels\nThis is clearly the case.\n2. Observations should be independent\nAs with the independent t-test observations should not be related/correlated, such as would be the case if observations are collected from entities within clusters, such individuals clustered within families, health facilities or other institutions etc. Again, the only way to verify this is from a full understanding of the study design and data collection processes. Here we can assume this holds because the data come from a simple random survey of separate individuals.\n3. Ideally no cell has an expected frequency less than 5 and no cells have expected values of 0\nWe can only check this assumption from the results once we’ve run the test. As we’ll see below cells are just the cross-classified category levels between the outcome and covariate. For example, if the outcome is hypertension (hypertension/no hypertension) and the covariate is sex (female/male) then you would have four cells: yes-female, yes-male, no-female, and no-male. We’ll also see below what the “expected” values are.\nNote: this is not really a formal rule and other researchers provide other thresholds such as no expected frequency less than 10. However, when any expected cell frequency is less than 5, particularly when there are few category levels in both outcome and covariate (e.g. with a “2 x 2 contingency table” where both variables just have two levels), some researchers recommendation applying “Yates’s correction” (named after its creator) that aims at correcting the error introduced by assuming that the discrete probabilities of frequencies in the table can be approximated by a continuous distribution (chi-square). However, this has been shown to result in overly conservative hypothesis tests and so other researchers prefer to use Fisher’s Exact test, which is what we will mention here later. Remember you can always do a sensitivity analysis: run both versions and present both and interpret in terms of any differences.\nHowever, if any cells have expected values of 0 you must merge the relevant category level in the covariate with one or more (logically chosen) category levels to ensure no expected values are 0. For example, if our outcome was hypertension status (hypertension/no hypertension) and our covariate was socio-economic status (low/medium/high) and we found that our “hypertension status = no, socio-economic status = low” cell had an expected frequency of 0 you could merge the low socio-economic status level with the medium socio-economic status level and see if that solved the problem.\n\n\n\nVideo instructions: run the chi-square test of independence\nWritten instructions: run the chi-square test of independence\n\n\nRead/hide\n\n\nLet’s run the test. First, load the “SBP data final.sav” dataset. From the main menu go: Analyze &gt; Descriptive Statistics &gt; Crosstabs. Then in the Crosstabs tool add the htn variable to the Row(s): box and the sex variable to the Column(s): box. Note: you can put either variable in either box and it doesn’t make any difference to the actual chi-square test of independence p-value, which is the only inferential statistic we get, but the cross-tabulated frequencies and percentages will just be presented “the other way around” from how I will produce and describe them, and the way I choose seems most logical to me, but you may disagree. Then click the Statistics button and tick the Chi-square box at the top-left, and then also tick the Phi and Cramer’s V box under Nominal (we’ll explain what these are later), then click Continue.\nLet’s also edit the options for the descriptive tables that SPSS will produce, so we can see the observed and expected counts and frequencies (as percentages). Click the Cells button and under Counts ensure Observed and Expected are ticked, and under Percentages ensure Row, Column and Total are ticked.\nLet’s also produce some clustered bar charts to help us interpret the results visually, which SPSS helpful allows us to do via the tool by ticking the Display clustered bar charts box.\nFinally click OK.\n\n\n\n\n\n\nIn the output window you’ll see four tables and a bar chart. The first table titled “Case Processing Summary” just tells us how many “cases” were included or excluded in the analysis. A case represents an observation, which in our dataset represents a separate participant who had data on both categorical variables. So for example if a participant lacked data for either their hypertension status or their sex (or both) they would be excluded from the analysis. We can see none (0%) of the observations (participants) had to be excluded due to missing values.\nWe will skip the second table for now and next look at the third table down titled “Chi-Square tests”. This primarily gives us the inferential result for our chi-square test of independence, which is just in the form of a p-value from a hypothesis test that you interpret along side the point estimates of the outcome frequencies in each group of the covariate. Note: in addition to the chi-square test of independence we also get (without asking) results from some other related tests that we won’t consider further, and in the final row of the table we get the total number of valid (i.e. non-missing) cases again. Just look at the top row (Pearson Chi-Square) for the results from our chi-square test of independence.\n\nWhat does it all mean?\n\n\nChi-Square tests section columns explained\n\n\nValue\n\nThis gives us the test statistic which is the chi-square (χ2) test statistic. It is used to compute the p-value, but you can ignore the value of the test statistic itself.\n\ndf\n\nThis is the degrees of freedom (df) for the test/p-value. It represents a measure of how many pieces of statistical information (observations) were freely available for the test. You can ignore this.\n\nAsymp. Sig. (2-sided)\n\nThis is our chi-square test of independence two-sided p-value. “Asymp. Sig. (2-sided)” stands for asymptotic significance (2-sided). This two-sided (or two-tailed) part means the p-value is computed on the assumption that the association between the outcome and covariate could be in either direction, while the “asymp.” part means that the rest assume an infinite sample size. The infinite sample size assumption is obviously not true but may be a reasonable and useful approximation. What represents a “reasonable sample size” isn’t easy to work out, but rough rules of thumb (a “rule of thumb” is an English language saying for a rough guide) usually say at least &gt;30 observations and no cell with &lt;5 observations.\nAs you can see our chi-square test of independence p-value is 0.000 (i.e. &lt;0.001 as if you remember SPSS does not give exact p-values once P&lt;0.001). This is lower than the standard 0.05 threshold for claiming “statistical significance”.\n\nWe can actually ignore the remaining two columns: “Exact Sig. (2-sided)” and “Exact Sig. (1-sided)”. This is because as they don’t apply to our test, only if you were interested in the results of “Fishers Exact test”.\nNote: see the footnotes to the table. One reads “0 cells (0.0%) have expected counts less than 5…”, which means that this assumption of the test was not violated and so we can interpret our results.\n\n\nNow let’s come back to the second table titled “Hypertension (SBP &gt;= 140 mmHg) * Sex (male/female) Crosstabulation” where this information comes from and understand it.\n\n\nHypertension (SBP &gt;= 140 mmHg) * Sex (male/female) Crosstabulation table explained\n\n\nThis is a cross tabulation showing us a range of statistics for each cell, where each cell is defined by one of the unique combinations of the two categorical variables’ levels (i.e. with two binary categorical variables we have 2x2 = 4 cells: htn = yes and sex = male, htn = yes and sex = female, htn = no and sex = male, and htn = no and sex = female). In addition the total value of each statistic is presented within each level of each categorical variable across the levels of the other categorical variable (e.g. the total for htn = no across both sex = male and sex = female). Within each cell we have the following statistics/values:\nCount\n\nThe number of observations (participants) in each cell.\n\nExpected Count\n\nThe expected number of observations calculated using the formula: (row total x column total) / overall total. This calculated the expected number of observations per cell if there was no association between the two variables.\n\n% within Hypertension (SBP &gt;= 140 mmHg)\n\nThis is the % of observations (relative frequencies) within each level of hypertension for the relevant level of sex.\n\n% within Sex (male/female)\n\nThis is the % of observations (relative frequencies) within each level of sex for the relevant level of hypertension.\n\n% of Total\n\nThis is the % of observations (relative frequencies) within each cell out of the total number of observations.\n\n\n\nSo it’s quite a complicated table to say the least to look at initially, but it’s crucial for interpreting the results of the test fully. In particular we need to look at the relative frequencies within each level of one of the categorical variables for the levels of the other variable to understand the strength and direction of possible associations between the variables, as reported in the example results reporting text above. These are the “% within Hypertension (SBP &gt;= 140 mmHg)” and “% within Sex (male/female)” values. Let’s work through both sets of relative frequencies. For ease of understanding we have reproduced the table below. If yours looks different then it’s not necessarily wrong but you may have put the variables in a different way around, so you may wish to go back and follow the instructions above more carefully to recreate the results as they look below.\n\n\n\nChi-square test of independence crosstabs table\n\n\nBecause of the way the table is laid out (with sex along the top and htn down the side) it makes most sense to look at the results from the “point of view” of the htn variable, looking at the relative frequency of men and women for each level of htn. So to do this with the above table we are looking across the rows at the “% within Hypertension…” values. First look at the row outlined in solid red for htn = no. This tells us that out of the individuals who were htn = no 47.9% were men and 52.1% were women. Next look at the row outlined in dashed red for htn = yes. This tells us that out of the individuals who were htn = yes 69.9% were men and 30.1% were women. Therefore, there appears to be a strong association between hypertension status and sex, with men being more likely to be hypertensive than women, i.e. as we “move” from one level of htn to the other the relative frequency of sex varies substantially from roughly 50%:50% to roughly 70%:30%. We can also look at these results from the “point of view” of the sex variable, but now because of the way the table is laid out it makes more sense to look down the sex level columns or reproduce the table with the variables the other way around. Look at the values outlined in solid blue in the male column. These show us that of the individuals who were men 67.9% were not hypertensive and 32.1% were. Finally look at the values outlined in dashed blue in the female column. These show us that of the individuals who were female 84.2% were not hypertensive and 15.8% were. So again it indicates the same direction and strength of association, because it’s the same data, but looking from “the point of view” of the other variable. I find looking from the “point of view” of the assumed outcome variable more logical and intuitive (i.e. the first “view”) because it’s the assume causal relationship, but you may disagree.\nLastly we come to the fourth table “Symmetric Measures”. This presents the Phi and Cramer’s V statistics and their associated p-values. The statistics both measure the strength of the association between the two variables in the analysis, and both go from 0 (no association) to 1 (“perfectly” associated). The associated p-value shows you, assuming the Phi/Cramer’s V value is 0, how likely you were to observe a value at least as great as the one observed due to sampling error alone. These statistics can help you interpret how strong any association is, but they are difficult to interpret. When both variables have two levels then they are equivalent in interpretation to a linear correlation coefficient with a simple and useful interpretation (the strength of association on a 0-1 or 0-100% scale), but when there are more than two levels in one or both variables there’s no clear intuitive interpretation!\nTherefore, I would recommend firstly looking at the p-value to see if there is any reasonable evidence for an overall association between your two categorical variables (i.e. based on the standard P ≤ 0.05 threshold), and the looking at the relative frequencies (% of observations within different levels of one variable for each level of the other) as we have done above to judge where any associations might exist, in what direction and how strong they appear to be.\n\n\n\nIn our methods section we would explain that we used a chi-square test of independence and justify why we did so. Then in a results section we could say something like:\n\nAmong males 67.9% (201/296) of individuals had hypertension (systolic blood pressure ≥140 mmHg) while among females 84.2% (219/260) of individuals had hypertension. A chi-square test of independence showed that this represented a statistically significant association between hypertension and sex (P&lt;0.001).\n\nThis allows readers to understand the direction and possible size of associations between the relative outcome frequencies in each of the covariate’s groups, and the associated inferential hypothesis test p-value result, which loosely speaking gives us a sense of how confident we can be in concluding that the exact observed association in the sample represents the association that exists in the target population.\n\n\n\nThe main limitation of the chi-square test of independence is that we do not get any confidence intervals on our raw measures of effect size: the relative frequencies (i.e. the raw measure of the direction and strength of the association). We only get an overall p-value that tells us, assuming there is no association between the two categorical variables, how likely we are to have observed an association (in either direction) at least as great as the one we have observed due to sampling error alone. So all it can indicate is if there is likely to be an overall association between the two variables given the uncertainty in the data. It tells us nothing about the direction or size of any association, or which levels it involves.\nWhen both variables have two levels, like with our example, we know any association must be between both levels of each variable. However, when there are more than two levels in one or both variables it’s even less clear. For example, assume we looked at the association between the htn and ses variables. If we found the p-value for a chi-square test of independence was &lt; 0.05 we could conclude there was evidence for an overall association. However, we could get a significant p-value if the relative frequency of hypertension was similar for two levels of ses and only differed for the remaining level, or we could get a significant p-value if the relative frequency of hypertension was different among all three levels of ses. Therefore, all we can do is conclude whether there is evidence for an overall association based on the p-value, and then cautiously interpret the relative frequencies (measures of effect) to see which levels, what direction and what size of associations appear to exist. As you can see this is much, much less robust and satisfactory than having confidence intervals for our raw measures of effect size.\n\n\n\nWhat if one or more of our expected cell counts are &lt;5? Then we can use a similar test that can deal with this problem (but is typically less powerful when it’s not an issue) called the Fisher’s Exact test. To include this test in our chi-square results table when we go to the Crosstabs tool add in the variables as above but now click the Exact button and then tick the Exact button (you can leave the Time limit per test as it is). Then click OK and in the Chi-Square Tests table you will see an additional row called Fisher’s Exact Test. Again all we get is a two-sided p-value (“Exact Sig. (2-sided)”) based on the test statistic (“Value”) given the degrees of freedom (“df”), which as always just tells us, assuming the null hypothesis is true, how likely we were to have observed data reflecting an association that differed from the null hypothesis at least as greatly as that observed.\n\n\n\nUsing the “SBP final data.sav” dataset and via the process outlined above use a chi-square test of independence to analyse the association between BMI and ACE inhibitor usage during the past three months, with the hypothetical research question being is BMI related to ACE inhibitor usage?\n\nFirst convert the BMI variable into a binary variable based on BMI values &lt;30 kg/m² and those ≥ 30 kg/m².\nRun the chi-square test of independence using the new categorical variable BMI (&lt;30 kg/m² or ≥ 30 kg/m²) and ace.\nHint: enter your new categorical BMI variable into the Row(s): box in the Crosstabs tool and then in the Crosstabulation table look across the top and bottom halves of the table. For example, the top half of the table will, depending on how you’ve coded your BMI categorical variable, include the count (frequency) and % of individuals who reported (yes) and did not report (no) using ACE inhibitors during the past month within the relevant BMI group. You can then compare the frequency and % of individuals reporting usage between the two BMI groups, along with the chi-square p-value result in the following table.\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Chi-square test of independence: the relationship between BMI and ACE inhibitor usage.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: overall sample size and frequency and proportion/percentage of individuals with low and high BMI who reported using ACE inhibitors during the past three months. Also be sure to include sufficient details about the type of analysis used, and of course the key inferential result. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\nExample results reporting text\n\n\nRead/hide\n\nI analysed the association between BMI, defined/grouped as &lt;30 kg/m² or ≥30 kg/m², and ACE inhibitor usage during the past three months. Out of a total sample size of 529 individuals, among individuals with reported BMI values &lt;30 kg/m² 4.7% (20/429) reported using ACE inhibitors during the past three months while among individuals with reported BMI values ≥ 30 kg/m² 7% (7/100) reported using ACE inhibitors during the past three months, but this difference was not statistically significant (P = 0.3) based on a chi-square test of independence. Therefore, there was no clear evidence for an association between BMI when grouped as &lt;30 kg/m² or ≥30 kg/m² and ACE inhibitor usage frequency during the past three months in this target population. However, the key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an observational cross-sectional study like this). Therefore, this is likely to represent a biased estimate of the independent association between BMI (as defined/grouped here) and ACE inhibitor usage within the past three months in this target population.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#overview",
    "href": "chi-sq-independence.html#overview",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "While the independent and paired t-tests allow us to analyse the association between a numerical outcome and a binary covariate, the chi-square (or chi-square) test of independence allows us to analyse whether there is an association between a categorical outcome (with any number of category levels) and a categorical covariate (again with any number of category levels), in terms of whether the relative frequency of observations across category levels in the outcome differs depending on the category level of the covariate. It’s probably called the chi-square test of independence because it uses the chi-square distribution and the inferential part of the test is a hypothesis test where the null hypothesis is that no association exists between the outcome and covariate, i.e. that they are independent.\nFor example, assume we are interested in whether there is an association between hypertension (a binary outcome) and socio-economic status, defined as having three levels: low, medium and high. Here for our purposes of illustration we don’t define these levels further, but in a real study they would be based on some, usually multivariate, computation/analysis. Statistically, we would be interested in whether the relative frequency (i.e. proportion/percentage) of hypertension differed depending on whether individuals were classed as being of low, medium, or high socio-economic status. This type of analysis could then be done using a chi-square test of independence (also called Pearson’s chi-square test after the inventor of the method, or the chi-square test of association), but note that the two categorical variables can have any number of levels.\nThe chi-square test of independence is still quite popular for such situations, which is why we are covering it, but it does have some big limitations and we’ll see later how to use a much more useful approach is logistic regression. Note: although the p-value calculated for the test is based on the chi-square distribution the test itself does not assume the underlying data follow any specific distribution and so it is technically a non-parametric test.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#scenario",
    "href": "chi-sq-independence.html#scenario",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "Using the “SBP data final.sav” dataset we’ll analyse whether there is an association between individuals’ hypertension status (hypertension/no hypertension), based on having a systolic blood pressure &lt;140 mmHg or ≥140 mmHg, and their sex (asumed to just be male/female), and then you can try a similar analysis by yourself.\n\nLoad the “SBP data final.sav” SPSS dataset.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#step-1-check-the-assumptions-of-the-chi-square-test-of-independence",
    "href": "chi-sq-independence.html#step-1-check-the-assumptions-of-the-chi-square-test-of-independence",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "1. Both the outcome and the covariate should be categorical (either ordinal or nominal), each having at least two levels\nThis is clearly the case.\n2. Observations should be independent\nAs with the independent t-test observations should not be related/correlated, such as would be the case if observations are collected from entities within clusters, such individuals clustered within families, health facilities or other institutions etc. Again, the only way to verify this is from a full understanding of the study design and data collection processes. Here we can assume this holds because the data come from a simple random survey of separate individuals.\n3. Ideally no cell has an expected frequency less than 5 and no cells have expected values of 0\nWe can only check this assumption from the results once we’ve run the test. As we’ll see below cells are just the cross-classified category levels between the outcome and covariate. For example, if the outcome is hypertension (hypertension/no hypertension) and the covariate is sex (female/male) then you would have four cells: yes-female, yes-male, no-female, and no-male. We’ll also see below what the “expected” values are.\nNote: this is not really a formal rule and other researchers provide other thresholds such as no expected frequency less than 10. However, when any expected cell frequency is less than 5, particularly when there are few category levels in both outcome and covariate (e.g. with a “2 x 2 contingency table” where both variables just have two levels), some researchers recommendation applying “Yates’s correction” (named after its creator) that aims at correcting the error introduced by assuming that the discrete probabilities of frequencies in the table can be approximated by a continuous distribution (chi-square). However, this has been shown to result in overly conservative hypothesis tests and so other researchers prefer to use Fisher’s Exact test, which is what we will mention here later. Remember you can always do a sensitivity analysis: run both versions and present both and interpret in terms of any differences.\nHowever, if any cells have expected values of 0 you must merge the relevant category level in the covariate with one or more (logically chosen) category levels to ensure no expected values are 0. For example, if our outcome was hypertension status (hypertension/no hypertension) and our covariate was socio-economic status (low/medium/high) and we found that our “hypertension status = no, socio-economic status = low” cell had an expected frequency of 0 you could merge the low socio-economic status level with the medium socio-economic status level and see if that solved the problem.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#step-2-run-the-chi-square-test-of-independence",
    "href": "chi-sq-independence.html#step-2-run-the-chi-square-test-of-independence",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "Video instructions: run the chi-square test of independence\nWritten instructions: run the chi-square test of independence\n\n\nRead/hide\n\n\nLet’s run the test. First, load the “SBP data final.sav” dataset. From the main menu go: Analyze &gt; Descriptive Statistics &gt; Crosstabs. Then in the Crosstabs tool add the htn variable to the Row(s): box and the sex variable to the Column(s): box. Note: you can put either variable in either box and it doesn’t make any difference to the actual chi-square test of independence p-value, which is the only inferential statistic we get, but the cross-tabulated frequencies and percentages will just be presented “the other way around” from how I will produce and describe them, and the way I choose seems most logical to me, but you may disagree. Then click the Statistics button and tick the Chi-square box at the top-left, and then also tick the Phi and Cramer’s V box under Nominal (we’ll explain what these are later), then click Continue.\nLet’s also edit the options for the descriptive tables that SPSS will produce, so we can see the observed and expected counts and frequencies (as percentages). Click the Cells button and under Counts ensure Observed and Expected are ticked, and under Percentages ensure Row, Column and Total are ticked.\nLet’s also produce some clustered bar charts to help us interpret the results visually, which SPSS helpful allows us to do via the tool by ticking the Display clustered bar charts box.\nFinally click OK.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#step-3-understand-the-results-tables-and-extract-the-key-results",
    "href": "chi-sq-independence.html#step-3-understand-the-results-tables-and-extract-the-key-results",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "In the output window you’ll see four tables and a bar chart. The first table titled “Case Processing Summary” just tells us how many “cases” were included or excluded in the analysis. A case represents an observation, which in our dataset represents a separate participant who had data on both categorical variables. So for example if a participant lacked data for either their hypertension status or their sex (or both) they would be excluded from the analysis. We can see none (0%) of the observations (participants) had to be excluded due to missing values.\nWe will skip the second table for now and next look at the third table down titled “Chi-Square tests”. This primarily gives us the inferential result for our chi-square test of independence, which is just in the form of a p-value from a hypothesis test that you interpret along side the point estimates of the outcome frequencies in each group of the covariate. Note: in addition to the chi-square test of independence we also get (without asking) results from some other related tests that we won’t consider further, and in the final row of the table we get the total number of valid (i.e. non-missing) cases again. Just look at the top row (Pearson Chi-Square) for the results from our chi-square test of independence.\n\nWhat does it all mean?\n\n\nChi-Square tests section columns explained\n\n\nValue\n\nThis gives us the test statistic which is the chi-square (χ2) test statistic. It is used to compute the p-value, but you can ignore the value of the test statistic itself.\n\ndf\n\nThis is the degrees of freedom (df) for the test/p-value. It represents a measure of how many pieces of statistical information (observations) were freely available for the test. You can ignore this.\n\nAsymp. Sig. (2-sided)\n\nThis is our chi-square test of independence two-sided p-value. “Asymp. Sig. (2-sided)” stands for asymptotic significance (2-sided). This two-sided (or two-tailed) part means the p-value is computed on the assumption that the association between the outcome and covariate could be in either direction, while the “asymp.” part means that the rest assume an infinite sample size. The infinite sample size assumption is obviously not true but may be a reasonable and useful approximation. What represents a “reasonable sample size” isn’t easy to work out, but rough rules of thumb (a “rule of thumb” is an English language saying for a rough guide) usually say at least &gt;30 observations and no cell with &lt;5 observations.\nAs you can see our chi-square test of independence p-value is 0.000 (i.e. &lt;0.001 as if you remember SPSS does not give exact p-values once P&lt;0.001). This is lower than the standard 0.05 threshold for claiming “statistical significance”.\n\nWe can actually ignore the remaining two columns: “Exact Sig. (2-sided)” and “Exact Sig. (1-sided)”. This is because as they don’t apply to our test, only if you were interested in the results of “Fishers Exact test”.\nNote: see the footnotes to the table. One reads “0 cells (0.0%) have expected counts less than 5…”, which means that this assumption of the test was not violated and so we can interpret our results.\n\n\nNow let’s come back to the second table titled “Hypertension (SBP &gt;= 140 mmHg) * Sex (male/female) Crosstabulation” where this information comes from and understand it.\n\n\nHypertension (SBP &gt;= 140 mmHg) * Sex (male/female) Crosstabulation table explained\n\n\nThis is a cross tabulation showing us a range of statistics for each cell, where each cell is defined by one of the unique combinations of the two categorical variables’ levels (i.e. with two binary categorical variables we have 2x2 = 4 cells: htn = yes and sex = male, htn = yes and sex = female, htn = no and sex = male, and htn = no and sex = female). In addition the total value of each statistic is presented within each level of each categorical variable across the levels of the other categorical variable (e.g. the total for htn = no across both sex = male and sex = female). Within each cell we have the following statistics/values:\nCount\n\nThe number of observations (participants) in each cell.\n\nExpected Count\n\nThe expected number of observations calculated using the formula: (row total x column total) / overall total. This calculated the expected number of observations per cell if there was no association between the two variables.\n\n% within Hypertension (SBP &gt;= 140 mmHg)\n\nThis is the % of observations (relative frequencies) within each level of hypertension for the relevant level of sex.\n\n% within Sex (male/female)\n\nThis is the % of observations (relative frequencies) within each level of sex for the relevant level of hypertension.\n\n% of Total\n\nThis is the % of observations (relative frequencies) within each cell out of the total number of observations.\n\n\n\nSo it’s quite a complicated table to say the least to look at initially, but it’s crucial for interpreting the results of the test fully. In particular we need to look at the relative frequencies within each level of one of the categorical variables for the levels of the other variable to understand the strength and direction of possible associations between the variables, as reported in the example results reporting text above. These are the “% within Hypertension (SBP &gt;= 140 mmHg)” and “% within Sex (male/female)” values. Let’s work through both sets of relative frequencies. For ease of understanding we have reproduced the table below. If yours looks different then it’s not necessarily wrong but you may have put the variables in a different way around, so you may wish to go back and follow the instructions above more carefully to recreate the results as they look below.\n\n\n\nChi-square test of independence crosstabs table\n\n\nBecause of the way the table is laid out (with sex along the top and htn down the side) it makes most sense to look at the results from the “point of view” of the htn variable, looking at the relative frequency of men and women for each level of htn. So to do this with the above table we are looking across the rows at the “% within Hypertension…” values. First look at the row outlined in solid red for htn = no. This tells us that out of the individuals who were htn = no 47.9% were men and 52.1% were women. Next look at the row outlined in dashed red for htn = yes. This tells us that out of the individuals who were htn = yes 69.9% were men and 30.1% were women. Therefore, there appears to be a strong association between hypertension status and sex, with men being more likely to be hypertensive than women, i.e. as we “move” from one level of htn to the other the relative frequency of sex varies substantially from roughly 50%:50% to roughly 70%:30%. We can also look at these results from the “point of view” of the sex variable, but now because of the way the table is laid out it makes more sense to look down the sex level columns or reproduce the table with the variables the other way around. Look at the values outlined in solid blue in the male column. These show us that of the individuals who were men 67.9% were not hypertensive and 32.1% were. Finally look at the values outlined in dashed blue in the female column. These show us that of the individuals who were female 84.2% were not hypertensive and 15.8% were. So again it indicates the same direction and strength of association, because it’s the same data, but looking from “the point of view” of the other variable. I find looking from the “point of view” of the assumed outcome variable more logical and intuitive (i.e. the first “view”) because it’s the assume causal relationship, but you may disagree.\nLastly we come to the fourth table “Symmetric Measures”. This presents the Phi and Cramer’s V statistics and their associated p-values. The statistics both measure the strength of the association between the two variables in the analysis, and both go from 0 (no association) to 1 (“perfectly” associated). The associated p-value shows you, assuming the Phi/Cramer’s V value is 0, how likely you were to observe a value at least as great as the one observed due to sampling error alone. These statistics can help you interpret how strong any association is, but they are difficult to interpret. When both variables have two levels then they are equivalent in interpretation to a linear correlation coefficient with a simple and useful interpretation (the strength of association on a 0-1 or 0-100% scale), but when there are more than two levels in one or both variables there’s no clear intuitive interpretation!\nTherefore, I would recommend firstly looking at the p-value to see if there is any reasonable evidence for an overall association between your two categorical variables (i.e. based on the standard P ≤ 0.05 threshold), and the looking at the relative frequencies (% of observations within different levels of one variable for each level of the other) as we have done above to judge where any associations might exist, in what direction and how strong they appear to be.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#step-4-report-and-interpret-the-results",
    "href": "chi-sq-independence.html#step-4-report-and-interpret-the-results",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "In our methods section we would explain that we used a chi-square test of independence and justify why we did so. Then in a results section we could say something like:\n\nAmong males 67.9% (201/296) of individuals had hypertension (systolic blood pressure ≥140 mmHg) while among females 84.2% (219/260) of individuals had hypertension. A chi-square test of independence showed that this represented a statistically significant association between hypertension and sex (P&lt;0.001).\n\nThis allows readers to understand the direction and possible size of associations between the relative outcome frequencies in each of the covariate’s groups, and the associated inferential hypothesis test p-value result, which loosely speaking gives us a sense of how confident we can be in concluding that the exact observed association in the sample represents the association that exists in the target population.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#limitations",
    "href": "chi-sq-independence.html#limitations",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "The main limitation of the chi-square test of independence is that we do not get any confidence intervals on our raw measures of effect size: the relative frequencies (i.e. the raw measure of the direction and strength of the association). We only get an overall p-value that tells us, assuming there is no association between the two categorical variables, how likely we are to have observed an association (in either direction) at least as great as the one we have observed due to sampling error alone. So all it can indicate is if there is likely to be an overall association between the two variables given the uncertainty in the data. It tells us nothing about the direction or size of any association, or which levels it involves.\nWhen both variables have two levels, like with our example, we know any association must be between both levels of each variable. However, when there are more than two levels in one or both variables it’s even less clear. For example, assume we looked at the association between the htn and ses variables. If we found the p-value for a chi-square test of independence was &lt; 0.05 we could conclude there was evidence for an overall association. However, we could get a significant p-value if the relative frequency of hypertension was similar for two levels of ses and only differed for the remaining level, or we could get a significant p-value if the relative frequency of hypertension was different among all three levels of ses. Therefore, all we can do is conclude whether there is evidence for an overall association based on the p-value, and then cautiously interpret the relative frequencies (measures of effect) to see which levels, what direction and what size of associations appear to exist. As you can see this is much, much less robust and satisfactory than having confidence intervals for our raw measures of effect size.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#expected-cell-counts-5",
    "href": "chi-sq-independence.html#expected-cell-counts-5",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "What if one or more of our expected cell counts are &lt;5? Then we can use a similar test that can deal with this problem (but is typically less powerful when it’s not an issue) called the Fisher’s Exact test. To include this test in our chi-square results table when we go to the Crosstabs tool add in the variables as above but now click the Exact button and then tick the Exact button (you can leave the Time limit per test as it is). Then click OK and in the Chi-Square Tests table you will see an additional row called Fisher’s Exact Test. Again all we get is a two-sided p-value (“Exact Sig. (2-sided)”) based on the test statistic (“Value”) given the degrees of freedom (“df”), which as always just tells us, assuming the null hypothesis is true, how likely we were to have observed data reflecting an association that differed from the null hypothesis at least as greatly as that observed.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "chi-sq-independence.html#exercise-chi-square-test-of-independence",
    "href": "chi-sq-independence.html#exercise-chi-square-test-of-independence",
    "title": "The chi-square test of independence",
    "section": "",
    "text": "Using the “SBP final data.sav” dataset and via the process outlined above use a chi-square test of independence to analyse the association between BMI and ACE inhibitor usage during the past three months, with the hypothetical research question being is BMI related to ACE inhibitor usage?\n\nFirst convert the BMI variable into a binary variable based on BMI values &lt;30 kg/m² and those ≥ 30 kg/m².\nRun the chi-square test of independence using the new categorical variable BMI (&lt;30 kg/m² or ≥ 30 kg/m²) and ace.\nHint: enter your new categorical BMI variable into the Row(s): box in the Crosstabs tool and then in the Crosstabulation table look across the top and bottom halves of the table. For example, the top half of the table will, depending on how you’ve coded your BMI categorical variable, include the count (frequency) and % of individuals who reported (yes) and did not report (no) using ACE inhibitors during the past month within the relevant BMI group. You can then compare the frequency and % of individuals reporting usage between the two BMI groups, along with the chi-square p-value result in the following table.\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Chi-square test of independence: the relationship between BMI and ACE inhibitor usage.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: overall sample size and frequency and proportion/percentage of individuals with low and high BMI who reported using ACE inhibitors during the past three months. Also be sure to include sufficient details about the type of analysis used, and of course the key inferential result. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\nExample results reporting text\n\n\nRead/hide\n\nI analysed the association between BMI, defined/grouped as &lt;30 kg/m² or ≥30 kg/m², and ACE inhibitor usage during the past three months. Out of a total sample size of 529 individuals, among individuals with reported BMI values &lt;30 kg/m² 4.7% (20/429) reported using ACE inhibitors during the past three months while among individuals with reported BMI values ≥ 30 kg/m² 7% (7/100) reported using ACE inhibitors during the past three months, but this difference was not statistically significant (P = 0.3) based on a chi-square test of independence. Therefore, there was no clear evidence for an association between BMI when grouped as &lt;30 kg/m² or ≥30 kg/m² and ACE inhibitor usage frequency during the past three months in this target population. However, the key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an observational cross-sectional study like this). Therefore, this is likely to represent a biased estimate of the independent association between BMI (as defined/grouped here) and ACE inhibitor usage within the past three months in this target population.",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.1. Chi-square test of independence"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Download the required datasets",
    "section": "",
    "text": "Before going any further please download the datasets files that we will be using throughout these sessions. These files can be downloaded as a single “zip” file by clicking here.\nAfter you have downloaded the zip file create a folder on your personal M: drive where you can store the files (e.g. call it something like “Statistics sessions datasets”). Once you have downloaded the zip file double click on it to open it and copy and paste all the files into the new folder you have create. Either highlight all the files and drag and drop them into the folder, or highlight all the files and press ctrl and c to copy, then go into the new folder and press ctrl and v to paste.\nYou will need to access these files throughout these sessions, so it’s important you have easy access to them.\nIf you are using your own Windows-based laptop and the zip file won’t open you may need to install an unzipper. You can download the free Winrar unzipping software from:\nwww.win-rar.com\nMac users should be able to unzip zip files with the in-built MacOS unzipping software.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "datasets.html#datasets",
    "href": "datasets.html#datasets",
    "title": "Download the required datasets",
    "section": "",
    "text": "Before going any further please download the datasets files that we will be using throughout these sessions. These files can be downloaded as a single “zip” file by clicking here.\nAfter you have downloaded the zip file create a folder on your personal M: drive where you can store the files (e.g. call it something like “Statistics sessions datasets”). Once you have downloaded the zip file double click on it to open it and copy and paste all the files into the new folder you have create. Either highlight all the files and drag and drop them into the folder, or highlight all the files and press ctrl and c to copy, then go into the new folder and press ctrl and v to paste.\nYou will need to access these files throughout these sessions, so it’s important you have easy access to them.\nIf you are using your own Windows-based laptop and the zip file won’t open you may need to install an unzipper. You can download the free Winrar unzipping software from:\nwww.win-rar.com\nMac users should be able to unzip zip files with the in-built MacOS unzipping software.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "independent-t-test.html",
    "href": "independent-t-test.html",
    "title": "The independent t-test",
    "section": "",
    "text": "In this practical we will practice applying the independent t-test to analyse the association between a normally distributed numerical outcome and a binary covariate, or put in more real-world terms: whether and by how much the mean of a numerical outcome differs between two groups.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.1. Independent t-test"
    ]
  },
  {
    "objectID": "independent-t-test.html#scenario",
    "href": "independent-t-test.html#scenario",
    "title": "The independent t-test",
    "section": "Scenario",
    "text": "Scenario\nYou and your colleagues have been tasked by the Kapilvastu district authorities in Nepal to help them understand the problem of high blood pressure and hypertension, and the associations between socio-demographic and health related characteristics and blood pressure level/hypertension. You have carried out a cross-sectional survey to address these aims, and collected data on systolic blood pressure, common socio-demographic characteristics, and some additional health-related characteristics. As per your statistical analysis plan you need to explore the association between important characteristics and levels of systolic blood pressure. These are therefore descriptive research questions, as we do not assume any causal association are necessarily behind any observed association, and the aim of these analyses is to help policy makers target preventative efforts at the most at-risk groups, as identified by their key characteristics.\nBelow we will just look at one of these associations. Specifically, the association between age, when grouped as individuals &lt;=40 and individuals &gt;40, and the level of systolic blood pressure in mmHg.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.1. Independent t-test"
    ]
  },
  {
    "objectID": "independent-t-test.html#exercise-1-use-the-independent-t-test-to-analyse-how-the-mean-level-of-systolic-blood-pressure-mmhg-differs-between-individuals-40-and-40",
    "href": "independent-t-test.html#exercise-1-use-the-independent-t-test-to-analyse-how-the-mean-level-of-systolic-blood-pressure-mmhg-differs-between-individuals-40-and-40",
    "title": "The independent t-test",
    "section": "Exercise 1: use the independent t-test to analyse how the mean level of systolic blood pressure (mmHg) differs between individuals <=40 and >40",
    "text": "Exercise 1: use the independent t-test to analyse how the mean level of systolic blood pressure (mmHg) differs between individuals &lt;=40 and &gt;40\n\nStep 1: create a binary covariate from a numerical variable\nOf course you may be interested in the association between a numerical covariate that is already binary, e.g. sex with levels of female and male, but this is a good opportunity to see how we can get SPSS to convert a numerical variable into a binary variable based on a threshold cut-point.\n\nLoad the “SBP data final.sav” SPSS dataset.\n\nVideo instructions: create a binary covariate from a numerical variable\nWritten instructions: create a binary covariate from a numerical variable\n\n\nRead/hide\n\nNext we need to create a two-level categorical variable defining our younger and older individuals. Usually you should choose the cut-point at which you define your two-levels based on theory or some sensible motivation (e.g. the age at which prior research suggests blood pressure may often change), but here let’s just compare those individuals above and below the median age, which is 40. We therefore want to create a variable that classifies every individual as either ≤40 or &gt;40.\n\nFirst, we can use the Compute Variable tool to create a new variable based on a logical test of whether the value in the original age variable is ≤40 or not. This will create a new variable that has the value 0 if the participant’s age is &gt;40 and the value 1 if their age is ≤40 (i.e. if the logical check we asked SPSS to do on the original age variable is TRUE then the new variable’s value is 1). From the main menu go: Transform &gt; Compute Variable. Then in the Compute Variable tool enter “age_young_old” as the name for the new variable in the Target Variable: box. Then in the Numeric Expression: box enter:\n\n\nage &lt;= 40\n\nThen click OK.\n\nThe output window will appear. Minimise this to go back to the main window and go to the Variable View. You should now see our new variable age_young_old has appeared. Let’s give it a variable label so it’s well described in SPSS: Age (0 = &gt;40 / 1 = &lt;=40). Then let’s give it some value labels so each level is well described. As it was created from a logical expression it takes only two values: 0 or 1. 1 should be coded as “≤40”, because when the participant’s age was less than or equal to 40 the logical expression would be true and the new variable would take the value 1. Similarly 0 should be coded as “&gt;40” as the logical expression would be false and the value set to 0. Check back to the instructions on adding value labels or ask if you need help.\nLastly, once that’s done you should always do a quick check that the variable has been created correctly using a “cross tabulation” between our old and new age variable. In the main menu go: Analyse &gt; Descriptive Statistics &gt; Crosstabs. Then in the Crosstabs tool add the original age variable into the Row(s): box and the new categorical binary age variable into the Column(s): box and click OK. If you’ve created the new age variable correctly all ages ≤40 should be counted in the age &lt;=40 column and all ages &gt;40 in the new age variable age &gt;40 column. Always perform such checks when creating new variables!\n\n\n\n\nStep 2: check the assumptions of the independent t-test are not violated\n\n\nRead/hide\n\nWhenever you run a statistical analysis you must ensure that the assumptions of the analysis are not violated, otherwise the results may not be valid or even meaningful. For simple analyses like the independent t-test we can check the assumptions before we run the test, but for more sophisticated analyses such as regression models we typically check the assumptions after running the analysis because SPSS produces the information to make the necessary checks only after running the analysis.\nThe independent t-test makes the following assumptions:\n1. Continuous outcome\nTechnically an independent t-test assumes a continuous outcome variable, but as long as assumptions 2 and 3 below are satisfied it’s fine to use a discrete outcome with an independent t-test. Here our outcome is continuous. Note: ideally when analysing a discrete outcome you would use a more specific model like a Poisson or negative binomial model, but this is beyond the scope of this introductory course.\n2. Independent groups (hence the “independent t-test”)\nThis means the observations in each group cannot be related. You can only really verify this by knowledge of your study design. With the SBP data the study took a simple random sample of individuals and we are then dividing them into two groups based on an age threshold. Therefore, by definition the two groups are statistically independent as all observations represent separate, randomly sampled individuals.\nTypically groups are only not independent in two situations. First, if you were taking repeated measurements from the same set of individuals but treating the measurements at each time period as different “groups”. Here there would be correlations between the measurements at the different time periods within individuals. Second, if you were taking measurements from two groups of separate individuals, but the individuals in the two groups were not statistically independent. For example, if there were different families with members in both the age groups, or if there were individuals from the same work places within each group. Again, in such situations there would be correlations between members of the same family or work place (e.g. due to genetics or shared risk factors etc).\nHere our groups or observations are clearly independent as the individuals in the study were randomly sampled and the two groups contain separate individuals.\n3. The outcome is approximately normally distributed within each group\nNote: technically it is the “residuals” or “model errors” that need to be normally distributed, which are the differences between the observed values of the data and the “model predicted values”. For a t-test these are simply the differences between the observed values of the data and the mean of the relevant group that the observation comes from, which actually means that for a t-test the residuals are identical to the observed values and so you can just view the distribution of the observed data. For more complicated regression models we’ll see that we need to calculate and plot the residuals separately.\nTo check the distribution of the residuals within each group you can use statistical tests, but most statisticians would recommend using graphical methods. We can visually check these two distributions easily in SPSS using histograms as follows. From the main menu go: Graphs &gt; Histogram. Then add the sbp variable into the Variable: box, then add the age_young_old variable into the Rows: box, tick the Display normal curve box and then click OK. You’ll see a histogram for the sbp variable for each age group. As you can see the outcome approximates a normal distribution fairly well for the ≤40 age (this is probably as good as you would see with real data), and is slightly right skewed for the &gt;40 age group. However, as previously mentioned the t-test is pretty robust to slightly skewed outcomes so this is nothing to worry about, and we can assume this assumption is met. Below are some examples of suitably and unsuitably distributed data for you to compare with for future reference.\nExamples of suitably and unsuitably distributed data for t-tests\n\n\nRead/hide\n\nStudents (and researchers) often struggle with knowing whether an analysis’s assumptions have been violated or not, and for good reason because it usually involves judgement based on experience. For the t-test like most analyses requiring normality there is no agreed “threshold” for when the skewness of a variable is considered “non-normal”. However, as t-tests and regression models are fairly robust to some non-normality the following examples should hopefully give you a sense of when you are okay to go ahead and when you are violating the assumptions and need to transform the data or use a different model.\nA. Probably suitably approximately normally distributed data within each group for a t-test, with n = 50 per group:\n\n\n\nApproximately normal (and approximately equal variance) data in each group of a t-test, n = 50 per group\n\n\nWith only 50 data points per group the distribution is pretty “lumpy” but you can see an approximate normal distribution in both groups, although less so for the lower group. Also note: these data are generated from a normal distribution, so you can see that when your sample size is low even with artificially simulated data from a known normal distribution the resulting sample can often be only somewhat approximately normal! You can also see that the variance in both groups looks approximately normal, which satisfies our next assumption (see below).\nB. Highly suitably normally distributed data within each group for a t-test, with n = 500 per group:\n\n\n\nApproximately normal (and approximately equal variance) data in each group of a t-test, n = 500 per group\n\n\nNow with 500 data points per group simulated from the same distribution unsurprisingly the distribution looks much more suitably normal.\nC. Probably not suitably normally distributed data within each group for a t-test, with n = 50 per group:\n\n\n\nNon-normal (and non-equal variance) data in each group of a t-test, n = 50 per group\n\n\nYou may think it doesn’t look much different from the first image, and you’d be right. Again, it comes down to judgement but when sample sizes are small there’s a lot of subjectivity involved. These data are simulated from a right-skewed “version” of the normal distribution. If you are concerned one option is to run your test on the raw data and then transform the data (we’ll see later how to do this) and re-run the test and see if the results change much, and just report both. This is called a sensitivity analysis.\nYou can also see that there is a lot more variance in the lower group’s data, which would violate our next assumption (see below).\nD. Definitely not suitably normally distributed data within each group for a t-test, with n = 500 per group:\n\n\n\nNon-normal (and non-equal variance) data in each group of a t-test, n = 500 per group\n\n\nNow with 500 observations per group it’s clear that there is some definite right-skew in the distribution of both group’s data, and that there is considerably more variance in the lower group.\n\n4. Equal variances in each group\nThis means that the variance (dispersion or spread) of outcome values is approximately the same in each group. We can check this by looking at the p-value for a test of this assumption called “Levene’s test of equality of variances” or by comparing the histograms from the two groups. SPSS produces the results of the Levene’s test when we run the t-test, so we will evaluate the results of this test/assumption once we’ve produced our t-test results below. Note: if this test is significant (P&lt;0.05) we can simply use the results from the version of the t-test that doesn’t assume equal variances in each group. The only disadvantage is that we lose a little bit of statistical power, but typically it’s not much and the results will be very similar. Hence, although we’ll interpret the “equal variances assumed” version of the t-test below it’s probably a better practice to just always use the more conservative but robust and safer set of results that do not assume equal variances is each group.\n\n\n\nStep 3: run the independent t-test\nVideo instruction: run the independent t-test\nWritten instruction: run the independent t-test\n\n\nRead/hide\n\n\nIn the main menu go: Analyse &gt; Compare Means &gt; Independent Samples T Test. Then in the Independent-Samples T Test tool add the sbp variable into the Test Variable(s): box. Then add our new covariate for age group (at the bottom of the list) into the Grouping Variable: box and click Define Groups…. This is where we tell SPSS what numerical values code for each of our two groups, and what way round we want to compare the two groups. Remember we created the age variable so that it took the value “0” for individuals &gt;40 and “1” for individuals ≤40. Let’s compare individuals &gt;40 to individuals ≤40. When it comes to comparing the groups this tells SPSS subtract the mean outcome in group 0 from group 1, i.e. the mean systolic blood pressure for individuals &gt;40 minus the mean systolic blood pressure for individuals ≤40. To do this ensure the Use specified values option is selected and add the value “0” to Group 1: and the value “1” to Group 2:.\nNote: you can of course swap the coding values around and compare individuals ≤40 to individuals &gt;40 and your results will be reversed but otherwise identical. Neither choice is necessarily “correct” it just depends on which way round you want the groups compared. If, in your version of SPSS, there is a box called Estimate effect sizes below the Grouping variable: options make sure this is not ticked. Finally click Continue and then OK. You’ll now see the output window appear with our results!\n\n\n\n\nStep 4: understand the results tables and extract the key results\nNow we’ve verified the first two assumptions of the independent t-test (independent observations and approximately normally distributed residuals) let’s look at the results tables where we can check the third assumption (equal variances in each group), extract the key results and interpret them.\nSo what do the tables mean?\n\n\nGroup Statistics table explained\n\n\nThe first table we see gives us descriptive statistics for the outcome variable for each level of our categorical covariate.\n\nFirst, make sure you look at and record the values under the “N” column, which are the group sample sizes, and check they match what they should be. You should see there were 295 individuals in the ≤40 group and 248 individuals in the &gt;40 age group. These sum to 543, which is less than the total number of individuals in the dataset: 556. There are three reasons why this might be: either some individuals are missing values for the outcome or the covariate or both variables. We know from when we prepared the dataset that some individuals are indeed missing values for their systolic blood pressure. You can quickly check this by going to the Data view, right clicking on the sbp variable and selecting Descriptive Statistics. In the Statistics results table that appears you should see that there are indeed 13 individuals with missing values for sbp, and hence only 543 individuals with complete data on this variable, which explains the sample size of the independent t-test.\nNext make sure you look at and record the mean value of the outcome for each group under the “Mean” column. You should see the mean systolic blood pressure (mmHg) was 128.2 for individuals &gt;40 and 125.1 for individuals ≤40 (rounded to 1 decimal place).\n\n\n\n\n\nIndependent Samples Test\n\n\nThis table gives us the results of the t-test we’re interested in, although SPSS lays out the results less than clearly in my opinion. Here’s what they all mean.\n\n\nLevene’s Test for Equality of Variances section columns explained\n\n\nUnder this part of the table are results that relate to a statistical test that is totally separate to the independent t-test called “Levene’s test for equality of variances”. This tests whether the variance (spread) of the outcome in each group is approximately equal. If they are not then this assumption is violated and we must only use the results from the version of the independent t-test that accounts for this.\nF\n\nThe F-value is the test statistic for the Levene’s test for equality of variances. You can ignore it as it is used to calculate the corresponding p-value of the test (see below), which is all we are interested in.\n\nSig.\n\nThe “Sig.” value is just the p-value associated with the test statistic (i.e. the F-value). For some reason SPSS always refers to p-values with a “Sig.” column heading. The null hypothesis for the Levene’s test is that the variances in both groups are equal. Therefore, if the p-value (“Sig.”) is &lt;0.05 this indicates that the variances in the two groups are unlikely to be equal. If this is the case it is then safer to use the version of the independent t-test that does not assume equal variances. Results from both versions of the independent t-test are presented. Just look at the far left of the table and you can see the top row of results are for the “Equal variances assumed” version and the bottom row for the “Equal variances not assumed” version.\nHere you should see that the p-value for the test is 0.244, i.e. it’s &gt;0.05. Therefore, we can assume the variances are approximately equal in each group. Hence, we can use the results for the independent t-test from the row called Equal variances assumed (see below).\nNote: SPSS only gives p-values to 3 decimal places, so when it says “0.001” it means the p-value is actually &lt;0.001. Therefore, you should write P&lt;0.001.\n\n\n\nt-test for Equality of Means section columns explained\n\n\nUnder this part of the table are the results for the actual independent t-test. We will use those from the row/version that do not assume equal variances.\nt\n\nThis is the value of the t-test statistic which is used when calculating the p-value. We can ignore this and just interpret the p-value directly (and of more use the confidence intervals).\n\ndf\n\nThis is the degrees of freedom which form part of the calculation of the test statistic (calculated as: n - 2). You can typically ignore this but it should closely match your “true” sample size, i.e. the number of genuinely independent observations.\n\nSig. (2-tailed)\n\nThis is the two-tailed p-value for the independent t-test. The “two-tailed” part means that it allows for the possibility that the difference between the two groups may be positive or negative, i.e. either group might have a greater mean than the other. The null hypothesis of the independent t-test is that the “true” difference between the two groups in the target population from which the sample was taken is exactly 0. Therefore, this p-value tells us how likely we are to have observed data giving a mean difference at least as great or greater (in either direction) as the one observed. Alternatively, you can more loosely interpret it as a probability measure of how consistent the data are with the null hypothesis of no difference. Again note that SPSS only gives p-values to 3 decimal places, so when it says 0.001 it means the p-value is &lt;0.001.\nHere you should see that the p-value is 0.072.\n\nMean Difference\n\nThis is the difference between the mean of the outcome for the group that we set as “Group 1” minus the mean of the outcome for “Group 2”. For us this means the mean SBP of participants aged &gt;40 minus the mean SBP of those aged ≤40. It therefore tells us the direction and size of any difference, and is therefore a key result/statistic from the test. You can swap the coding of the groups around if it makes more sense (just go back and re-run the analysis but change the coding), but the difference will be reversed (i.e. a positive difference will become a negative difference).\nHere you should see that the mean difference (rounded to one decimal place) is 3.1.\n\nStd. Error Difference\n\nThis is the standard error of the mean difference, which estimates the sampling variability associated with our estimate of the parameter in the target population. This is used when calculating the 95% confidence intervals and the p-value. You can ignore it and just interpret the confidence intervals and p-value.\n\n95% Confidence Interval of the Difference (Lower and Upper)\n\nThese are the estimated lower and upper 95% confidence intervals around the estimated mean difference, which is the point estimate or best single estimate of the mean difference in the target population. Formally speaking, if we repeated our study an infinite (or very large number) of times and each time we calculated the mean difference and the 95% confidence interval around those mean differences then 95% of those 95% confidence intervals would contain the true mean difference found in the target population (i.e. the mean difference that we would find if we sampled 100% of the target population). Informally speaking we can think of the 95% confidence intervals as defining a range of values that are consistent with the likely/probable true mean difference in the target population based on the sampled data.\nHere you should see that the lower and upper confidence intervals for the mean difference (rounded to one decimal place) are -0.3 and 6.4.\n\n\n\n\n\nStep 5: report and interpret the results\nReporting the results\nIn a methods section you should explain that you used an independent t-test to analyse the data and justify why.\nIn a results section we would want to report the sample size, the mean of the outcome for each group, and the key results of the independent t-test (the mean difference, the 95% confidence intervals and the associated p-value). Therefore, we could write something like the following (using “n” as it is commonly used to refer to sample size):\n\nWe compared the systolic blood pressure among individuals aged &gt;40 (n = 248) to those aged (n = 295). Among those aged &gt;40 the mean systolic blood pressure was 128.2 mmHg and among those aged ≤40 the mean systolic blood pressure was 125.1 mmHg. Using an independent t-test (equal variances assumed) we estimated that the mean difference in systolic blood pressure between those aged &gt;40 compared to those aged ≤40 was 3.1 mmHg (95% CI: -0.3, 6.4).\n\n\nNote: the convention is to compactly present confidence intervals for estimated statistics in brackets like above, indicating the confidence level of the confidence interval (usually 95%), and to use a comma “,” or possibly the word “and” to separate the lower and upper confidence interval, but not a dash “-” as this can look like a negative symbol.\nAlso note: we are focusing on the mean difference and the confidence intervals, not the p-value here as it tells us nothing more and far less than those results do.\nLastly, if you felt it wasn’t clear where the inferential result had come from, say if you were reporting result from different analyses, then you should certainly make this clear in the results section too, e.g. you might say something like “based on an independent t-test the mean difference was…”.\n\n\nDiscussion: interpret the direction and size of the difference in terms of the implications for practice and policy. Is the difference “statistically significant”, i.e. can we make a clear inference that there even is a difference on average between the groups, and if so is it a small difference, a medium difference, a large difference etc in terms of what is being measured and the implications for practice and policy?\n\nInterpreting the results\nHow do we actually interpret these results then? Remember with statistical inference we are aiming to make a “probabilistic inference”, or a generalisation with some level of uncertainty, about the likely value of the population parameter of interest, which here is the mean difference in systolic blood pressure between individuals aged &gt;40 compared to individuals aged ≤40 in our target population, based on the data from our sample. And our confidence intervals around our sample statistic (the sample mean difference) are what allow us to do that. More specifically, the confidence intervals around the mean difference tell us that the mean difference in systolic blood pressure between individuals aged &gt;40 compared to individuals aged ≤40 in the target population is “quite likely” to be somewhere between -0.3 and 6.5 mmHg. Alternatively and equivalently we can interpret the results in terms of a randomly selected individual in our target population, and say that if we randomly selected one individual from the &lt;=40 group and one from the &gt;40 group then on average the systolic blood pressure of the individual in the &gt;40 would be “quite likely” to be between -0.3 mmHg lower to 6.5 mmHg higher than the individual in the ≤40 group.\nNote: strictly or technically speaking our confidence intervals tell us how much error is associated with our sampling process, and that if we repeated our study many times 95% of the time the 95% confidence intervals that we obtained around our estimated mean difference would contain the true mean difference in the target population. Therefore, our interpretation of the value being “quite likely” to fall within our given confidence interval range is somewhat informal and loose.\nAlso note: critically this interpretation assumes that all other sources of bias are negligible, which is extremely unlikely! Therefore, you should always treat inferential results very carefully, and evaluate them in the context of how much likely bias there is in the results other than sampling error, which is the only source of bias that inferential statistics account for. E.g. if you knew there was likely a large amount of other sources of error in the study, such as some serious selection bias, then the 95% confidence intervals would not accurately reflect the total error or bias in the results because they can only account for sampling error in the absence of all other errors/bias.\nConsequently, our confidence intervals indicate that we can’t have much confidence over whether there is even a positive or negative difference in systolic blood pressure between these two groups in the target population, let alone how big any difference is with any great precision. However, the result does tell us that the practical significance or clinical significance of the true difference in mean systolic blood pressure between these two groups is likely to be small, given the confidence intervals suggest it is likely to be at most just 6.5 mmHg (the confidence interval furthest from 0). Always remember though, this is a probabilistic result (not certain) and there is always still a non-negligible chance that the true difference is greater, possibly much greater, than we estimated, i.e. outside the range of the 95% confidence intervals, and as above this is actually inevitable with any study unless completely free of other sources of bias.\nLastly, note: when setting up the t-test if we swapped the order of the comparison around we would get the same result but expressed as the ≤40 age group compared to the &gt;40 age group, and so we’d get a mean difference of -3.1 mmHg (95% CI: -6.5, 0.3). Note the 95% confidence interval order is also reversed.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.1. Independent t-test"
    ]
  },
  {
    "objectID": "independent-t-test.html#exercise-2-use-the-independent-t-test-to-analyse-how-the-mean-level-of-systolic-blood-pressure-mmhg-differs-between-individuals-with-bmi-30-kgm²-and-30-kgm²",
    "href": "independent-t-test.html#exercise-2-use-the-independent-t-test-to-analyse-how-the-mean-level-of-systolic-blood-pressure-mmhg-differs-between-individuals-with-bmi-30-kgm²-and-30-kgm²",
    "title": "The independent t-test",
    "section": "Exercise 2: use the independent t-test to analyse how the mean level of systolic blood pressure (mmHg) differs between individuals with BMI <30 kg/m² and ≥30 kg/m²",
    "text": "Exercise 2: use the independent t-test to analyse how the mean level of systolic blood pressure (mmHg) differs between individuals with BMI &lt;30 kg/m² and ≥30 kg/m²\nUsing the “SBP final data.sav” dataset and via the process outlined above use an independent t-test to analyse the association between BMI and systolic blood pressure.\n\nDivide bmi into two groups based on BMI values &lt;30 kg/m² and those ≥30 kg/m², which are the values typically used to define obesity when defined in terms of BMI.\nCompare individuals with BMI values &lt;30 kg/m² to those with BMI values ≥30 kg/m² using the independent t-test.\nExtract the mean difference and confidence intervals around this estimate.\nIn the “Exercises” folder open the “Exercises.docx” Word document and scroll down to Independent t-test: the association between BMI and systolic blood pressure.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: sample size, group sizes and group outcome means. Also be sure to include sufficient details about the outcome variable and the comparison made, including how the two independent groups were defined, as well as the type of analysis used, and of course the key inferential results. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\n\nExample independent t-test results reporting text\n\n\nRead/hide\n\nUsing an independent t-test I analysed the association between BMI and systolic blood pressure (mmHg) for individuals having low compared to high BMI (&lt;30 kg/m² compared to ≥30 kg/m²). Out of a total sample size of 516 individuals, 419 individuals had a BMI &lt;30 kg/m² (mean systolic blood pressure = 121.6 mmHg) and 97 individuals had a BMI ≥30 kg/m² (mean systolic blood pressure = 146.7 mmHg). This represented a mean difference of -25 mmHg (95% CI: -28.9, -21.2) for those with a BMI &lt;30 kg/m² compared to those with a BMI ≥30 kg/m². Therefore, having a BMI &lt; 30 kg/m² appears to be associated with a substantially lower systolic blood pressure on average than having a BMI ≥30 kg/m².",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.1. Independent t-test"
    ]
  },
  {
    "objectID": "independent-t-test.html#next-steps-optional",
    "href": "independent-t-test.html#next-steps-optional",
    "title": "The independent t-test",
    "section": "Next steps (optional)",
    "text": "Next steps (optional)\nIf you have time and want to practice and learn more you can try the following exercises:\n\nRepeat the independent t-test but reverse the coding of the groups to convince yourself that the results are identical but just reversed.\nUse the independent t-test to analyse whether there is a difference in the mean systolic blood pressure between individuals from different socio-economic groups (ses variable). As there are three socioeconomic groups in the variable this would require you to first recode the ses variable into a new variable so that two of the socio-economic groups are pooled to give a binary variable.",
    "crumbs": [
      "6. Bivariate tests for numerical variables",
      "6.1. Independent t-test"
    ]
  },
  {
    "objectID": "introduction-to-spss.html",
    "href": "introduction-to-spss.html",
    "title": "An introduction to SPSS",
    "section": "",
    "text": "In this short practical we will make sure we can all load SPSS, and we will load some data and take a look at the main features of the interface.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.1. Introducing SPSS"
    ]
  },
  {
    "objectID": "introduction-to-spss.html#additional-spss-and-related-statistical-software-information",
    "href": "introduction-to-spss.html#additional-spss-and-related-statistical-software-information",
    "title": "An introduction to SPSS",
    "section": "Additional SPSS and related statistical software information",
    "text": "Additional SPSS and related statistical software information\n\n\nRead/hide\n\nSPSS was originally called the “Statistical Package for the Social Sciences” reflecting the original main audience, but over time it has become widely used. Although it might look similar to Microsoft Excel at first glance, rather than using functions within “cells” to do things, all data processing and analysis is done via commands run from the menu, and it has a far wider range of much more powerful and sophisticated statistical analyses available than Excel. Although we will only be using the menus commands to run our analyses, SPSS also has the option to use a programming interface to run all commands via code (written text commands). This approach has a number of huge advantages over using the menu commands. One of the major advantages of using code/scripts is that you can tweak and re-run your entire analysis, which might be the product of days or weeks of work, just by highlighting the script and re-running it. To run all the commands stored in your script manually via the menus might take hours! Another major advantage is it makes it very easy to collaborate on analyses, assuming your collaborators can understand and work with SPSS code, because you can simply share your code. However, getting to grips with this approach is typically a steep learning curve, and so we will not look at this further.\nHowever, there are many other excellent statistical software packages available, with R and Stata being the most commonly used in global health research. And unlike SPSS, if you are looking for a free statistical software package my recommendation would definitely be R (https://www.r-project.org/). There is a document giving a brief overview to R on the Statistics for Health Sciences Minerva site, but the main advantages of R are 1) it’s open source and therefore completely free, 2) it’s huge user community means you will always find solutions to any problems you run into already waiting for you to read online, and 3) it’s huge range of user-developed “packages” mean that R will almost always be able to do what you want it to do. Note that both R and Stata are intended to be run as programming languages, i.e. by writing and running code rather than clicking through endless menus. However, there are also “graphical user interfaces” available for R so that you can run R (albeit with reduced functionality) like we will be using SPSS, as a menu-based program. Similarly, you can also run quite a lot of the main analyses and tools in Stata via its built-in menus.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.1. Introducing SPSS"
    ]
  },
  {
    "objectID": "introduction-to-spss.html#the-datasets",
    "href": "introduction-to-spss.html#the-datasets",
    "title": "An introduction to SPSS",
    "section": "The datasets",
    "text": "The datasets\nBefore we go further a brief word about the datasets we will be using in these SPSS practical sessions.\nThe datasets that we’ll be using in these data analysis sessions are a mix of simulated (i.e. artificially generated) and real data. The main dataset that we will be using for most analyses is the SBP final data.sav dataset, which is an SPSS format dataset as indicated by the “.sav” suffix. We will be using a more basic Excel version of this dataset and the SPSS format version of this dataset for the first few exercises.\nThe SBP final data is a simulated dataset but it is designed to be representative of a typical cross-sectional dataset of individual-level data. Although the dataset is simulated it is designed to be realistic and includes typical features like missing data. For simplicity we will therefore talk about the dataset as if it was a real dataset from now on, but be aware that it is somewhat simplified with fewer variables than we would likely collect for such a study. Specifically, the dataset contains data collected from a survey investigating what biological, health and socio-economic factors influenced individuals’ blood pressure and hypertension status. The dataset was collected from a study using a simple random survey of individuals aged 18 or over (the only eligibility criteria) who lived in a single city, and the data were collected at approximately the same point in time (e.g. over the course of a few weeks). The dataset contains data from 556 participants. Therefore, the “unit of observation” in this dataset is the individual.\nNote: we do not imply any social/political judgments based on how we have coded certain variables in this dataset.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.1. Introducing SPSS"
    ]
  },
  {
    "objectID": "introduction-to-spss.html#exercise-1-loading-spss",
    "href": "introduction-to-spss.html#exercise-1-loading-spss",
    "title": "An introduction to SPSS",
    "section": "Exercise 1: loading SPSS",
    "text": "Exercise 1: loading SPSS\nPlease note that all video-based SPSS instructions were made with SPSS version 27, so they may look a little different to the more recent version (29) you will be using, but the instructions should all work.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.1. Introducing SPSS"
    ]
  },
  {
    "objectID": "introduction-to-spss.html#opening-spss",
    "href": "introduction-to-spss.html#opening-spss",
    "title": "An introduction to SPSS",
    "section": "Opening SPSS",
    "text": "Opening SPSS\nIf you happen to already have a copy of SPSS installed on your laptop feel free to use that. However, you can access the latest version of SPSS either from the campus computers that are available in the computer rooms where we will hold all the practical sessions, or from your personal laptop if you’d prefer. In both cases you can access SPSS via the online “appsanywhere” software platform, that allows you to access all kinds of software for free.\nBelow you can read about how to run SPSS from either a campus computer or your own laptop via the appsanywhere platform.\n\nCampus computers\n\nOpen the Google Chrome internet browser and navigate to https://appsanywhere.leeds.ac.uk/. You may have to login: if so use your usual university username but remember to add ‘@leeds.ac.uk’ at the end of your username. You should see a blue popup message appear at the bottom right of the screen saying “Validation in progress…”. The AppsAnywhere “Cloudpaging Player” app will also launch while this happens. This needs to be running for you to access software via the appsanywhere platform.\nAfter a few seconds you should then see a second popup message appear saying “Validation Successful”. If you see a message saying “Validation failed” click the “Retry” button, but if you see the same message again see the second bullet point below for help. Otherwise, you can then click on the search field at the top of the https://appsanywhere.leeds.ac.uk/ page and enter “SPSS”. Look for “SPSS Statistics 29” and click the “Launch” button, which will load SPSS after a while. This can take a little time as it may need to install some files on the computer.\nOnce you have run SPSS from a computer it should be available directly from the Cloudpaging Player app, so next time try just running the app from the start menu, highlighting SPSS in the list of “Applications” and clicking the “Launch” button. If you can’t see SPSS you’ll have to go via the appsanywhere.leeds.ac.uk site again though.\nIf you saw a message saying “Validation failed” even after retrying then the app probably isn’t installed. You can check by looking whether the app is in the Windows Start menu. Just click on the Widows button in the bottom left corner of your desktop and type “cloud”. If the Cloudpaging Player app icon and name doesn’t appear it’s not installed. IT say you have to contact them to get it installed, so if it’s not installed please try other computers until you find one where the app is installed.\n\n\n\nPersonal laptops\nWindows-based laptops\n\nYou will need to install the Cloudpaging Player app to access software from the AppsAnywhere platform if you haven’t previously done this. Please follow the instructions on the IT help page here. It shouldn’t take too long, but may take 5-10 minutes. Once you have installed the app go to the Appsanywhere website at https://appsanywhere.leeds.ac.uk/. A purple spinning icon should appear in the bottom right corner of your screen while the Cloudpaging Player loads and validates that you have access to the software on the website. This should change to a green tick icon fairly quickly. You can then enter “SPSS” in the search box on the webpage. Look for “SPSS Statistics 29” and click the “Launch” button, which should load SPSS after a while.\nNote that once you’ve run SPSS from your laptop once it should be available directly from the Cloudpaging Player app. So next time try just running the app from the start menu/desktop shortcut, highlighting SPSS in the list of “Applications” and clicking the “Launch” button. Although if you can’t see SPSS you’ll have to go via the appsanywhere.leeds.ac.uk site again though.\nAny problems let us know, but please be aware we are not IT we are just academics! Therefore, if we can’t solve the problem quickly we recommend moving onto a campus computer, as they should have no issues running SPSS. You can always speak to IT later about trying to get access to SPSS via your personal laptop for the later sessions if that’s really important to you.\n\nMacs\n\nIf you have a Mac then unfortunately you cannot run AppsAnywhere directly and you must access it via the university’s Virtual Windows Desktop. You will first have to install the Virtual Windows Desktop app. To do this follow the instructions here.\nOnce you’ve installed the Windows Virtual Desktop app run the “Remote Desktop” app. You will have to sign in using your university ID. Remember to include @leeds.ac.uk at the end of your username. Then in the window that appears double click on the “WVD - Academic” icon.\nOnce the virtual Windows desktop appears open Google Chrome internet browser and go to the Appsanywhere website at https://appsanywhere.leeds.ac.uk/. A purple spinning icon should appear in the bottom right corner of your screen while the Cloudpaging Player loads and validates that you have access to the software on the website. This should change to a green tick icon fairly quickly. You can then enter “SPSS” in the search box on the webpage. Look for “SPSS Statistics 29” and click the “Launch” button, which should load SPSS after a while.\nNote that once you’ve run SPSS from your laptop once it should be available directly from the Cloudpaging Player app. So next time try just running the app from the start menu/desktop shortcut, highlighting SPSS in the list of “Applications” and clicking the “Launch” button. Although if you can’t see SPSS you’ll have to go via the appsanywhere.leeds.ac.uk site again though.\nAny problems let us know, but please be aware we are not IT we are just academics! Therefore, if we can’t solve the problem quickly we recommend moving onto a campus computer, as they should have no issues running SPSS. You can always speak to IT later about trying to get access to SPSS via your personal laptop for the later sessions if that’s really important to you.",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.1. Introducing SPSS"
    ]
  },
  {
    "objectID": "introduction-to-spss.html#exercise-2-loading-data-and-understanding-the-key-features-of-the-interface",
    "href": "introduction-to-spss.html#exercise-2-loading-data-and-understanding-the-key-features-of-the-interface",
    "title": "An introduction to SPSS",
    "section": "Exercise 2: loading data and understanding the key features of the interface",
    "text": "Exercise 2: loading data and understanding the key features of the interface\n\nFirst load SPSS, then follow the instructions below to practice loading a dataset and understanding the layout and key features of SPSS.\n\nVideo instructions: loading data, and understanding the key features of interface\nWritten instructions: opening SPSS, loading data, and understanding the key features of interface\n\n\nRead/hide\n\n\nWhen you load SPSS unless it is an older version you will get two windows appear. The upper one has “Welcome to SPSS” in the top left corner. Just close this window leaving open the “data editor” window, which will say “Untitled…” in the top left corner, and which looks like this:\n\n\n\n\nSPSS Main Window View\n\n\n\nSPSS has two main windows: the data editor, as we’ve just seen in the image above, and the “output window”, which displays the SPSS syntax or code that the buttons you click create to tell SPSS what to do (which you can ignore and you’ll probably find annoying that it’s displayed every time you do something) along with the results of those commands (usually printing tables of results from analyses and producing graphs, which are useful). This only appears once you tell SPSS to do something though. For simplicity we will just refer to the data editor window as the main window from now on.\n\n\nLoading data in SPSS\n\nNext let’s load some data so we can explore the main window (and the output window will appear for the first time). Go back to the main window in SPSS and in the main menu click File &gt; Open &gt; Data. Then in the Open Data tool window that opens navigate to the folder where you saved the “MSc & MPH statistics computer sessions practical files.zip” folders and go into the “Datasets” folder in the same way you would locate a folder in Windows Explorer. Find the file called “Example basic data.sav” and either double click on it or click on it once and then click the OK button.\nThis is an SPSS format dataset (.sav) and therefore contains more data and formatting of the data than other formats such an Excel (.xlsx) or .csv etc. This allows it to store things like variable labels and value codes (see below).\nYou will eventually see the output window appear and display some code that is telling SPSS to load the dataset, and then SPSS should take you back to the main window where you will see the raw values of the data in the Data View tab. See number 3 in the image above. The variable names are in the top row and the raw data values are in the cells. You will see there are three variables.\n\n\n\nThe main features of the main window\nRefer to the “SPSS main window view” image above and the numbered features:\n\nThe main menu. This is where you select commands to do most things you need to do.\nThe raw data view. Similar to Excel, here you will see the raw data displayed (although you can choose whether to display categorical variables’ labels or raw values if numerically coded - see below).\nThese tabs allow you to either view the Data View, where the raw data is displayed in the main area (2), or the Variable View, where information about each variable is displayed and where you can edit variables’ characteristics.\nVariable names will be displayed here once any data are loaded.\n\n\n\nVariable names/labels\nBefore going further we will just set an option (or check that it’s already set) in SPSS so that it displays variables’ names rather than variables’ labels in the various tools we will be using. We will see what the difference between these two things are later. This is because in the instructions below we refer to variables’ names rather than their labels. However, you can certainly choose to display either for your own analyses.\n\nFrom the main menu bar go: Edit &gt; Options and then under the General look under the Variable Lists section and select the Display names option as shown below:\n\n\n\n\nSPSS Main Options\n\n\n\n\nThe Variable View and variable properties\nNow that we’ve loaded some example data into SPSS let’s look at the variable properties we can edit in the Variable View tab in the main window.\n\nName = the variable’s name. Like a file name, this should be short and descriptive and clear.\nType = the “type” of format in which the variable is stored, e.g. numbers, string (letters and possibly numbers and/or characters) etc. Note this is just how the variable is stored, and not the type of variable in terms of categorical, numerical etc (see “Measure” below). You can often just ignore this and you should not alter this unless you know what you are doing!\nWidth = for string variables, width refers to how many characters a value can hold. Again you can just ignore this and should not alter this unless you know what you are doing!\nDecimals = how many decimal places will be displayed (note: changing this does not alter how many decimal places a value actually has stored though).\nLabel = the descriptive label of the variable, used to briefly describe what the variable measures in more detail than the variable name.\nValues = used to create or edit value labels, i.e. labels attached to different numerical values. We’ll come back to this shortly.\nMissing = used to set or change which specific values (if any) represent different types of missing data. We will not be using this.\nColumns = how wide the displayed columns are currently.\nAlign = whether values are left or right aligned.\nMeasure = what type of data are stored in each variable. In the terminology we’ve been using: nominal = a binary variable or a categorical variable with 3 or more category levels, ordinal = an ordinal categorical variable, and scale = a discrete or continuous numerical variable.\nRole = I’m not actually sure what this does so presumably it’s not anything that important and you can ignore it. Google if interested!",
    "crumbs": [
      "2. Introducing SPSS & preparing data",
      "2.1. Introducing SPSS"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "In this practical we will look at using linear regression to estimate associations between any type of covariate and a continuous outcome.",
    "crumbs": [
      "8. Regresson modelling",
      "8.1. Linear regression I"
    ]
  },
  {
    "objectID": "linear-regression.html#uses",
    "href": "linear-regression.html#uses",
    "title": "Multiple linear regression",
    "section": "Uses",
    "text": "Uses\nLinear regression can be used for description (particularly to describe associations), causal inference or prediction. Here we will just look at using it for describing associations, which typically means using separate models to describe each association of interest, as we do not want to adjust for other covariates. You can read more about the rationale behind this broad approach in this excellent recent framework for descriptive epidemiology: https://pubmed.ncbi.nlm.nih.gov/35774001/",
    "crumbs": [
      "8. Regresson modelling",
      "8.1. Linear regression I"
    ]
  },
  {
    "objectID": "linear-regression.html#key-terminology-and-concepts",
    "href": "linear-regression.html#key-terminology-and-concepts",
    "title": "Multiple linear regression",
    "section": "Key terminology and concepts",
    "text": "Key terminology and concepts\nIf necessary please read the below guide to key terminology and concepts before reading further as we will be using these terms when discussing linear regression and the modelling process in SPSS without further explanation.\n\nModel = Loosely speaking: the outcome variable and the set of covariates that you are analysing via linear regression, plus their functional form (see below). Note: although simpler analyses like the independent t-test are referred to as “statistical tests”, creating the false illusion of a fundamental difference from regression “modelling”, they have the same underlying mathematical form. Simply put, parameteric statistical tests like the independent t-test are models too.\nCoefficient or parameter or (model) term = The point estimate measuring/indicating the direction and size of the association between a given covariate and the outcome that your linear regression estimates.\nFunctional form/model parameterisation = In what mathematical form you add covariates to your model. Practically speaking this is whether your model assumes 1) a simple linear association between a given covariate and the outcome (also called a “main effect”), or 2) an interaction between a given covariate and one or more other covariates and the outcome, or 3) a non-linear association between a given covariate and the outcome (although many other functional forms are possible).",
    "crumbs": [
      "8. Regresson modelling",
      "8.1. Linear regression I"
    ]
  },
  {
    "objectID": "linear-regression.html#scenario",
    "href": "linear-regression.html#scenario",
    "title": "Multiple linear regression",
    "section": "Scenario",
    "text": "Scenario\nWe wish to describe important associations between key socio-demographic and relevant health related characteristics and individuals’ systolic blood pressure using the data collected in the SBP final dataset. As these are descriptive research questions and we want to describe the associations of interest as they exist, we will use repeated linear regression models where we model the association between each characteristic (covariate) of interest without adjusting for any other covariates. This will produce unadjusted or crude associations that reflect the associations as they appear without any assumption that they may reflect causal associations. One or more of these associations may of course reflect causal associations, but it’s unlikely they will be good (accurate) estimates of causal associations because this is an observational study so adjusting for the inevitable confounding effectively, and without making potentially worse mistakes like introducing collider bias, is a huge challenge that is beyond the scope of this module, but see the materials in the linear regression lecture on Minerva for an introduction to the complex world of causal inference using observational studies/data.",
    "crumbs": [
      "8. Regresson modelling",
      "8.1. Linear regression I"
    ]
  },
  {
    "objectID": "linear-regression.html#exercise-1-describe-the-population-level-association-between-bmi-and-systolic-blood-pressure-using-linear-regression-assuming-a-linear-association",
    "href": "linear-regression.html#exercise-1-describe-the-population-level-association-between-bmi-and-systolic-blood-pressure-using-linear-regression-assuming-a-linear-association",
    "title": "Multiple linear regression",
    "section": "Exercise 1: describe the population-level association between bmi and systolic blood pressure using linear regression (assuming a linear association)",
    "text": "Exercise 1: describe the population-level association between bmi and systolic blood pressure using linear regression (assuming a linear association)\n\nLoad the “SBP data final.sav” dataset.\n\n\nStep 1: explore the data\nWritten instructions: explore the data for a linear regression\n\n\nRead/hide\n\nIt is always advisable to conduct some focused exploration of your dataset to understand the data and the guide certain decisions in your model building process. Note: this would follow your data preparation stage, where you would already have a good sense of the key characteristics of each variable, such as the type of data it contains, the range of values present etc.\nIt is usually recommended to thoroughly explore you data in terms of: 1) the distribution of your outcome and covariates, to ensure you understand them well and are taking a suitable modelling approach (e.g. linear regression rather than another type of regression), and 2) what functional form of model makes good sense given the associations between your independent and outcome variables, primarily whether there are any clear/strong non-linear associations or interactions, although we will not look at exploring interactions in this practical as they are beyond the scope of this course. Again, see the linear regression lecture additional materials on Minerva for some introductory information on this.\n\nUnivariate exploration\nTo understand the distribution of your variables you can use histograms for numerical variables and bar charts for categorical variables. Let’s run a histogram for our outcome variable.\n\nFrom the main menu go: Graphs &gt; Histograms. Add sbp into the Variable: box, tick the Display normal curve box and click OK. What do you see?\n\nWhat does the distribution of the outcome look like?\n\n\nRead/hide\n\nThere appears to be a slightly odd “gap” near the mean, but overall the variable appears to pretty closely follow a normal distribution.\n\nNext let’s look at a bar chart for our ses variable.\n\nFrom the main menu go: Graphs &gt; Legacy Dialogues &gt; Bar. Then click the Simple option and Define. Add ses to the Category axis: box, tick the % of cases option and click OK. What do you see?\n\nWhat does the distribution of the socio-economic status variable look like?\n\n\nRead/hide\n\nMost participants were of low socio-economic status, with successively smaller proportions being of medium and high socio-economic status.\n\nYou can use these two types of graphs to explore the distribution of all the variables. In a real analysis you would certainly want to do this, but for the sake of time you may want to move on now that you know how to do this.\n\n\nBivariate exploration\n\n\nNumerical covariates\nFirst let’s look at associations between the outcome and numerical variables (i.e. bivariate associations) to understand whether it’s reasonable to assume simple linear associations for your numerical covariates or whether any need to be modelled via the addition of non-linear terms to the model. We’ll just look at bmi as this is our focus for this exercise, but we’d do this for all numerical covariates in practice. We’ll use a scatterplot.\n\nFrom the main menu go: Graphs &gt; Legacy Dialogues &gt; Scatter/Dot. Then select the Simple option and click Define. Add the sbp variable into the Y Axis: box and bmi into the X Axis: box then click OK. What do you see?\n\nWhat does the association between bmi and systolic blood pressure look like?\n\n\nRead/hide\n\nThere’s a clear linear association displayed here.\n\nWhat if we had seen a clear non-linear association? See the additional materials on the Minerva lecture folder for more info, but in brief there are two main options within a regression modelling framework:\n\nConvert your numerical covariate into a categorical variable.\nInclude additional “polynomial” terms of the relevant covariate. This just means that as well as including, say, age in the model you include age² or possibly higher-order terms as well.\n\nOption 1 is often the best choice because although it might not model the association as well as option 2 it provides results that are easier to interpret. If you ever need to do this as always you should think carefully and critically about what cut-points to use when converting your numerical variable to a categorical variable. There aren’t necessarily clear “rules” about this, but within the framework we’ve discussed it would be most consistent to choose cut-points based on theory rather than driven by what the sample data suggest are key cut-points.\n\n\n\nStep 2: run the linear regression model\nSorry there are no video-based instructions.\nWritten instructions: run the linear regression model\n\n\nRead/hide\n\nRemember linear regression allows you to look at associations between a numerical outcome variable and any number of numerical or categorical covariates, assuming all the assumptions of the method are met (we’ll check these out shortly). So let’s see how we build, run and estimate our linear regression model in SPSS.\n\nFrom the main menu go: Analyze &gt; General Linear Model &gt; Univariate. Note: a univariate general linear model is essentially another name (less commonly used) for (multiple) linear regression, although confusingly SPSS also has various “regression” modelling tools as well that produce different linear regression model but with slightly different options available or (somewhat pointless) restrictions compared to this tool. One of the main benefit of the General Linear Model - Univariate tool is that you can add categorical variables without first converting them to dummy variables (see later for an explanation of what this means).\nNext in the Univariate tool we add our outcome variable sbp to the Dependent Variable: box. Then we add our covariate(s). SPSS has separate boxes for numerical and categorical covariates, so we add numerical variables (e.g. bmi, salt and age) into the Covariate(s): box and categorical variables (e.g. sex, ses and ace) into the Fixed Factor(s): box (a factor is another term for a categorical variable). We can ignore the Random Factor(s): box and WLS Weight: box (see the help if you want to understand what they are for).\nFor our purposes let’s look at the association between bmi and sbp first, so add those variables into the relevant boxes.\nNext click the Save button and under Predicted Values tick the Unstandardized box, and then under Residuals tick the Unstandardized box, and then under Diagnostics tick the box for Cook’s distance. This tells SPSS to save the unstandardised predictions and residuals, and values for “Cook’s distance”. We will explain these later.\nLastly click the Options button, and then under Display tick the Parameter estimates box and click Continue and then OK. The output window will then pop up with the results, but first…\n\n\n\n\nStep 3: check the assumptions of linear regression\nBefore we look at the results that appear in the output window we must first check whether we can treat the results as robust and valid. Our results are only potentially valid if the assumptions of the linear regression model have been met/hold, i.e. if they have not been violated. When it comes to interpretation we would of course also have to consider all other potential sources of bias. Below we’ll go through the modelling assumptions and how to check them, which is more complicated than for the simpler statistical tests we ran previously.\n1. Continuous outcome\nTheoretically the outcome variable must be continuous, but like with t-tests this can be relaxed and you can safely use linear regression for discrete outcome variables as long as the other assumptions hold.\n2. Independent observations\nTechnically this means that the residuals or model errors (variation not explained by the model) of one observation should not be correlated/related (be able to predict) to the residuals of other observations. As with the t-tests you should be able to understand from your study design whether you have independent observations or not. There are two main reasons for non-independent observations. 1) You have outcome measurements on your units of observation at more than one point in time (repeated measures) that are all included in the outcome variable. 2) You have outcome measurements on your units of observation that are nested within a larger cluster, such as patients within facilities, where patients from the same facility are going to be more similar and have correlated outcomes compared to patients from different facilities. We know our study design (simple random sampling of participants) ensures our observations are independent, so we don’t need to worry about this assumption further. What if your data are not independent? See below.\nDealing with non-independent observations in linear regression modelling\n\n\nRead/hide\n\nThere are sophisticated and powerful ways of dealing with problems of non-independence, but we don’t go into them here. However, a simple solution for non-independent observations due to having multiple measurements across time is to just use observations from one time point only as your outcome (if this makes sense), or to take the average across all time points (if this makes sense), or to take the difference between your first and last time points and use these change scores as your new outcome (if this makes sense). And a simple solution for having non-independent observations due to clustering is to calculate summary values of the outcome based on all observations within each cluster. For example, if your outcome is a numerical variable, such as SBP, and you are looking at patients within facilities, then you can calculate the mean SBP across all patients in each facility, and then use the facility-level mean SBP as your outcome. For binary categorical variables (e.g. hypertension – yes/no) you can select one level (e.g. hypertension = yes) and calculate the proportion or percentage of observations in that level per cluster. For categorical variables with &gt;2 levels you have to create separate summary percentage variables for each level.\n\n3. Normally distributed residuals\nRemember that in linear regression the residuals or model errors are simply the differences between each observed outcome value and the model predicted value (based on the linear regression equation). Technically the assumption here is “multivariate normality of residuals or errors”. In practical terms this just means checking that the residuals are (approximately) normally distributed, which luckily is easy to do. Linear regression assumes normally distributed errors, and if this assumption is violated then the resulting confidence intervals and p-values for coefficients can be biased (usually too narrow and too small respectively).\nWhen we ran our linear regression using the General Linear Model – Univariate tool we told SPSS to calculate the “unstandardised” residuals for each observation and save them as a new variable, which SPSS will have called RES_1. To check whether the residuals are approximately normally distributed we could use a statistical test, but again this is sensitive to sample size and even if violated doesn’t necessarily mean our results won’t be robust, so it’s best to use a histogram.\n\nFrom the main menu go: Graphs &gt; Histogram. Then add the RES_1 variable into the Variable: box, tick the Display normal curve box and click OK. What can you conclude?\n\nHow are the residuals distributed?\n\n\nRead/hide\n\nThe residuals appear to be reasonably approximately normal so we can safely assume this assumption has not been violated in our model. If the residuals were not approximately normally distributed then we can try to transform the outcome to increase the normality of their distribution. You can use the same methods as discussed and practiced in the “Inferential analysis 2: the independent t-test applied to a skewed outcome” practical, specifically see the “Step 2: transform the outcome” section.\n\n4. Linearity of the associations between the residuals and the numerical variable(s)\nAnother key assumption that is necessary to avoid bias in inferences is that the residuals do not show any trends in their association with the covariate(s).\n\nWe will use a scatterplot. From the main menu: Graphs &gt; Scatter/Dot. Then select Simple Scatter and click Define. Add the RES_1 variable into the Y Axis: box and the bmi variable into the X Axis: box and click OK. What do you see?\n\nWhat is the association between the residuals and bmi?\n\n\nRead/hide\n\nThere is no clear non-linear trend as the residuals are scattered fairly evenly and linearly across values of age.\n\n5. Homoscedasticity: constant variance of the residuals across model predicted values\nThe technical name for this assumption is homoscedasticity, but in practical terms this just means that there should be no systematic pattern or change in the amount of variation (i.e. spread) in the residuals across the model predicted values (also called the model fitted values). If the residual variance changes with predicted values (most commonly increasing at higher predicted values) then this is known as heteroscedasticity, and this can again lead to confidence intervals and p-values for coefficients can be biased (usually too narrow and too small respectively). Again there are statistical tests available to “test” for this, but we will use graphical methods. Again this is very easy to check: we just plot the model predicted values against the model residuals and hope to see “random noise”.\nSee the following three figures for an illustration of some examples of clear heteroscedasticity and homoscedasticity:\n\n\n\nIllustrations of example heteroscedasticity and homoscedasticity residual patterns\n\n\nNote: these are in an idealised/very clear form. You will often see some tapering of points at either end of the main spread of points because there is usually less data at these places, but this is not usually something to worry about.\nLet’s now produce the necessary plot and check for violation of this assumption:\n\nFrom the main menu go: Graphs &gt; Scatter/Dot, then select Simple Scatter and click Define. Add the RES_1 variable into the Y Axis: box and the PRE_1 variable into the X Axis: box and click OK. Note: you can also get SPSS to make this graph via the Options menu in the General Linear Model Univariate tool, but it is a lot smaller and harder to see when made that way. What can we conclude?\n\nIs the homoscedasticity assumption violated or not?\n\n\nRead/hide\n\nThe spread of the residuals looks fairly even across all values of the model predictions for the outcome, so we can safely assume this assumption has not been violated.\n\nWhat if there had been a clear pattern in the residuals when plotted against the model predicted values?\nHow to deal with heteroscedasticity\n\n\nRead/hide\n\nThe first thing to do would be to try and find out why this was. This is usually either caused by 1) having an outcome that covers a very large range of values, because typically the variance will be greater at larger values, even if the model is correctly specified, 2) having an outcome that varies more at higher/lower values of a numerical covariate and/or varies differently between one or more categorical variable levels for whatever (genuine/real world) reason, or 3) having an incorrectly specified model, which means your model is either missing important and necessary non-linear term and/or interaction terms. This is beyond the scope of this module, but you can learn more about modelling non-linear associations and interactions via the materials on Minerva for the linear regression lecture.\nIdentifying scenario 2) and 3) involves exploring the association between the residuals and all covariates in turn to try and identify the culprits, and careful thinking about whether any key covariates may be missing from your sufficient adjustment set. While identifying scenario 1) involves ruling out scenario 2) and 3) and having an outcome with a large range of values (typically spanning a number of orders of magnitude).\nHow to solve this? If the problem appears to be due to scenario 3) you may be able to identify the culprit missing variable or missing functional form and update the model to solve the problem. If the problem involves scenario 1) or 2) then a simple and often sufficient solution is to use a linear regression model with “robust standard errors”, also known as “heteroskedasticity-consistent standard errors” or “Huber-White (robust) standard errors” (after the two inventors of the method). While we will not discuss or practice this solution further you can get an overview of how to run the method in SPSS here:\nhttps://www.ibm.com/support/pages/can-i-compute-robust-standard-errors-spss\n\n\n\nStep 4: consider additional possible issues (before interpreting the results)\nNo extremely influential observations\nSometimes single or multiple observations (individuals in our “SBP data final.sav” dataset) can have an excessive influence on the results, i.e. the coefficients and/or confidence intervals and p-values of your linear regression model, especially in smaller datasets. What this means is that including or excluding these observations from the analysis can change the results and potentially your overall interpretation of the results substantially. This is obviously not a good situation when the results are so sensitive to one or a few observations. If such observations exist in your model then you need to explore them and try and understand why they are so influential and whether they should be retained in the model. Observations can be influential in two main ways:\n\nOutliers: observations can have a large residual value, i.e. an outcome value that is unusually larger or small given its covariate values.\nLeverage: observations can have values for one or more covariates that are unusually large or small compared to all other values for that covariate.\n\nSeparately or together these two processes can give rise to observations that are highly influential. There are quite a few ways to explore these issues, but for time and simplicity we will just look at one widely used approach: the Cook’s distance (D) statistic. Without going into details for each observation in a dataset Cook’s D measures “how much” the values in a regression model change when that observation is removed from the model. Cook’s D starts at 0 with higher values indicating more influential observations. Various rules of thumb have been proposed for judging when a value of Cook’s D indicates that the observation should be looked at, but these may fail, and it is simpler and probably more robust to just judge (based on the type of graph we will produce below) whether any observations have a value of Cook’s D that is relatively much greater than all the other Cook’s D values. We already told SPSS to calculate Cook’s D as a new variable in the General Linear Model Univariate tool when we selected the Cook’s distance option in the Save options.\nThe easiest way to explore which observations appear to have excessively large values for Cook’s D is to create a scatterplot of Cook’s D against the observation ID variable (which is just a simple count from the first to the last observation).\n\nRemember in the main menu we go: Graphs &gt; Legacy Dialogues &gt; Scatter/Dot. Then select the Simple Scatter option and click Define. Then add the Cook’s D variable COO_1 to the Y Axis: box and the id variable to the X Axis: box and click OK.\nRemember you can interact with an SPSS graph by double clicking on it. We can then click twice on an observation to highlight it alone with a yellow circle, which allows you to then right click and select “Go to Case” to see that observation in the Data View. What do you see on the graph?\n\nInterpreting Cook’s D values\n\n\nRead/hide\n\nOne observation appears to have a clearly much higher value for Cook’s D than all the other observations. Interacting with the graph we can explore these observations, which has an ID of 520.\n\nWhat do you notice about the outcome and/or covariate values for these observations?\nWhat do you notice about these influential values?\n\n\nRead/hide\n\nObservation id 520 has a very large value for BMI of 41, but otherwise appears normal, so this is likely driving its influential power.\n\nWhat should we do? Generally unless you can be certain that an observation is influential due to an error in the outcome or an covariate then you should not make any changes to these values nor should you exclude the observation from your primary analysis. However, a simple, transparent and widely recommended approach is to conduct a sensitivity analysis by removing such observations from the dataset (i.e. create a copy of the dataset and then delete them entirely) and re-running your analysis to see if the results change substantially. If the results do not change in any important way then you should report this lack of change following the removal of the observations, but include the sensitivity analysis results in an appendix etc so readers can verify the truth of this. If the results do change substantially then it makes most sense to report both sets of results in the main paper and interpret accordingly, i.e. be clear that the conclusions change depending on whether such extreme observations are included or not. Either way you must be transparent and open about their effects.\n\n\nStep 5: understand the results tables and extract the key results\nSo now that we have verified that the assumptions of the model are not clearly violated, we can finally interpret our results. This is the really interesting and exciting part of any analysis! As you’ll have filled the output window with lots of graphs from the assumptions checking you may wish to re-run the linear regression again. Either way in the output window the results are presented in three tables.\nThe first table “Between-Subjects Factors” isn’t that useful (assuming we understand our data well), and just shows the number of observations in each level of each categorical variable. The second table “Tests of Between-Subjects Effects” shows an “ANOVA” table. This can be used to understand the “statistical significance” of each term (but only the overall term for categorical variables, not each level) in relation to how much variation it accounts for in the outcome variable. However, this is arguably of little value when our final table “Parameter Estimates” (which SPSS doesn’t provide by default!) provides us with both an estimate of the “statistical significance” (i.e. the p-value) of all terms including categorical variable levels, but also much more usefully it gives us the linear regression coefficient (i.e. the estimated direction and size of the association) and its 95% confidence interval for every covariate (or more specifically every term) in the model. Therefore, we will largely ignore the second table (apart from coming back to one piece of information it provides that should really just be in the “Parameter Estimates” table), and just focus on the “Parameter Estimates” table.\nSo what does it all mean?\n\n\nParameter estimates table columns explained\n\n\nParameter\n\nEach row indicates which term in the model the following results apply to. Terms are either the intercept or (in our case) main effects of covariates, or with more complicated models they may include interaction terms and/or non-linear terms like polynomial terms. For numerical variables this means one row per variable. However, because each level of a categorical variable is actually treated as a separate “dummy variable” (coefficient) as standard in a linear regression model each categorical variable level has its own row.\n\nB\n\nB stands for “betas”, because in the linear regression model when represented mathematically the coefficients are usually represented by the Greek letter beta. The betas are more commonly referred to as the linear regression parameter estimates or coefficients. They tell us the best estimate (point estimate) of the direction (positive or negative) and size of the association between each parameter in the regression model and the outcome variable.\nFor all numerical covariates they represent the expected mean change in the outcome variable for every 1-unit increase in the covariate.\nFor categorical variables it’s a bit more complicated. There are many different ways of looking at categorical variable effects, but the most common is called dummy coding, and this is the default presentation in SPSS and most (probably all) stats software. With dummy coding one level in the categorical variable (e.g. female in the variable sex) is set as the reference level. Then the coefficients for the other level(s) represent the expected mean difference in the outcome variable between each level and the reference level (e.g. male compared to female). Unfortunately (for no obvious reason) SPSS’s univariate general linear model tool does not display value labels for categorical variables in the “Parameter Estimates” table and so all you see are the numerical codes (you’ll have to check the value labels in the Variable View if you can’t remember the value coding). By default SPSS sets the level with the highest value as the reference level. You will notice that the reference level always has a coefficient value of 0, with a superscript letter “a” linking to a footnote explaining that “This parameter is set to zero because it is redundant”, i.e. it’s the reference level.\nA note on the intercept. You may be wondering what the “Intercept” parameter represents? In a simple linear regression with just one covariate this corresponds to the Y-intercept (hence the name), i.e. where the linear regression line crosses the y-axis (and the covariate or x-value is 0). In a multiple linear regression this represents the expected/model predicted mean value of the outcome when all numerical covariates have a value of 0 and all categorical variables are at their reference levels. Therefore the intercept will rarely have any useful interpretation (e.g. it assumes BMI = 0) and is usually ignored as a necessary but informative structural part of the model (unless you centre your variables, which we will not be looking at here).\n\nStd. Error\n\nThis is the standard error of the coefficient, which is an estimate of the sampling variability of the coefficient in the target population. This is used when calculating the 95% confidence intervals and p-value, but you can ignore these and just interpret the 95% confidence intervals and (if you wish) p-values.\n\nt\n\nThis is the t-statistic for the coefficient and is used when calculating the confidence intervals and the p-value associated with the coefficient. You can calculate 95% confidence intervals and p-values assuming normally distributed data, but using the t-distribution is more conservative (safe) for small sample sizes and is equal to assuming normally distributed data at large sample sizes. You can ignore these and just interpret the 95% confidence intervals and (if you wish) p-values.\n\nSig.\n\nThis is the two-tailed (although now it doesn’t mention that explicitly!) p-value associated with each coefficient. Again, assuming the “true” value of the coefficient in the target population is 0, the p-value represents the probability of observing a coefficient at least as great as that observed (positively or negatively as it’s a two-tailed p-value) due to sampling error alone.\n\n95% Confidence Interval (Lower Bound and Upper Bound)\n\nThese are the lower and upper 95% confidence intervals for the coefficient based on the t-distribution. As usualy, loosely speaking they represent a range of values that we can be reasonably confident contain the “true” coefficient that exists in the target population.\n\n\n\nThen lastly if we go back to the “Tests of Between-Subjects Effects” table and look at the footnotes we see one footnote: “a. R Squared = X (Adjusted R Squared = X)”. In a linear regression model the R² value represents the proportion (or % if you multiply it by 100) of variation in the outcome variable that is explained by the model, i.e. by all the terms of variables in the model. However, whenever you add a term to a model the R² value will increase even if the term is a random number variable, and has no true explanatory power for the outcome. Therefore, it is better to use the adjusted R2 value which makes a correction for the number of variables in the model. Note, R² values in health sciences are rarely as high as the one seen here, which is due to the artificial nature of the data.\nTherefore, typically we are just interested in the key descriptive statistics about the sample (number of observations and missing values, which are more easily obtained via separate descriptive analysis of the dataset), and in terms of the inferential results we want the parameter or coefficient estimates, their associated 95% confidence intervals (and possible their associated p-values), and often also the R² value of the model.\n\n\nStep 6: report and interpret the results\n\n\nNumerical covariates\nAs we’ve looked at the association between a numerical covariate and our outcome let’s consider how to interpret associations in general between numerical covariates and continuous outcomes via linear regression. In the next exercise we’ll look at categorical covariates.\nStatistical interpretation\n\nFor numerical covariates linear regression coefficients represent the model-predicted mean (or more loosely the average) change in the outcome (i.e. in units of the outcome) for every 1-unit increase in the covariate’s units, while holding the effect of all other covariates constant, i.e. they measure the mean independent association. Note: this change does not depend on the value of the covariate, i.e. the same association is assumed to exist across the full range of values that the covariate can take in the sample data, but it should not be considered to hold if you were considering values of the covariate outside of the range of values seen in the sample data.\n\nLooking at bmi in the parameter estimates table the point estimate (i.e. the single best estimate) of the regression coefficient is 4.2. This means that, based on the model, the point estimate of the association between age and systolic blood pressure in the target population is that for every 1-unit increase in bmi, i.e. for every 1 kg/m2 higher on the bmi scale that a participant is, the model indicates that the expected mean systolic blood pressure is 4.2 mmHg higher. If you were adjusting for other covariates, typically in the context of trying to estimate a causal association, then this interpretation would be whilst holding the effect of all other covariates constant.\nHowever, how sure can we be about the true direction and size of the regression coefficient (i.e. the association of interest) in the target population given our sample size and the sampling error in the sample? This is what our confidence intervals help us to estimate. Remember, formally they provide a range of values that, hypothetically speaking, if we were to repeat our study and analysis many times (technically an infinite number of times), would contain the true value of the regression coefficient in the target population X% of the time, where the true value of the regression coefficient would be the value of the regression coefficient that we would get if we measured every individual in our target population and ran the model, and X% is the confidence level (typically 95%). Informally and more loosely speaking a 95% confidence interval around a regression coefficient gives us a range of values that we can view as likely including the true value of the regression coefficient that exists in the target population (but we can’t say within that range which values are more/less likely).\nTherefore, in the parameter estimates table we can see that the 95% confidence intervals for the regression coefficient for bmi are 3.8 and 4.6. Consequently, we can be reasonably confident that the true value of the regression coefficient (i.e. the true linear slope) in the target population is between 3.8 and 4.6. And so because the 95% confidence intervals are fairly narrow we can conclude that we have obtained a reasonably accurate estimate of the likely association between age and systolic blood pressure in the target population. However, as always this is assuming there is no bias in the results, which in a real study is very unlikely, and therefore in a real study we must consider all likely sources of bias and their likely impacts when assessing the inferential results!\nPractical importance\nNow we know how to interpret the result statistically how do we interpret its real-world practical importance? For example, what can we conclude about its importance clinically and for public health programmes? These are complicated questions that have no simple answers and different people will have differing views depending on their views of the evidence (result) and the wider context. Most importantly you need to think carefully and critically and have strong subject matter knowledge to make robust interpretations and suggestions/recommendations. However, we can give some guidance about things to consider. You should clearly consider whether individuals or other units of observation can have their values of the covariate of interest moved or not, and what this implies for what clinical practice and public health programmes etc can or cannot do about that the characteristic represented by that covariate. For example, an individual cannot alter their height, while their age changes but out of their control, but their bmi can be affected by themselves or outside influences (e.g. interventions).\nFor example, our regression coefficient for bmi is 4.2, so clinically it appears that even slightly lower values of bmi are associated with quite big negative differences in expected mean systolic blood pressure of individuals. So practically it looks like it may make sense to focus intervention efforts on individuals with higher bmi, but take care as this is not necessarily a causal association.\nAnother thing to consider when interpreting the associations and their implications is the range in bmi values for individuals in the sample. Here bmi range from 17.8 to 41. So does it make sense to make statistical inferences about the association between bmi and systolic blood pressure for individuals with values of bmi outside of this range? Almost certainly not.",
    "crumbs": [
      "8. Regresson modelling",
      "8.1. Linear regression I"
    ]
  },
  {
    "objectID": "linear-regression.html#exercise-2-describe-the-population-level-association-between-socio-economic-status-and-systolic-blood-pressure-using-linear-regression-assuming-a-linear-association",
    "href": "linear-regression.html#exercise-2-describe-the-population-level-association-between-socio-economic-status-and-systolic-blood-pressure-using-linear-regression-assuming-a-linear-association",
    "title": "Multiple linear regression",
    "section": "Exercise 2: describe the population-level association between socio-economic status and systolic blood pressure using linear regression (assuming a linear association)",
    "text": "Exercise 2: describe the population-level association between socio-economic status and systolic blood pressure using linear regression (assuming a linear association)\n\nStep 1: explore the data\n\nCategorical covariates\nNow let’s see how to visually explore the association between categorical variables and a numerical outcome. To do this we can use bar charts or boxplots. Boxplots are probably better as they provide more information within the plot, although you can present much of the same information on a barchart if you know how using statistical software like R. Why would we want to do this, given you cannot have a non-linear association between a categorical variable and a numerical outcome? The main reason would be to inform us whether any levels within categorical variables might be suitable for merging/pooling together if necessary, and to understand whether the variance within category levels is approximately equal, which is a key assumption of linear regression.\n\nLook at the figure below to understand how to interpret boxplots:\n\n\n\n\nBoxplot explained\n\n\nLet’s look at the association between the outcome and the categorical variable socio-economic status with a boxplot:\n\nFrom the main menu go: Graphs &gt; Legacy Dialogues &gt; Boxplot, then select Simple and click Define. Add the sbp variable into the Variable: box and ses into the Category Axis: box and click OK. What do you see?\n\nWhat does the association between socio-economic status and systolic blood pressure look like?\n\n\nRead/hide\n\nIt looks like systolic blood pressure may be slightly lower on average as you move from low to medium to high socio-economic status.\n\n\n\n\nStep 2: run the regression model\nRepeat the above steps for exercise 1 step 2 but instead of adding bmi as the covariate when setting up the model add the ses socio-economic covariate.\n\n\nStep 3: check the assumptions\nThe assumptions remain the same as for exercise 1. The only difference is now the residuals are grouped. So instead of looking for homoscedasticity in relation to an even “spread” of our residuals vs a continuous covariate we can use a boxplot, as in step 1, to view whether the spread of residuals is approximately the same in each group.\nWe can also repeat the same process to look at the spread of the residuals vs the model predicted values, but we’re again looking for equal spread within each group of the covariate.\n\n\nStep 4 remains the same as for exercise 1, so we can repeat the same process.\n\n\nStep 5: understand the results tables and extract the key results\nYou can see the full details of what the tables contain in general back in exercise 1 step 5, but the key results are again the regression coefficients and their confidence intervals, which are contained in the “Parameter Estimates” table.\nSo again, let’s focus on the regression coefficients, which are in the “B” column, and their confidence intervals, which are in the “95% Confidence interval” set of two columns. We have four regression coefficients in the “B” column. We can see the “B” in row 1 is the intercept, so that’s not necessarily of interest, although here it represents the mean systolic blood pressure (mmHg) when the ses variable = 3. SPSS doesn’t make this particularly easy to work out, but we can see which level of the ses variable has been treated as the reference level by looking for the regression coefficient that has a valud of 0, and a superscript a linked to a footnote saying “This parameter is set to zero because it is redundant.” This is not very clearly telling us that ses = 3 has been taken as the reference level.\nIf we look at what the ses variable coding (e.g. right click on the variable in the “Data View” and click “Variable information”) we can see that ses = 1 = low, ses = 2 = medium and ses = 3 = high. So currently ses = high is the reference. Therefore, we can interpret the parameter estimates as follows. In the “ses=1” row we can see a “B” value of 6.013 with 95% confidence intervals of 1.5 and 10.5. This is telling us that in the sample individuals in the lowest socio-economic group have systolic blood pressure values that are on average 6 mmHg higher than individuals in the highest socio-economic group, and in the target population we sampled from the true difference in the mean systolic blood pressure for low socio-economic status individuals compared to high socio-economic status inidividuals is likely to be between 1.5 and 10.5.\nWe can also see that in the “ses=2” row we can see a “B” value of 1.895 with 95% confidence intervals of -3.072 and 6.863. This is telling us that in the sample individuals in the medium socio-economic group have systolic blood pressure values that are on average 1.895 mmHg higher than individuals in the highest socio-economic group, but in the target population we sampled from the true difference in the mean systolic blood pressure for medium socio-economic status individuals compared to high socio-economic status inidividuals is likely to be anywhere between -3.072 and 6.863 (i.e. we’re not even clear if the difference is positive or negative, let alone it’s likely magnitude).\n\n\nStep 6: report and interpret the results\nWe can report these results in the same way as illustrated for exercise 1 step 6.",
    "crumbs": [
      "8. Regresson modelling",
      "8.1. Linear regression I"
    ]
  },
  {
    "objectID": "paired-categorical-test.html",
    "href": "paired-categorical-test.html",
    "title": "McNemar’s test",
    "section": "",
    "text": "McNemar’s test\nLike with the paired t-test there is an alternative to the chi-square test of independence for the situation where you want to compare a binary outcome between two groups (i.e. a covariate with two levels) but where those two groups are paired. This test is known as McNemar’s test. A typical scenario where this test might be applicable is where you have a binary outcome, e.g. a covid test (positive/negative), with the outcome measured/recorded from the same set of individuals at two points in time, e.g. before/after an intervention. We will not look further at this test or practice its use in this course, but a good overview of the test with instructions of how to use it in SPSS can be found here:\nhttps://statistics.laerd.com/spss-tutorials/mcnemars-test-using-spss-statistics.php",
    "crumbs": [
      "7. Bivariate tests for categorical variables",
      "7.3. Paired categorical variable test (McNemar's test)"
    ]
  },
  {
    "objectID": "population-description.html",
    "href": "population-description.html",
    "title": "Population characteristics",
    "section": "",
    "text": "In this practical we will practice some common inferential analysis methods to estimate common univariate statistical properties of characteristics in a population.",
    "crumbs": [
      "4. Population description"
    ]
  },
  {
    "objectID": "population-description.html#descriptivesample-statistics-and-inferential-statistics",
    "href": "population-description.html#descriptivesample-statistics-and-inferential-statistics",
    "title": "Population characteristics",
    "section": "Descriptive/sample statistics and inferential statistics",
    "text": "Descriptive/sample statistics and inferential statistics\n\n\nRead/hide\n\nIt’s important to be very clear on the distinction between descriptive and inferential statistics.\nDescriptive statistics\nIn summary, descriptive statistics, which are sometimes also called sample statistics or summary statistics, summarise statistical properties of individual variables (via “univariate” analyses) or summarise associations between variables (typically via “bivariate” analyses) as they exist in your sample. For example, common univariate statistics are means, which summarise the typical value of a numerical variable, such the the mean systolic blood pressure in mmHg in the sample, and proportions/percentages, which summarise the frequency of occurrence of some event or condition, represented as one level of a binary/categorical variable, such as the proportion/percentage of smokers in the sample (as opposed to non-smokers).\nAssuming you have no sources of bias in your study these descriptive statistics will reflect the truth about your sample. For example, if you have no bias then the true mean systolic blood pressure in mmHg in your sample will be the sample mean of all the systolic blood pressure values.\nInferential statistics\nHowever, on their own these descriptive statistics do not allow you to make robust generalisations about the same statistical properties of individual variables or associations between variables in your target population. Remember, when we are interested in the statistical properties of individual variables or associations between variables in a target population we call the statistical quantities that reflect these properties “population parameters”, and we assume that they are fixed for the population and time point of interest. In theory, if we could take a census of the whole target population and measure our outcomes and associations of interest without error then we could measure our population parameters without error. However, almost always our target population is far too large to do this and we need to take a sample, ideally using robust, representative, probability sampling methods, as we have seen. The equivalent sample statistic is then our best “point estimate” of the population parameter of interest, but as the name implies it is an estimate with an unquantified amount of error.\nIt is easy to see why this is the case. Let’s assume we want to infer the mean systolic blood pressure in mmHg for some target population that contains 100,000 individuals. Let’s assume we take a simple random sample of just 2 individuals and measure, without error, their systolic blood pressure in mmHg. The mean of these 2 systolic blood pressures values will be the true mean systolic blood pressure for our microscopically small sample of 2! However, does anyone believe that a mean of just two individuals’ systolic blood pressures, however representative they are, is likely to accurately reflect the true mean systolic blood pressure for the overall target population? Of course by chance it might be really accurate, but in general we’re very likely to get a sample that produces an estimated mean that is not reflective of the true population-level mean. And if we instead took another simple random sample of 2 other individuals and computed a new mean systolic blood pressure it would be very likely to be different from the first mean. So if each sample mean would vary, and we can usually only collect one sample, how can we tell how accurate our sample mean is?\nThis is the problem of sampling error and sample size. Each sample would likely produce a different estimate of our fixed population parameter, and depending on the sample size the estimate would be likely to vary more/less between each repeated sample.\nTherefore, to let us infer the likely value of our population parameter we need to combine our sample statistic (i.e. our best point estimate) with some inferential measure. As we will discuss in the relevant lectures, this can be done most effectively by calculating the corresponding confidence intervals for the sample statistic, or a different approach involving hypothesis testing would be to compute an associated p-value. Note: this broad inferential approach is not just for when we are infering the statistical properties of individual variables in a target population, but also for when we are infering associations in a target population. We combine the relevant sample statistic with a suitable inferential measure.\nDescriptive statistics and descriptive studies/research\nThe terminology around descriptive statistics and descriptive studies can be a source of confusion. The key thing to remember is that descriptive statistics describe samples only, and are used in all studies to initially explore our data, plan our analyses, and describe the key characteristics of the sample so we can judge how representative our sample is compared to the target population in terms of these key characteristics. Whereas a descriptive study, or a study where one or more quantitative research questions are about description, is almost always about the goal of describing characteristics/associations in a target population via sample and using inferential statistics (even if that target population is not clearly specified). You can certianly find studies that have only described a sample alone using sample statistics and no inferential measures, but in my experience that always seems to be because the authors have misunderstood statistical inference and don’t seem to understand what they are doing or how limited their results are.\nSo if studies say they are aiming to describe characteristics or associations in a given target population, and they have taken a sample from that target population to do this, then if they know what they are doing then they mean that they are going to use inferential statistics (typically confidence intervals around point estimates) to try and infer the likely (but ultimately unknown) values for those characteristics/associations in the target population.",
    "crumbs": [
      "4. Population description"
    ]
  },
  {
    "objectID": "population-description.html#confidence-intervals",
    "href": "population-description.html#confidence-intervals",
    "title": "Population characteristics",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\n\nRead/hide\n\nInterpretation\n\nNote: formally the interpretation of a confidence interval applies to a fully defined target population that you have sampled using a probability sampling method. Once you start trying to interpret a confidence interval in relation to a target population that you haven’t sampled from, or based on an analysis of a sample taken using some kind of purposive approach, then strictly speaking there is no robust, mathematical basis for that interpretation. However, you can certainly think about interpreting a confidence interval in relation to a different target population that you didn’t sample from, and you can certainly calculate confidence intervals based on purposively sampled samples, but you need to carefully consider how these factors affect the robustness and validity of the interpretation.\n\nConfidence intervals take the form of a lower and upper value that surround the point estimate. For example, let’s say we estimate the mean age in a target population as 30, based on computing the mean age in a simple random sample from that population. Let’s assume we then compute a 95% confidence interval around that mean with lower and upper values of 25 and 35. We would usually write these statistics all together as:\n\nMean age: 30 (95% CI: 25, 35)\n\nWhere CI = confidence interval. Note, we separate the lower and upper interval via a comma not a hyphen “-”, as hyphens can be mistaken for negative symbols.\nStrictly speaking confidence intervals are a mathematical statement about hypothetical resampling in relation to our point estimate of interest. Unfortunately, this can be quite a confusing, technical concept. More concretely, what this confidence interval is telling us is that if we were to repeat our random sampling process infinitely many times and each time we were to compute the mean age and its 95% confidence interval then 95% of those 95% confidence intervals would contain the true (but unknown) value of the mean age in the target population. Hence, we can never be certain what the exact mean age of the target population is based on the confidence interval, but it does give us an interval (i.e. a range of values) within which it is likely to lie. Yes, this is a frustratingly indirect statement about the true mean that we want to measure, but that is how the theory works, and that is the technically correct interpretation.\nNote: many people, including many researchers/papers, would say we can be “95% confident” the true (but unknown) population parameter lies within the 95% confidence interval. However, technically speaking this phrase “95% confident” doesn’t have any precise, concrete, mathematical meaning. Also, the probability that the population parameter of interest lies within a given confidence interval is either 100% or 0%. We could know which of these scenarios was true if we were able to measure the whole population and calculate the population parameter. So it’s pretty questionable whether this is a valid or useful approach, but you will see it often.\nNote: There is nothing special about a 95% confidence interval compared to a higher or lower level of confidence, such as a 90% confidence interval. However, for a given variable as you increase the confidence level the interval increases. For example, for our hypothetical situation above we may find a 99% confidence interval around our mean age of 30 is 10 and 50. While we may find a 80% confidence interval would be from 28 to 32. The interpretation remains the same. For example, for 99% confidence intervals we obtain confidence interval limits that, if we were to repeat the sampling and estimation and infinite number of times, would contain the true population parameter value 99% of the time.\nTherefore, there is a trade-off in the usefulness of the confidence interval as you vary the confidence level. Too high a confidence level results in a high level of certainty but for a very wide interval, while too low a confidence level results in a low level of certainty but for a very narrow (precise) interval. The widespread use of 95% confidence intervals in the health sciences is largely just a convention, i.e. due to tradition. For a given confidence level, all else being equal, if we increase the sample size the interval will become narrower. Therefore, if you have sufficient sample size then a higher level of confidence would clearly be preferable.\n\nNote: it is absolutely critically important to be aware, and when interpreting a confidence interval to always remember that, the accuracy or validity of the above interpretation of a confidence interval depends entirely on there being no bias in the study. Confidence intervals quantify the amount of variation in the sample data relative to the sample size. So they allow us to decide how closely we are likely to have estimated the population parameter of interest given the sample size and variation in the outcome, which may come from the fundamental/natural variation in the outcome’s values in that target population, and some extra random variation may come from non-differential error like truely random measurement error.\n\n\nHowever, confidence intervals tell you nothing about any sources of bias, such as selection bias, or any non-random (differential) sources of error, like non-random missing data or non-random measurement error. Remember, study bias can occur throughout the entire study cycle, from planning the data collection tools through to reporting the study results. Therefore, if the study results are affected by any sources of bias, which is inevitable to some extent, then the accuracy and validity of the confidence interval will be affected, usually to an unknown extent. For example, using the age example from before: if older people were less likely to agree to participate in the survey that collected their age data, then this would be a form of selection bias, and the resulting 95% confidence interval and point estimate for the mean age of that population would be skewed downwards, and the confidence intervals may no longer include the true (but unknown) value of the mean age in the population 95% of the time. Again, it is usually not possible to estimate the effect of such biases, so they must be judged more qualitatively via rigorously exploring and understanding all possible sources of bias and the likely size of their impact.\n\nComputing confidence intervals\nDon’t worry: we will leave the computation of our confidence intervals up to the software. However, when computing a confidence interval we need to make an assumption about the type of probability distribution that we assume the sample statistic of interest comes from (i.e. the probability distribution we would see if we were to take many repeated samples, calculate the sample statistic each time, and plot their distribution on a histogram). Here we will just be focusing on estimating population-level means for numerical outcomes and percentages for the levels of a binary/categorical outcome.\nFor continuous variables a reasonable assumption about the appropriate probability distribution will often be the normal distribution, and if we plot the distribution of the variable’s values and they are approximately normal it is usually safe to assume a normal distribution will be appropriate for the confidence interval. If the distribution is skewed, as will usually be the case for discrete variables such as counts, we can try and transform the distribution of the outcome variable before computing the confidence interval, while still assuming a normal distribution when computing the confidence interval, or we can use a more appropriate distribution, but that is beyond this course.\nFor binary/categorical variables you can also assume a normal distribution, even though this will never be correct, as long as it is approximately correct. When the percentage being estimate isn’t too small or too large, then the normal distribution is usually an okay approximation. “Too small” and “too large” are usually taken to be &gt;10% and &lt;90%, but these are just really rough rules of thumb. However, we can compute confidence intervals that make assumptions about the likely probability distribution of the sample statistic that are more appropriate for binary/categorical variables. There are actually many such possible methods, with little clear evidence on which is best and under what circumstances. We will look at this in a bit more detail in the exercise.",
    "crumbs": [
      "4. Population description"
    ]
  },
  {
    "objectID": "population-description.html#scenario",
    "href": "population-description.html#scenario",
    "title": "Population characteristics",
    "section": "Scenario",
    "text": "Scenario\nYou and your colleagues have been tasked by the Kapilvastu district authorities in Nepal to help them understand the problem of high blood pressure and hypertension, and the associations between socio-demographic and health related characteristics and blood pressure level/hypertension. You have carried out a cross-sectional survey to address these aims, and collected data on systolic blood pressure, common socio-demographic characteristics, and some additional health-related characteristics. So far, you have cleaned and prepared the data, and computed descriptive statistics for key characteristics. As per your statistical analysis plan you now need to use the data to estimate the likely mean systolic blood pressure and prevalence (%) of hypertension for the whole target population and for women and men separately.",
    "crumbs": [
      "4. Population description"
    ]
  },
  {
    "objectID": "population-description.html#exercise-1-estimate-a-mean-and-a-proportionpercentage-and-their-associated-95-confidence-intervals-in-relation-to-the-overall-target-population",
    "href": "population-description.html#exercise-1-estimate-a-mean-and-a-proportionpercentage-and-their-associated-95-confidence-intervals-in-relation-to-the-overall-target-population",
    "title": "Population characteristics",
    "section": "Exercise 1: estimate a mean and a proportion/percentage and their associated 95% confidence intervals in relation to the overall target population",
    "text": "Exercise 1: estimate a mean and a proportion/percentage and their associated 95% confidence intervals in relation to the overall target population\n\nAim: estimate the mean systolic blood pressure (mmHg) and the prevalence (%) of hypertension and their associated 95% confidence intervals to allow statistical inferences to be drawn about the typical level of systolic blood pressure and the prevalence of hypertension in the overall target population.\n\n\nFirst, load the analysis-ready “SBP data final.sav” SPSS dataset.\nNext, in the “Exercises” folder also open the “Exercises.docx” Word document and scroll down to Population description. You can enter the requested values here and then compare them to the answers given.\nNext, follow the instructions below on how to compute two common univariate inferential statistics for numerical and categorical variables, specifically means and percentages, with associated confidence intervals to allow inference about these measures in the entire target population.\n\n\nSorry: at present there is no video-based set of instructions for this method.\nWritten instructions: calculating and their 95% confidence intervals for continuous/discrete numerical variables with approximately normal distributions in SPSS\n\n\nRead/hide\n\n\nTo compute a confidence interval for any sample statistic requires us to make an assumption about the probability distribution we believe the sampling distribution of the sample statistic to have come from. We can make this decision based on our knowledge of the likely data generating process and the shape of the distribution (as visualised via a histogram). If a numerical variable is approximately normally distributed then we can describe the typical value (or more technically the central tendency) via the sample mean and compute a 95% confidence interval around this mean, to allow us to make a statistical inference about the likely population mean.\nWhen a numerical variable is approximately normal we can usually safely compute a confidence interval based on a normal distribution (i.e. we are unlikely to go too far wrong and compute a very biased confidence interval if this is the case). However, in practice most software will actually compute one based on a t-distribution. This is because a t-distribution is appropriate for continuous data that follow a a symmetrical, bell-curve shape, like the normal distribution, but it allows for more variation at smaller sample sizes by having wider “tails” at either end of the distribution. This can be shown to give more accurate confidence intervals that include the true population parameter at the desired coverage rate. For example, a 95% confidence interval for a mean based on a small sample size (e.g. &lt;30) computed via the normal distribution might exclude the true population parameter &gt;5% of the time, while a t-based distribution should exclude it only 5% of the time as desired. Technically, if you use a normal distribution to compute a confidence interval this is only valid when the sample size is infinite, and at larger sample sizes the normal distribution and the t-distribution become equivalent, so there is no real reason not to use the t-distribution over the normal when possible.\n\n\nNote: technically speaking we are just considering analytical confidence intervals based on probability distribution functions here. However, there are empirical/numerical approaches such as bootstrap confidence intervals, which you may can read a bit about here\n\n\nSo let’s check that the variable is approximately normally distributed, otherwise the mean may not be the best measure of the typical value of the variable, and its 95% confidence intervals would not be accurate (technically speaking they may under or over cover the true interval on average).\nFrom the main menu go to: Graphs &gt; Histogram Then in the Histogram tool click on the sbp variable and add it to the Variable: box by clicking the blue arrow next to the box. Tick the Display normal curve box below. This will display a theoretical normal distribution curve based on the mean and SD of the variable, which helps you to see how closely the data match the distribution we would expect if they were normally distributed. Then just click OK.\nWhat does the distribution of the data look like? I would say there is no evidence of any important skew.\nNow let’s compute the mean of the variable and a t-based 95% confidence interval to allow us to make a statistical inference about the likely value of the true (but unknown) mean systolic blood pressure in mmHg in the entire target population.\nTo compute our t-based 95% confidence intervals we will use a “one-sample t-test”. There are other types of “t-tests” that we cover, which are used for comparing means between groups, but we don’t look at the one-sample t-test in detail as it is rarely used. If you wish to read about it a brief overview with SPSS instructions is here. In brief though, a one-sample t-test tests the hypothesis that the mean of the sample variable equals a given value that you choose. We are not interested in the hypothesis testing aspect of this test, but we can use it to give us the mean of a numerical variable and the t-based 95% confidence intervals for that estimated mean.\nTo do this from the menu go: Analyze &gt; Compare Means &gt; One-Sample T Test. Then add the sbp variable into the Variables: box using the arrow. Note that we leave the Test Value: box with the default 0 (see below for why). Then click OK to run the test.\nYou will get three tables returned in the results window. The “One-Sample Statistics” table presents the sample size for the variable, which is useful because you should always present the sample size (i.e. count) for any estimate and the relative frequency (e.g. percentage) of complete/missing data for the variable.\n\n\nNote: by default SPSS removes any observations from the variable with missing values when computing the results.\n\n\nYou can then look at the “One-Sample Test”. The first column we are interested in is the “Mean Difference”. Technically, the one-sample t-test compares the mean of the variable to an assumed population mean value that you choose. By default SPSS sets this value as 0, and indeed as above we left it as 0. Therefore, when the test compares the difference between the outcome mean and 0 it is subtracting 0 from the outcome mean, and of course anything minus 0 is just itself. Hence, the “Mean Difference” is actually just our outcome mean. You can also see the sample mean or point estimate of the population mean in the “One-Sample Statistics” table, but they will match. Then we can also see the t-based 95% lower and upper confidence intervals for the mean difference under the “95% Confidence Interval of the Difference” “Lower” and “Upper” columns. Again though, while these are technically the confidence intervals for the mean difference as the difference is between the outcome mean and 0 we can just treat them as confidence intervals for the outcome mean.\n\n\n\nWritten instructions: calculating proportions/percentages and their 95% confidence intervals for categorical variables\n\n\nRead/hide\n\n\nNote: this approach is applicable to both binary variables or categorical variables with three or more category levels. In the case of a categorical variable with three or more category levels we just treat each category level as a separate binary variable in practice (with each unit of observation either being in that category or not). For example, for the variable “education level” with category levels “none”, “primary”, and “more than primary” we estimate the proportion/percentage of individuals in the 1) “none” category vs “primary or more than primary”, 2) the “primary” category vs “none or more than primary”, and 3) the “more than primary” vs “none or primary”.\n\n\nAs above, to compute a confidence interval requires an appropriate probability distribution given the data. Clearly, a binary variable cannot formally be normally distributed: it can only take values of 0 and 1. However, as long as the underlying proportion is not “too extreme” we can actually treat it as an approximately normally distributed variable and compute normal-based confidence intervals. By “too extreme” the usual rule of thumb is the proportion for the variable must be &gt;0.1 and &lt;0.9. Beyond these limits the implied distribution becomes too non-normal for the approximation to be useful. For example, the lower/upper confidence interval will often be &lt;0 or &gt;1, and the bias in the accuracy of the confidence interval’s coverage becomes more substantial.\nAlternatively and more appropriately, we can use a probability distribution formally applicable to a binary variable and our assumed data generating process, such as the binomial distribution. Generally, such a confidence interval is called a binomial proportion confidence interval.\nUnlike for numeric variables that are approximately normal there are actually a large number of different approaches available though, and there is a surprisingly limited amount of research comparing these methods under different situations. However, the current rough consensus appears to be that the “Wilson score interval” method with the continuity correction generally does well. There is a Wikipedia page with a great summary of the situation [here] (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Comparison_and_discussion).\nSPSS offers the option to calculate a big range of these different binomial proportion confidence intervals all at the same time. Let’s see how we can do this for a categorical variable with three category levels for each category level.\nFrom the menu go Analyze &gt; Compare Means &gt; One-Sample Proportions. Then add the htn variable from the Variable box into the Test Variable(s): box using the blue arrow.\nNext click on the button labelled “Confidence intervals” at the top right. Then under Select Type(s) tick Clopper-Pearson (“Exact”), which is based directly on the binomial distribution, Wald, which just uses the normal distribution (often called normal approximation confidence intervals), Wilson Score and lastly Wilson Score (Continuity Corrected). Then click Continue.\nNow we need to decide which category level we want to calculate our proportion and confidence intervals for. If you right click on the htn variable and select Variable Information you can see that the variable is numerically coded with 1 = hypertension and 0 = no hypertension. So we want to look at the proportion/percentage of individuals coded 1 (with hypertension).\nTo do this in the Define Success area select the button next to Value(s) and in the box to right of this button enter the value 1. Then click OK.\n\n\nNote: if you have a categorical variable coded with letters or words instead of numbers you can also just enter the relevant letter or word into the Define Success: Value(s): box to compute the proportion and associated 95% confidence interval for that category level.\n\n\nAll the results we want are in the “One-Sample Proportions Confidence Intervals” table. As you can see the results for each confidence interval method are on a separate row, and you can see which method is on which row under the “Interval Type” column. Under the “Observed” heading there are three columns giving the number of “Successes”, which means the count of observations (i.e. individuals) in the category we selected, the number of “Trials”, which just means the number of observations or the sample size of the variable, and the corresponding “Proportion”. You can of course multiply this by 100 to convert to the percentage scale. We then have the lower and upper 95% confidence intervals under the correspondingly named final two columns. As you can see for this variable and these methods they actually give very similar results, and indeed the method using the normal approximation (“Wald”) is pretty much the same as the other methods. You would see larger differences if estimating proportions nearer 0 or 1.\n\n\n\nYou can check that the results you obtained are correct by revealing the correct results below:\n\n\nRead/hide\n\nFor systolic blood pressure we can see that the sample mean systolic blood pressure is 126.5 mmHg, and the 95% confidence intervals indicate that the population mean is likely to be between 124.9 and 128.2 mmHg, which we can compactly write as “mean systolic blood pressure = 126.5 mmHg (95% CI: 124.9, 128.2)”.\nTherefore, we have a pretty accurate estimate given the sample size and variability in the outcome values (assuming no bias, which is always a big and unlikely assumption).\nFor the hypertension prevalence we first need to consider which result to use. While all the results are pretty similar, some evidence suggests that the Wilson Score approach can perform better (give more reliable inferences), so using that result we can see that 25% of individuals (136/543) have hypertension in the sample, and the % in the target population is likely to be between (i.e. the 95% confidence interval is) 21.6% and 28.9%. Again, we can write this compactly as “hypertension prevalence = 25% (95% CI: 21.6%, 28.9%)”.\nSo again, given our sample size we’ve got quite an accurate estimate of the population parameter (assuming no bias, which is always a big and unlikely assumption).",
    "crumbs": [
      "4. Population description"
    ]
  },
  {
    "objectID": "population-description.html#exercise-2-estimate-means-and-proportionpercentages-and-their-associated-95-confidence-intervals-in-relation-to-subgroups-within-the-target-population",
    "href": "population-description.html#exercise-2-estimate-means-and-proportionpercentages-and-their-associated-95-confidence-intervals-in-relation-to-subgroups-within-the-target-population",
    "title": "Population characteristics",
    "section": "Exercise 2: estimate means and proportion/percentages and their associated 95% confidence intervals in relation to subgroups within the target population",
    "text": "Exercise 2: estimate means and proportion/percentages and their associated 95% confidence intervals in relation to subgroups within the target population\n\nAim: for women and men estimate the mean systolic blood pressure (mmHg) and the prevalence (%) of hypertension and their associated 95% confidence intervals to allow statistical inferences to be drawn about the typical level of systolic blood pressure and the prevalence of hypertension in these two subgroups within the target population.\n\n\nWe will continue to use the same dataset (“SBP data final.sav), and we will be computing the same measures. The only difference is we will do this separately for women and men. Follow the instructions below, which involve creating new variables before repeating the process from the previous exercise to compute the results of interest.\n\n\nSorry: at present there is no video-based set of instructions for this method.\nWritten instructions: calculating means, percentages and their 95% confidence intervals for subgroups in SPSS\n\n\nRead/hide\n\nCreate new outcome variables for women and men\n\nWe will compute new outcome variables for each subgroup (sex), with the values of the outcomes missing for the subgroup that is not of interest (i.e. the female outcome variable will have missing values for all males in the dataset and vice versa). Then we will just repeat the same process as for exercise 1 to compute our desired measures for each variable, and they will therefore be specific to each subgroup only.\nFirst we’ll create a new women-only systolic blood pressure variable. From the main menu go Transform &gt; Compute Variable.\nLet’s call our new variable “sbp_fm”. In the Compute Variable tool window that appears in the Target Variable box type “sbp_fm”. Now double click on the sbp variable in the variable list. It should appear in the Numeric Expression box. If we don’t add any other commands to this logical expression all it’s telling SPSS to do is to create a new variable with the name given in the Target Variable box, where the values are just the same values as those in the sbp variable. So effectively just copying row-by-row the values from sbp and pasting them into sbp_fm. However, we only want to do this for those values from the women in the dataset.\nTherefore, next in the bottom left of the tool window click the blue-bordered If.. button. This is where we will tell SPSS to only copy and paste if the observations are from women. At the top of the tool window select the button that is next to the text Include if case satisfies condition:. Double click on the Sex (male/female) variable in the variable list. It should appear in the box below the text reading “Include if case satisfies condition:”. Add the text “= 2” so that the full text in the box reads:\n\n\nsex = 2\n\n\nYou can also just copy and paste this command. This is a logical condition telling SPSS to only carry out our logical expression (copying and pasting) for observations where sex = 2. If you right click on the sex variable and select Variable Information you can see that this categorical variable is numerically coded and the value 2 represents women.\n\n\nNote: if you wanted to do this with a categorical variable that was string coded you would just type the relevant string. For example, if the sex variable actually had values of “male” or “female” to select males you would type (including the quotation marks):\n\n\nsex = “male”\n\n\nFinally, click Continue to get back to the original tool window and then click OK. You should see a new variable appear to the right of your existing variables called sbp_fm. If you scroll down you will see that when you get to the men in the dataset the new variable only has missing values. This is what we want.\nNow repeat the above process but call your new variable sbp_m and use the following command in the If.. logical selection part:\n\n\nsex = 1\n\n\nYou should now have two new variables for the systolic blood pressure values of just the women and just the men in the dataset.\nYou can now repeat this process for the hypertension variable, creating two new variables: one for women’s hypertension outcomes and one for men’s hypertension outcomes.\nYou can then repeat the process used in exercise 1 to estimate the mean systolic blood pressure and the % prevalence of hypertension and their associated 95% confidence intervals for women and men separately. Simply use the separate sex-specific systolic blood pressure and hypertension outcome variables that you have created in place of the overall ones you used previously. SPSS will automatically ignore/exclude any missing values, which for each sex-specific outcome variable will include the values for all individuals of the opposite sex.\n\n\n\nYou can check that your results match what they should be by revealing the correct results below:\n\n\nRead/hide\n\nSystolic blood pressure (mmHg)\nWomen\n\nMean systolic blood pressure = 120.5 mmHg (95% CI: 118.1, 122.8)\n\nMen\n\nMean systolic blood pressure = 132 mmHg (95% CI: 129.7, 134.2)\n\nHypertension (confidence intervals based on Wilson Score method)\nWomen\n\n% hypertension = 16% (95% CI: 12%, 20.9%)\n\nMen\n\n% hypertension = 33.2% (95% CI: 28%, 38.9%)",
    "crumbs": [
      "4. Population description"
    ]
  },
  {
    "objectID": "probability-sampling.html",
    "href": "probability-sampling.html",
    "title": "Probability sampling",
    "section": "",
    "text": "In this practical we will practice carrying out three common probability sampling methods using Excel.",
    "crumbs": [
      "1. Probability sampling"
    ]
  },
  {
    "objectID": "probability-sampling.html#key-terminology-units-of-observation-and-analysis",
    "href": "probability-sampling.html#key-terminology-units-of-observation-and-analysis",
    "title": "Probability sampling",
    "section": "Key terminology: units of observation and analysis",
    "text": "Key terminology: units of observation and analysis\n\n\nRead/hide\n\n\nSampling unit: these are the entities or things that you sample for the study. These will often be individuals but could be anything, such as households or health facilities. The sampling unit should be fully defined in the description of the study’s sampling methods.\nUnit of observation (also called study units or study subjects): these are the entities or things that you collect data from. Again, these will often be individuals but could be anything, such as households or health facilities. Note that you might sample “higher level” units, such as entire households, but collect data from “lower level” units, such as individuals within those households. The unit of observation should be fully defined by the study’s eligibility criteria.\nUnit of analysis: the entity or thing that you analysing in your study and thereby strictly aim to make statistical inferences about using the data collected. This will often be the same as the unit of observation, but it may differ depending on how you are analysing the data. For example, the unit of observation may be individuals, but the unit of analysis may be the household if you aggregate individual outcomes within each household (e.g. as a single mean or percentage value per household).",
    "crumbs": [
      "1. Probability sampling"
    ]
  },
  {
    "objectID": "probability-sampling.html#overview-of-key-concepts",
    "href": "probability-sampling.html#overview-of-key-concepts",
    "title": "Probability sampling",
    "section": "Overview of key concepts",
    "text": "Overview of key concepts\n\n\nRead/hide\n\nProbability sampling\nBroadly speaking, sampling is the process whereby we choose units of observation within our target population to collect data from. Therefore, the first stage of sampling, which should always occur very early on in the research and study design process, is when you clearly and explicitly define your target population. In summary, the only sampling methods that are guaranteed to produce unbiased statistical inferences, albeit only on average (or to use the more technical term “in expectation”, meaning in the long-run), are “probability sampling” methods. More loosely these are sometimes referred to as random sampling methods. This is because all frequentist statistical methods of analysis assume that the sample data have been randomly sampled from a given target population using some specific probability sampling method. Therefore, the only way to robustly satisfy this assumption is clearly to use some form of probability sampling method.\nTo take a probability sample you require a sampling frame, which is a list of all units of observation in the target population that you are aiming to generalise your results to (e.g. all individuals in a region, all primary care health facilities in a country etc). Therefore, the second stage of any sampling process is to obtain or create a sampling frame for your target population. Note: this excludes multi-stage cluster sampling methods, where you require a sampling frame for the first stage clusters, but at the second/subsequent stages you usually create the sampling frame via a mapping process, as none usually exist.\nNote: for a probability sampling method to be valid it must be possible to calculate each unit of observation’s probability of being selected, which may or may not be equal for all units, and should not be zero for any unit. For example, if the target population contained 100 individuals and we randomly select 10 individuals via simple random sampling then the probability of being sampled would be 10/100 = 0.1 for each individual.\nNon-probability sampling\nWhile desirable it is clearly often not feasible to use probability sampling methods, usually because of the lack of a suitable sampling frame. For example, if you are studying patients attending a healthcare facility over a given period and you need to collect data on sampled patients on the day they attend the facility it’s typically impossible to construct a sampling frame because you won’t know who will be visiting each day! Therefore, you may often need to resort to non-probability sampling methods such as consecutive sampling, while minimising the opportunity for any researcher sampling bias, and hope that your samples are representative of your target population.",
    "crumbs": [
      "1. Probability sampling"
    ]
  },
  {
    "objectID": "probability-sampling.html#exercise-1-simple-random-sampling",
    "href": "probability-sampling.html#exercise-1-simple-random-sampling",
    "title": "Probability sampling",
    "section": "Exercise 1: Simple random sampling",
    "text": "Exercise 1: Simple random sampling\n\nA brief overview of the method\nSimple random sampling involves taking a random sample of a given size from a sampling frame, which results in each unit of observation having an equal probability of being selected. You can take a simple random sample from a sampling frame in various ways, such as by generating random numbers that correspond to IDs that are pre-allocated to all units of observation, or you can do it by simply randomly sorting the list and just selecting the first n units of observation, where n is your sample size. We will use this second approach as it’s easy to implement in Excel.\n\n\nAdvantages and disadvantages\n\n\nRead/hide\n\n\nAdvantages\n\nEasy to implement and explain.\nRequires minimal data on your target population units of observation: just a list of all units of observation, plus usually some way of identifying/contacting them once selected.\nStatistically efficient: you maximise the precision/power of your inferential analyses when you use simple random sampling.\n\nDisadvantages\n\nAlthough simple random sampling guarantees that you will select a sample that is representative (i.e. unbiased) of your target population on average/in the long-run, i.e. over many hypothetically repeated samples, it is not the best approach at achieving this goal for any given sample, and obviously in practice you typically only ever take one sample! This is especially the case when the population is heterogeneous (highly varied in the characteristics/associations of interest) and/or if sample size is small, say in the tens or low hundreds rather than the high hundreds or thousands (as the sample size increases the chances of getting an unrepresentative sample decrease). As these two situations are often true it is often better to use stratified random sampling instead, if possible. Other approaches to deal with this issue also exist but are beyond the scope of this module.\nIf your sampling units cover a large geographical area simple random sampling can produce a very logistically inefficient, costly, and geographically spread-out sample. See cluster sampling for an possible solution to this problem.\n\n\n\n\nScenario\nYou work for a district governmental health department and you have been tasked with assessing the capacity and service delivery characteristics of the public primary care facilities across the whole district. Specifically, you need to report the typical staffing levels, resources and equipment levels, and services delivery levels for all public primary care facilities (e.g. the mean no. drs and nurses per facility.\nHowever, while there are 246 such facilities within your district you only have resources to survey 50 facilities (if you could survey all 246 it would be a “census” not a sample). You run a sample size calculation that indicates a sample size of 50 will be sufficient to provide usefully precise, albeit fairly rough, estimates of these characteristics at the district level (note a sample size of 50 is typically going to be too small for a real study, but we are only practicing sampling here so it doesn’t matter). The target population to which you want to be able to generalise your results is therefore all 246 facilities.\nLuckily there is an existing comprehensive list (i.e. sampling frame) of all 246 existing public primary care facilities in your district. You have a copy of this list, in the form of an Excel spreadsheet, that includes facility names, addresses and telephone numbers, plus additional information that we will use in subsequent exercises.\n\nOur aim is therefore to take a simple random sample of 50 facilities from the list.\n\n\n\nExercise: taking a simple random sample using Excel\n\nGo into the “Datasets” folder that you should have moved to a suitable folder on your computer and load the “Health facilities list - simple random sample.xlsx” Excel spreadsheet. If you haven’t downloaded the datasets for the computer practical sessions and moved them to a suitable folder go to Datasets.\n\nVideo instructions: taking a simple random sample using Excel\nWritten instructions: taking a simple random sample using Excel\n\n\nRead/hide\n\n\nIn the “Health facilities list - simple random sample.xlsx” Excel spreadsheet you will see it has various self-explanatory columns/variables including “facility_name”, “facility_address”, and “facility_tel”.\nIn column A (the blank column immediately to the left of the “facility_name” column) enter the following (or similar) word as a heading: random_no\nImmediately under this new column in cell A2 (the first row where the facility data starts) click on the cell and type =rand() and press enter. This Excel function generates a random number between 0 and 1 to six decimal places.\nThen simply ensure that cell A2 is selected (i.e. you’ve clicked on it) and then just double click on the small solid square at the very bottom right of this cell. This should copy and paste the function all the way down to the end of the facility data.\nNext click on the “random_no” column heading (cell A1) and then in the menu “ribbon” click on Data and click the Filter tool. You should see little “drop-down” menu buttons appear in the right of each column heading. Click on the drop-down menu button in the “random_no” column heading (cell A1) and select Sort Smallest to Largest.\nThis will immediately sort all the data in order from those in the same row of the smallest random number value to the largest. Therefore, the list will now be randomly sorted! Note: the random numbers will all change immediately after sorting so they will actually no longer be ordered. This is because they are functions and will get re-calculated each time you change anything. However, this doesn’t matter because once all data have been sorted based on the original random numbers we don’t need those original values anymore.\nNext, look at the bottom left of the window and you should see a tab titled Sheet1. This is the current worksheet. Click on the little + symbol to the right of this sheet to create a new worksheet. This will automatically be given the name “Sheet2”.\nNow click back on the Sheet1 worksheet. You can now simply copy the details of the first 50 facilities in the newly randomly sorted list and copy them into the Sheet2 worksheet. This is now your simple random sample of health facilities. If this was a real study you could then use the contact information to recruit and plan your data collection.",
    "crumbs": [
      "1. Probability sampling"
    ]
  },
  {
    "objectID": "probability-sampling.html#exercise-2-stratified-random-sampling",
    "href": "probability-sampling.html#exercise-2-stratified-random-sampling",
    "title": "Probability sampling",
    "section": "Exercise 2: Stratified random sampling",
    "text": "Exercise 2: Stratified random sampling\n\nA brief overview of the method\nIn summary, stratified random sampling involves the following three main steps.\n\nDefine your strata. Strata are simply a set of two or more mutually exclusive and comprehensive groups that cover all your sampling frame’s units of observation. This just means that every unit of observation in your sampling frame is a member of one and only one stratum. For example, if we were sampling individuals and had data on their ages we could stratify the them (i.e. the sampling frame) based on age, most simply by splitting all individuals into two age groups, say those aged &lt;18 and those aged 18 years or more. We will see below what to consider when selecting strata.\n\nNote: the singular of strata is stratum, e.g. “we create many strata but sample each stratum separately”. Note: you can define n strata for any single stratification variable, and your total strata will be the product of the number of strata created for each variable. For example, if you stratify based on age, grouped into &lt;18s and ≥18s, and sex, grouped into male or female, you have 2 x 2 = 4 strata in total. As you can see the total number of strata therefore increases rapidly with every extra stratification variable and/or group added! Note also: you cannot create a strata with no units of observation in. For example, if there were no &lt;18 men in your sampling frame you could not create an &lt;18-male strata group as the analysis would not work.\n\nDecide on the sample size for each strata. There are two different versions of stratified random sampling: one that uses “proportionate stratification” and one that uses “disproportionate stratification”.\n\nProportionate stratification means that the sizes of your sample’s strata are proportional to the size of the strata in the target population. For example, using the example above of stratifying by age with two groups of &lt;18 and ≥18: if 25% of the target population were aged &lt;18 (and therefore 75% are aged ≥18) whatever your sample size was 25% of the sample size would come from your &lt;18 stratum and 75% from your ≥18 stratum. This would result in a representative distribution of ages in your sample and preserve the equal probability of selection for all units of observation in your sampling frame.\nDisproportionate stratification is when the size of your sample’s strata is not proportional to their size in the target population. If this is the case then the units of observation in your sampling frame no longer have an equal probability of selection. This means you would have to calculate sampling weights to “map” the same back to the target population and avoid biased results when analysing the data.\n\nTake a simple random sample (or less commonly a systematic random sample) of the relevant size in each strata.\n\nSo we can use the same approach as we took above for the simple random sample and just repeat it within each strata the appropriate number of times.\n\n\nAdvantages and disadvantages\n\n\nRead/hide\n\n\nAdvantages\nThe advantages and uses of stratified random sampling differ somewhat depending on whether you are using a proportionate or disproportionate stratification, and the reason for doing either depends on your goals and skills.\n\nFor studies aiming to describe the characteristics of a target population, where you have no particular interest in any specific sub-populations (compare to scenario 2 below), compared to using simple random sampling stratified random sampling with proportionate stratification can help you to: a) reduce the chances of obtaining an unrepresentative sample, at least in terms of the characteristics represented by your chosen strata, and b) increase the precision of your estimates for a given sample size. While for studies aiming to estimate associations within a target population, where you have no particular interest in any specific sub-populations, compared to using simple random sampling stratified random sampling can similarly help you to increase the precision with which you can estimate your associations of interest for a given sample size, giving you a more “statistically efficient” sample.\n\n\nHow well you achieve these goals depends on how well your chosen strata capture characteristics or variables that account for variation in your outcomes of interest. That is, you want units of observation to be as similar (homogeneous) as possible within strata and, on average, as dissimilar (heterogeneous) as possible between strata. For example, if we are interested in estimating rates of cardiovascular events then stratifying by age makes a lot of sense, because age is one of if not the biggest “causes” of cardiovascular events, i.e. the likelihood of having experienced a cardiovascular event will be quite similar for individuals within a young-age stratum and very different for those individuals compared to individuals in an geriatric-age stratum.\nNote: using proportionate stratification is not actually necessary to achieve these goals, but it results in a sample that does not need reweighting during analysis to avoid unbiased results, and calculating weights is complicated, plus there is typically no good reason to use disproportionate stratification in this case (again see scenario 2 below).\n\n\nIn other situations however you be particularly interested in specific sub-populations. In this case you can use stratification to “oversample” those sub-populations to ensure you have enough sample size to estimate characteristics/associations for those sub-populations (strata) with sufficient precision/power. For example, you may wish to ensure you can estimate certain characteristics or associations within a certain small, ethnic minority group with sufficient precision. With a simple random sample you would, on average, take a sample from the ethnic minority group that was proportional to its population size. For example, if the ethnic minority group were just 1% of the target population and you took a sample of 1000 individuals from the target population then on average you would only sample 10 individuals from the ethnic minority group! Hardly much use. Instead you could create strata for each ethnic group and take a fixed, larger (disproportionate) sample from the relevant ethnic minority group than you would take if you were using simple random sampling. This would be using disproportionate stratification.\n\n\nHowever, as noted earlier if you do this the added complication is that the relative sizes for one or more strata will then, by design, not match their relative sizes in the target population. This means there is not an equal probability of selection for all sampling units, and you would have to calculate and use sampling weights to “map” the sample back onto the target population and avoid obtaining biased results. As this is a more complicated process and this type of stratified sampling is not commonly used (although a form of it is commonly used in multi-stage cluster sampling) we will not look at it further. Note: when using disproportionate stratification you would still be able to gain the advantages mentioned for scenario 1 above if your strata, either those that are disproportionately sampled or indeed other strata, capture important sources of variation within your outcomes of interest.\n\nIn the following exercise we will just look at how to implement the first approach discussed above.\nDisadvantages of stratified random sampling with proportionate stratification\n\nYou require data on the characteristics needed to define your strata for all members of your target population, and it is often not possible or difficult and/or costly to obtain this data.\nCompared to simple random sampling it is a somewhat more complicated and time consuming process (although this is typically a minor limitation).\nWhen analysing data from a stratified random sample you need to use non-standard analytical methods (or non-standard versions of typical analytical methods) to account for and obtain the benefits of your stratified sample, in terms of increased precision. However, this is actually very straight forward to do with modern software and we will see how to do this in the complex survey practical sessions.\n\n\n\n\nScenario\nWe will use the same basic scenario as for the simple random sampling exercise previously, where we are aiming to conduct a survey of n = 50 public primary care health facilities. However, in these exercises we will take a stratified random sample. We will assume that there is likely to be substantial variation in at last some of our outcomes of interest between facilities in different sub-districts. For example, maybe some sub-districts have typically larger populations and so the facilities in those sub-districts experience a greater burden of patients leading to typically poorer outcomes of interest. To increase our chances of getting a representative sample for the target population we can therefore stratify the sampling by sub-district to ensure that no sub-district is over or under represented in the sample relative to its size. As we have the sub-district location of each facility in our sampling frame we can easily stratify our sampling by sub-district.\nIn the scenario/data there are just three sub-districts and they each contain quite different numbers of health facilities. We will assume that we are only interested in computing overall, district-level results for any outcomes. We will therefore need to split our sample size into three (i.e. so we have separate sample sizes for each sub-district) such that each sub-district’s sample size is proportional to the number of facilities in that sub-district. This will ensure that our results apply to the overall district level (if we used an equal sample size for each sub-district then the sub-districts with fewer health facilities in would be over-represented in the overall results, and vice versa).\nNote: in a real study if you had data on additional characteristics that you thought were likely to be related to variation in the outcomes of interest you would probably further increase your chances of getting a representative and more statistically efficient sample by creating additional strata using those data.\n\n\nExercise: taking a proportionate stratified random sample using Excel\n\nLoad the “Health facilities list - stratified random sample.xlsx” Excel spreadsheet.\n\nVideo instructions: taking a proportionate stratified random sample using Excel\nWritten instructions: taking a proportionate stratified random sample using Excel\n\n\nRead/hide\n\n\nIf we know there is substantial variation in health facility characteristics on average between sub-districts we may want to reduce the chances of getting an unrepresentative sample size for one or more sub-districts and increase the precision of our estimates (for a given sample size) compared to taking a simple random sample, which is likely to happen when the sample size is very small (like n = 50). As mentioned previously in reality we would probably want to create strata based on additional variables related to our outcomes, maybe things like some measure of facility size, staffing, or resources etc, but we will just keep things simple here.\nTherefore, instead of risking getting an unrepresentative distribution (number) of health facilities within each sub-district and less precise estimates, as would be likely with a simple random sample, we can take stratified random sample. Here we will see how to take a stratified sample such that the strata sample sizes are proportional to their relative population sizes. This ensures that the sample represents the distribution of strata seen in the population, which means we don’t need to weight our analyses.\nIn the “Health facilities list - stratified random sample.xlsx” Excel spreadsheet look at the Full facility list worksheet if it’s not already in view (you can change worksheet by clicking on the tabs at the bottom left of the window). Now look at the table titled Sub-district frequency table at the upper right of the spreadsheet. First we need to work out the proportion of facilities in each sub-district by dividing the number of facilities in each sub-district by the total number of facilities. Under the Population percentage heading click on the first empty cell for the Hills sub-district. Now enter = and then click on the Population size value for Hills (22), then type /, then click on the Population size total (at the bottom of the column: 245). The resulting formula should be: =G3/G6. Press enter to tell Excel to run the computation. The resulting value should be 0.089…\nRepeat this for the other sub-districts. If you’ve done it correctly the total value under the Population percentage column should equal 1.\nNext we need to use these proportions to compute the sample size required for each strata to maintain the strata sample sizes proportional to the strata population sizes. Under the Strata sample size column click on the first empty cell for the Hills sub-district. Now enter = then type ROUND(50*, then click on the cell containing the Population proportion value for Hills that you previously computed (0.089…), then type , 0). The final full function should be =ROUND(50*H3, 0). Press enter to tell Excel to run the computation. The resulting value should be 5.\nLet’s explain the function you just used =ROUND(50*H3, 0). ROUND works by rounding the first value in the brackets to the number of decimal places listed after the comma. So we’ve told Excel to round our computed sub-district sample size (50 x H3 - where H3 is the proportion of health facilities in the population in that sub-district) to 0 decimal places - i.e. to the nearest integer.\nNow repeat this computation for the other two sub-districts. If you’ve done it correctly the total value under the Strata sample size column should equal 50.\nNext we can take our stratified random sample. As a stratified random sample is just a repeated simple random sample within each strata all you now need to do is take a simple random sample of the relevant size for each sub-district, i.e. a simple random sample of 4 health facilities in the Hills sub-district and so on. In Excel it’s probably easiest to just create separate worksheets for each strata. To save you time, because it’s not something you probably need to practice, I’ve already created these three additional worksheets (the tabs along the bottom of the Excel spreadsheet): one containing the data for each sub-district\nTherefore, create a new worksheet by clicking on the little + symbol to the right of the Hills worksheet. Then repeat the simple random sample process learned in the last exercise for each of the sub-district lists in turn. After randomising each list select the required number of facilities for that sub-district (starting from the top of the randomised list) and copy their details into your sample worksheet. Refer back to that exercise for the steps if needed. Note: you could also achieve a stratified random sample with proportional strata sizes by just randomising the total list of all health facilities and then selecting the first n health facilities from each sub-district as they appear in the randomised order, where n = the required sample size for each strata. However, that would be more time consuming.",
    "crumbs": [
      "1. Probability sampling"
    ]
  },
  {
    "objectID": "probability-sampling.html#exercise-3-systematic-random-sampling",
    "href": "probability-sampling.html#exercise-3-systematic-random-sampling",
    "title": "Probability sampling",
    "section": "Exercise 3: Systematic random sampling",
    "text": "Exercise 3: Systematic random sampling\n\nA brief overview of the method\nIf you can reasonably assume that your sampling frame is randomly ordered with respect to your outcomes of interest and any causal factors that are related to your outcomes of interest then systematic random sampling should produce a similarly unstructured random selection as simple random sampling, and you can use the same “versions” of typical analytical methods that can be used when analysing data from a simple random sample. If your sampling frame is grouped in relation to a categorical characteristic that is related to your outcomes of interest, or is ordered in relation to a continuous/discrete characteristic that is related to your outcomes of interest, then you gain use it to obtain a random but similarly structured sample as obtained via proportionate stratified sampling. To gain the increased precision that can come from analysing a stratified random sample you would have to use relevant stratified analysis methods.\n\nTake your sampling frame and select a random starting point (i.e. random unit of observation).\nBased on your desired sample size calculate a “skip pattern”. This is just a number which then determines how many units of observation are skipped after your starting point before sampling another unit.\nSample your random starting point and based on your skip pattern all successive units of observation that your skipped sampling pattern “lands on” until you reach the end of your sampling frame, by which time you should have sampled your desired sample size (or just under - see the exercise).\n\n\n\nAdvantages and disadvantages\n\n\nRead/hide\n\n\nAdvantages\n\nStratified random sampling can be useful when sampling physical units that are ordered when you have no sampling frame, but you know (at least approximately) how many units there are. For example, if you want to sample n refugee tents in a camp that are ordered in rows but there is no list of all tents, you could use a drone to take an aerial photo and estimate the total number of tents (e.g. count a few rows to get a row average and multiply by the total number of rows). You could then pre-plan a sampling route through the camp and apply the method, taking the first tent as your first sampling frame unit and so on.\nAnother advantage that systematic random sampling has over simple random sampling is that it can reduce the chances that you will obtain a non-representative sample in relation to a characteristic of interest (e.g. an outcome of interest or a characteristic that is known to be causally related to your outcome of interest). For categorical characteristics this has the same result as for proportionate stratified sampling, but if there are many (often small) category levels (i.e. strata), it can be quicker and easier to use systematic random sampling, because all you need to do is order by that characteristic and then take your systematic random sample, and you will get (at least approximately) the proportionate sample size for each category level.\nThis also works for numerical characteristics, where stratified sampling cannot necessarily be applied easily or would force you to crudely group the sampling frame based on cut points. Here, all you need to do is order the sampling frame in relation to the numerical characteristic that is either your outcome of interest, or is assumed to be related to your outcomes of interest, and then take your stratified random sample. You will then ensure that your sample has a representative distribution of that numerical characteristic. For example, if the number of doctors in a health facility is likely to be related to the health facility outcomes we are interested in and we have data on those doctor numbers we can use systematic random sampling to ensure that we get a sample that has a distribution of doctor numbers that match the distribution in the sampling frame/target population.\n\nDisadvantages\n\nSystematic random sampling is a bit more complicated to implement than simple random sampling.\nAs with stratified random sampling you need additional data on important characteristics of your units of observation that are related to the characteristics or associations of interest to obtain a clear benefit from this approach, and such data is often difficult, costly or impossible to obtain.\nMost critically, if there is a repeating pattern to the ordering of your sampling frame in terms of the distribution of characteristics that affect your outcomes of interest then a systematic random sample may produce a seriously biased/unrepresentative sample. This is usually the case when the pattern repeats regularly. For example, if you are using systematic random sampling to sample from a list of health facilities that are ordered from small to large within each area, and your sample size is such that you’re only selecting one or a few per area, the pattern might ensure that you only select typically smaller/larger facilities in each area. Be careful!\n\nWhen to use systematic random sampling?\nClearly systematic random sampling can have some advantages over simple random sampling if used carefully, but what about in relation to stratified random sampling? Remember stratified random sampling can use either simple or systematic random sampling to take the samples within each strata. Therefore, it depends on your aims and there’s no single answer for all circumstances. However, broadly speaking if you have good data on what are likely to be important strata for your characteristics or associations of interest then a stratified random sample may be the better choice This is because compared to simple random sampling stratified random sampling is more likely to produce a representative sample while also being likely to increase the precision of your estimates (i.e. increase your statistical efficiency) compared to systematic random sampling, but it also avoids the risk of producing a biased sample that systematic random sampling can result in when there are unrecognised, typically small-scale, meaningful repeating patterns in the ordering of the sampling frame.\nHowever, for completeness we will practice below how to take a systematic random sample when we have some additional data on important strata.\nLastly, note that one form of systematic random sampling is often used in the first stage of multi-stage cluster samples to take a non-stratified random sample of primary-stage clusters (often villages or city blocks) with probability proportional to the size of primary-stage clusters. This involves a slight modification of the approach we will see below, and as it’s typically only used in this specific circumstance we won’t look at it further.\n\n\n\nScenario\nWe will use the same basic scenario as for the simple random sampling and stratified random sampling exercises previously, where we are aiming to conduct a survey of 50 public primary care health facilities. However, in this exercise we will take a systematic random sample. We will use the stratified random sampling exercise sampling frame where the health facilities were ordered into sub-district groups. As long as there is no smaller scale pattern in the ordering of the list a systematic random sample should ensure a random sample of health facilities while also ensuring the sample is evenly distributed across the three sub-districts.\n\n\nExercise: taking a systematic random sample using Excel\n\nLoad the “Health facility list - systematic random sample.xlsx” Excel spreadsheet.\n\nVideo instructions: taking a systematic random sample using Excel\nWritten instructions: taking a systematic random sample using Excel\n\n\nRead/hide\n\n\nTo save time we have already created a numerical sequence in the first column from 1 to 245. This will be used to identify our sampled health facilities.\nFirst, order the list by the variable “no_drs”: click anywhere on the values and from the top menu click Data then Sort. In the sort tool that appears under Column where it says Sort by click the drop-down menu and select no_drs. Then click OK. The list should now be ordered by the number of doctors per health facility, and our systematic random sample will now ensure we get a representative sample across the range of numbers of doctors per health facility.\nNext, calculate the skip pattern k. This is calculated as N/n, where N = total sampling frame size and n = sample size. If the result is a whole number (integer) use this value, or if not round up to the nearest whole number. Therefore, in cell G1 enter skip and press enter. Then in cell G2 enter =245/50 and press enter. You should get the value 4.9. Therefore, we round this up so our skip pattern k = 5.\nNext we need to select a random starting point between the first unit of observation and that corresponding to our skip pattern k, i.e. a random number between 1 and 5. To do this use the Excel function randbetween. In cell H1 enter the heading facility_selection_id. Then click on cell H2 and enter =randbetween(1, 5) and press enter. This will create a random value between 1 and 5 for the random starting point. Next overwrite this value with the same value by clicking on the cell and typing the number corresponding to the random value (e.g. if the random value is 3 click on the cell and type 3). This ensures that Excel won’t create a new random value each time you update any other functions.\nThen click on cell H3 and type =H2+5. Then press enter. Then click again on cell H3 and then double click on the small solid square at the bottom right of the selection box that appears around cell H2 to extend the formula down to the bottom of the data. This new column then lists the IDs of the facilities that our systematic random sample has selected. Note that for this to work the original facility ID must be a count from 1 to n, where n is the maximum number of facilities. Note also that the facility_selection_id values keep going beyond the maximum value of the original facility ID.\nTherefore, click on the little + symbol to the right of the Sheet1 tab to create a new spreadsheet. Then click back to Sheet1. Now copy and paste the values in the facility_selection_id column from the first value until the greatest value that is ≤245. This is your sample list of facility IDs, which should be both a random sample and implicitly stratified by number of doctors. Therefore, you would have to either go back to the original list and copy the details of each facility, as per the sampled IDs, or use the MATCH function in Excel to match up values, but we do not look at that here.\nFinally, if you go to Sheet2 and highlight all the IDs and look at the bottom right of the window Excel tells us the count (i.e. number of selected rows) of IDs is 49. So we are missing one from our targeted sample size. This is just due to the rounding of the skip pattern. If the original skip pattern (before rounding) had been 5 we it would have resulted in a sample size of 50. Therefore, to get our desired sample size we just need to randomly select one more health facility by going back to the start of the list once you reach the end (e.g. for our skip pattern of 5 if your final selected facility was number 244 then we’d count 245, 1, 2, 3, and then number 4 would be our final selected facility).",
    "crumbs": [
      "1. Probability sampling"
    ]
  },
  {
    "objectID": "probability-sampling.html#single-stage-cluster-sampling-and-multi-stage-cluster-sampling",
    "href": "probability-sampling.html#single-stage-cluster-sampling-and-multi-stage-cluster-sampling",
    "title": "Probability sampling",
    "section": "Single-stage cluster sampling and multi-stage cluster sampling",
    "text": "Single-stage cluster sampling and multi-stage cluster sampling\nThe following optional information is just for awareness and understanding purposes but we will not practice either of these methods.\n\n\nRead/hide\n\n\nCluster random sampling\nCluster sampling involves sampling higher-level units of observation, such as households, schools, villages etc, which contain your lower-level units of observation, typically individuals. If you then also sample units within clusters (i.e. not every unit is selected) then you are using some form of multi-stage cluster sampling method, and that is too complicated for us to look into further, but see below for a bit more detail. However, if you are sampling clusters and then selecting all eligible units within each sampled cluster for data collection you can simply use any of the methods previous covered to sample your clusters. For example, if you wanted to survey community members in a number of communities and you could construct a sampling frame listing all the households in each community you wanted to survey, but not the household members, then you could take a simple random sample of households from your list and then select all eligible individuals within every sampled household for data collection. Or you may wish to stratify the sampling if you also collected data on, say, the total size of each community etc.\nEither way this would result in a representative sample (on average) with no need to re-weight the data, unlike if you had also sampled individuals within households. However, when you take a cluster sample you need to account for the clustering in your analysis. This is because standard methods of analysis assume observations (i.e. data points) are statistically independent from one another, but clearly individuals within the same household etc are not independent, and they therefore do not provide the same amount of statistical information about a population as fully independent observations would. Therefore, ignoring clustering in analyses results in falsely high levels of precision/power. We will see one way that you can account for clustering in the complex survey practical sessions.\n\n\nMulti-stage clustered random sample\nMulti-stage clustered random sampling is far too complicated to go into for this module, but we will see how to analyse data from multi-stage clustered samples in the complex survey practical sessions. It is typically only used by large-scale household surveys, such as the Demographic and Health Surveys by USAID and its in-country partners. Such projects almost always use professional statisticians or survey methodologists. In brief though, this approach is actually some combination of the earlier methods covered above, and usually combines a first stage systematic random sample, with probability proportional to size to select primary sampling units (usually census units, often called “enumeration areas”, that correspond to villages or city blocks), with a second stage of sampling of households, which either again uses systematic random sampling, but typically with equal probabilities of selection, or a simple random sample. However, there are many variations with possibly additional levels of sampling. This means that the probability of selection of the ultimate sampling units is never equal with such methods and complicated sampling weights need to be calculated to ensure analyses produce unbiased results.",
    "crumbs": [
      "1. Probability sampling"
    ]
  },
  {
    "objectID": "sample-size.html",
    "href": "sample-size.html",
    "title": "Sample size",
    "section": "",
    "text": "In this practical we will see how we can estimate sample sizes when our goal is either 1) estimating summary measures of single outcomes, specifically in terms of estimating means (for continuous/possibly discrete outcomes) or proportions (for categorical outcomes), or 2) carrying out null-hypothesis significance testing in relation to measures of association, specifically differences between means (i.e. between-group differences for continuous or possibly discrete outcomes) or proportions (i.e. between-group differences for binary outcomes, which could include a single level of a categorical variable with &gt;2 levels).",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#overview-of-key-concepts",
    "href": "sample-size.html#overview-of-key-concepts",
    "title": "Sample size",
    "section": "Overview of key concepts",
    "text": "Overview of key concepts\n\n\nRead/hide\n\nAlmost every quantitative research study involves one or more research questions (usually the most important ones) that can only be feasibly answered via statistical inference. Recall, statistical inference is the process of using sample data to make inferences (draw conclusions) about unknown characteristics (quantified as population parameters) in a population (or more theoretically some data generating process). Practically speaking, we typically cannot use a census due to financial/logistical constraints, so we sample from our target population (ideally using a probability sampling method), collect data from that sample, and then analyse that data using appropriate methods of statistical analysis to draw conclusions about the population parameters of interest in the target population.\nAs discussed in class, there are three broad types of research question that quantitative research addresses: descriptive (including associations), causal and predictive. The sample size methods we consider here are quite generalisable and are applicable to both descriptive and causal questions, but tend to be used in (arguably) unnecessarily restricted ways depending on the research question/goal(s) of the study. Below we will focus on the descriptive study scenario.\nFor example, what is the prevalence of COVID-19 in a specific population at a specific time point/period? Here, the natural (unknown) population parameter would be the prevalence of COVID-19 in the population at that time point/period. We could then take a sample from the population and estimate the likely value of this population parameter using our study sample data. Our best point estimate would be via the sample statistic known as the sample prevalence (i.e. the proportion/percentage of individuals in the sample with COVID-19), but we would also need to combine this with some measure of uncertainty that can help us infer the likely value of the unknown population parameter, given the point estimate tells us nothing about the amount of sampling error the estimate likely contains. Usually this measure of precision would be a confidence interval.\nHowever, we may also be interested in describing associations in terms of differences between groups. For example, how does the prevalence of COVID-19 differ between men and women? Here, we might seek to measure the association between COVID-19 prevalence and sex in terms of the difference in prevalence between sexes, specifically the difference in the % of men/women who have COVID-19 at the time point/period of interest. Similarly, in a causal study the key question is often about a binary comparison between an intervention/exposure and a control/comparison, which (depending on the outcome of interest) can often be naturally quantified as a difference in means (for continuous outcomes) or percentages (for binary outcomes).\nWe would then again combine this sample statistic (i.e. the difference in means/percentages) with some measure of how uncertainty about where the unknown population parameter lies. Again, usually this would be a confidence interval. However, as we have discussed, although our estimate and confidence interval provides the best understanding of the likely direction, size and precision of any such association in the population, null-significance hypothesis testing, in the form of p-values, are very often used to make inferences about the existence of such associations (as opposed to them being due to sampling error). We might therefore use an appropriate hypothesis test to test how unlikely it would be to observe a difference at least as great as the one observed if we assume that there is actually no difference in the population. Then, if the corresponding p-value were less than 0.05 we may reject the null hypothesis and accept the alternative hypothesis that the data are more consistent with there being a difference in the outcome between the two groups in the population.\nSo where does sample size come into this? When we seek to make statistical inferences, like in the examples above, the sample size can be loosely thought of as simply the number of units of analysis (i.e. outcome values, e.g. individuals, health facilities, repeated measures of individuals over time etc) that we need to collect to give ourselves a “reasonable chance” of answering our research question satisfactorily.\nIn the sample size calculation we define what we mean by a “reasonable chance” and “satisfactorily”, and the reason we cannot guarantee that we will answer our research question via statistical inference is because we can only make probabilistic conclusions using such methods. This is because we are basing our inferences on samples, which may or may not be perfectly representative of our target population due to sampling error. It is also critical to remember that our standard inferential methods of confidence intervals and p-values do not account (as standard) for any other sources of bias (such as information bias, selection bias, confounding bias). So at the sample size calculation stage, we are effectively ignoring any other biases and just thinking about how we can account for sampling error.\nMore formally, in a sample size calculation you make a series of assumptions, such as what level of variation in an outcome we expect to get in our sample, and then a sample size calculation can tell you how large a sample size you need to get results with sufficient level of precision or power (we’ll come back to these concepts shortly) - assuming no other bias. However, the validity or accuracy of a sample size calculation depends entirely on the validity/accuracy of the assumptions (and any other biases), as we’ll discuss. And whether the results can be validly generalised to the target population depends on the representativeness of the sample and the lack of any biases affecting this, which the sample size calculation itself can do nothing about. As always, the important thing is to think carefully and critically when planning your sample size, and not just mindlessly plug in some optimistic values.\nThere are two main approaches typically used for calculating sample sizes for quantitative studies which can be thought of as:\n\nThe confidence interval (or precision) based approach.\nThe hypothesis testing based approach.\n\nIn practice they look quite different, and they do work quite differently in practice. However, they are actually very closely related, particularly in the underlying maths (not that we look into that).\nIn this practical we will make use of the sample size calculation tools in SPSS.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#confidence-interval-based-approach",
    "href": "sample-size.html#confidence-interval-based-approach",
    "title": "Sample size",
    "section": "Confidence interval based approach",
    "text": "Confidence interval based approach\n\nOverview\n\n\nRead/hide\n\nIn summary, this approach involves calculating the sample size we need to estimate our summary statistic of interest with a given level of precision, by which we mean the width or range of the confidence intervals around our sample statistic of interest that we want to get. Specifically, we set a level of precision that means the sample size will ensure the confidence intervals we get are no wider than a pre-specified range, assuming all the assumptions that go into the calculation are exactly true. In practice, this is typically only used to calculate sample sizes when the goal is estimating summary measures of single variables, such as means, proportions or rates. Here we will just focus on means and proportions.\nHowever, this approach can certainly be used when the target population parameter and summary statistic are a measure of association, such as a difference in means, but the second of the two approaches we cover below is overwhelmingly used when the sample size is focused on a measure of association. See below for more discussion on this point. See below for more discussion on this point.\n\n\n\nThe confidence interval approach and associations/causal effects\n\n\nRead/hide\n\nWhy is the confidence interval approach rarely used for analytical studies looking at associations/causal effects?\nThe lack of use of the confidence interval approach in these situations is probably because of the dominance of the null-hypothesis significance testing approach to statistical inference for measures of association/causal inference, whereas with descriptive studies the main aim is often estimating a range of characteristics (i.e. summarising individual variables) to a given level of precision rather than testing hypotheses about differences/associations (although we often also want to look at associations [typically differences] in those summary measures between key groups. And as we will see below, when using a null-hypothesis significance testing approach the more natural/logical sample size calculation approach is arguably the hypothesis testing approach to sample size calculations. However, I would argue that given the benefits of using confidence intervals for making statistical inferences rather than null-hypothesis significance testing tests, we should make use of the confidence interval sample size approach as the norm, but we don’t look at that here.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#hypothesis-testing-based-approach",
    "href": "sample-size.html#hypothesis-testing-based-approach",
    "title": "Sample size",
    "section": "Hypothesis testing based approach",
    "text": "Hypothesis testing based approach\n\nOverview\n\n\nRead/hide\n\nThe hypothesis testing approach is primarily used for descriptive/causal studies that are trying to understand whether a given association/causal effect exists. It is most often (but certainly not always) the case that the association of interest will be studied and analysed in terms of a difference in a continuous or binary outcome between two groups, which are usually independent groups but they can be related. However, the association may also be analysed in terms of a slope/trend, such as a regression coefficient or other similar measure, but these are much less commonly seen. Also, as we explain below, in theory this approach can also be used for descriptive studies, but this is probably never rarely done in practice. As these other approaches/scenarios are very rarely/never used we won’t look at them further and we below will just assume we are considering the approach where our association of interest is analysed in terms of a difference in a continuous outcome (summarised as means) or binary outcome (summarised as percentages) between two independent groups.\nThe hypothesis testing approach is arguably harder to understand well than the confidence interval approach. It may be easiest to understand once you understand the process. Therefore, we will explain it in summary here by detailing the key steps you go through.\nFirst, we pre-specify a target difference that represents the smallest difference in our outcome between the two groups that we want to be able to detect. This target difference also explicitly or implicitly incorporates our assumption about what level of variation will exist in the outcome variable in the sample data (depending on whether it’s a continuous or binary outcome - see later). We also pre-specify the threshold below which we would declare a p-value from a null-hypothesis significance test of the null hypothesis that there is no difference in the target population “statistically significant”. That is, the threshold below which we would reject the null hypothesis and assume that the alternative hypothesis of a difference existing in the target population was more likely. This is usually the conventional P≤0.05 level.\nThen the last main assumption is maybe harder to understand. Here we pre-specify the probability that we will obtain a p-value from a null-hypothesis significance test of the the null hypothesis that is statistically significant based on our chosen threshold for significance (i.e. equal to or below our chosen threshold). Technically speaking, this probability is the proportion/percentage of the time we would obtain a p-value from a null-hypothesis significance test of the the null hypothesis that is statistically significant, based on our chosen threshold for significance, if we were to repeat the study an infinite number of times. The reason we can’t guarantee we will get a statistically significant p-value even if all our other assumptions are correct is because of sampling error. Even if the true difference in the outcome between the two groups that exists in the target population is equal to (or larger) than our pre-specified target difference, in any given randomly selected sample we might, due to sampling error, select a sample where the difference in the sample doesn’t reflect the difference in the target population and is in fact smaller. However, we can at least ensure that this only happens a given proportion of the time, if all our other assumptions are valid.\nThen conditional on our assumption about the target difference in the sample data being exactly true, and conditional our assumption about the level of variation in the outcome in our sample data being exactly true, and conditional on their being no bias/error in the study and analysis other than sampling error, the resulting calculation will tell us what sample size we need to obtain to ensure that we have the pre-specified probability that we set of obtaining a statistically significant p-value when testing the null hypothesis. That is a lot of complicated ifs, so it can take some time and repeated exposure to the concepts and logic before it might make more sense, so don’t worry if it’s not very clear initially. It’s another advantage of the, arguably conceptually simpler, confidence interval based approach.\nUnfortunately, there is also quite a lot of technical terminology associated with this approach that we haven’t used yet. However, we must familiarise ourselves with this terminology. First, the threshold at which we declare an effect statistically significant and reject the null hypothesis is known as the “alpha level” or the “level of statistical significance”, and this is the probability (in the long-run) that we would falsely reject the null hypothesis if true. Then the probability that we will be able to detect our target difference if it exists via getting a p-value less than our level of significance is known as the “power” of the hypothesis test. The power is also equivalent to 1 - “beta” (i.e. “1 minus beta”), where beta is the probability (in the long-run) of getting a false negative result, i.e. the probability of not rejecting the null hypothesis when it is actually false. Therefore, power is the probability of correctly rejecting the null hypothesis when it is false in the long-run assuming all assumptions that go into the sample size calculation are true.\nYou may be wondering why we need to pre-specify values for the alpha level and the power. Why not just set both to their maximum so that give ourselves the best chance of obtaining a statistically significant result? The short answer is there is a trade-off with the resulting required sample size (see below for more details). By convention, researchers typically set alpha to 0.05 (or less commonly 0.01) and power to 0.8 (or less commonly 0.9). See below for more details.\nWhy can’t we always minimise our alpha level and maximise our power?\nMore specifically, alpha is also the false positive rate of a hypothesis test, or the rate at which we will falsely declare a difference statistically significant in the long-run when conducting null hypothesis significance tests in a given scenario. Therefore, we want this to be as small as possible to avoid making mistakes, i.e. falsely declaring there to be a statistically significant difference, which we interpret to mean there is likely to really be a difference in the target population. However, the smaller we set the alpha level the larger the sample size required to detect any given difference as statistically significant for a given power (i.e. the larger the sample size required to detect any given difference as statistically significant with a given probability).\nSimilarly, the higher we set the power level the larger the sample size required to detect any given difference as statistically significant for a given alpha level or level of statistical significance. This trade-off is simply a function of the maths underlying hypothesis testing approach sample size calculations: we need more statistical information and therefore a larger sample size to both reduce how often we make false positive decisions and increase how often we make true positive decisions from null-hypothesis significance testing. Similarly, for a given alpha and power level we require more statistical information or a larger sample size to detect as statistically significant smaller and smaller target differences (assuming they exist).\nTherefore, for any given target difference we clearly want to minimise our level of statistical significance to reduce our false positive rate while maximising our power level to increase our chances of detecting statistically significant differences (assuming they exist!), while not requiring an unfeasibly large sample size. Consequently, typical values of alpha used in almost all sample size calculations are either 0.05 (the common statistical significance threshold) or less commonly 0.01 (i.e. 5% or 1%), while typical values for the power are 0.8 or less commonly 0.9 (i.e. 80% or 90% power). Very broadly speaking, these values typically allow you to come up with a feasible sample size calculation for not “unreasonably” small target differences. However, these are not magic values and they are very much arbitrary conventions with no logical or natural basis for them other than the fact that people like round numbers, and a 5% false positive rate (0.05) and an 80% chance of detecting a difference as statistically significant if it exists seemed like “reasonable” values to researchers who have gone before us, given how much these two values/assumptions “cost” in terms of the required sample size.\n\n\n\nWhat about descriptive studies?\n\n\nRead/hide\n\nThis hypothesis testing based approach can also be used for estimating sample sizes related to sample statistics that measure characteristics, e.g. means and proportions as estimated in a cross-sectional survey, if you wanted to test hypotheses about whether those characteristics differ from a given null hypothesis value (i.e. an assumed population value). However, this approach is almost only ever used to calculate sample sizes required for primarily analytical studies where the main research questions are about associations/associations, and then this is usually typically further restricted/framed just in terms of differences between two groups, e.g. differences between two means or two proportions, where the intention is to use null-hypothesis significance testing to see whether the observed sample difference differs significantly from the assumed null hypothesis difference (usually of no difference).",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#exercise-1-sample-size-required-to-estimate-a-mean-to-a-given-level-of-precision",
    "href": "sample-size.html#exercise-1-sample-size-required-to-estimate-a-mean-to-a-given-level-of-precision",
    "title": "Sample size",
    "section": "Exercise 1: Sample size required to estimate a mean to a given level of precision",
    "text": "Exercise 1: Sample size required to estimate a mean to a given level of precision\n\nScenario\nYou work for a regional ministry of health in a region where public health facilities have reported increasing numbers of patients seeking treatment for cardiovascular diseases over the last decade. However, there is no good data on the cardiovascular health of the population. Therefore, your department has tasked you with conducting a population survey on the cardiovascular health of the population. As it is hard to measure actual cardiovascular health you will focus on systolic blood pressure as a key proxy indicator or risk factor for cardiovascular health. The primary aim of the survey is therefore to estimate the distribution of blood pressure, specifically systolic blood pressure (mmHg), values within the target population, along with collecting other relevant health and socio-demographic data. As the primary or key outcome variable is systolic blood pressure and your aim is to estimate the distribution of this outcome in the target population a confidence interval based approach to the required sample size makes most sense. This is because this approach will allow you to plan a sample size that will enable you to estimate the distribution of the outcome to a certain level of precision (i.e. to estimate it with a certain maximum confidence interval width).\n\n\nSample size assumption inputs\nWe will estimate the sample size required to estimate the mean so that the confidence intervals around the mean are of a maximum desired width. To make the calculation there are just three assumptions/inputs to consider.\n1. What confidence level do you want for your confidence interval?\nThe convention is a 95% confidence interval, and unless you have a good reason to choose otherwise use this.\n\nTherefore, we will use 95%.\n\n2. What standard deviation (SD) is your outcome variable expected to have in your sample data?\nThis may be estimated from values in the literature from similar studies, or from pilot data (although this is risky as pilot studies by definition are small and cannot produce unreliable estimates of any sample statistics), or you can use the following rough rule of thumb:\n\nTake the range of values for your outcome that roughly 95% of the population are likely to fall between/within. Divide this by 4 for a conservative estimate of the population SD for the outcome.\n\nFor our example, in our scenario we might assume, from clinical knowledge/existing literature, that about 95% of people in our target population have systolic blood pressure values between 80 and 150 mmHg. Therefore, the expected range is 150 - 80 = 70. We then divide this by 4: 70/4 = 17.5. You should then round this up for safety to at least 18, although for greater safety you might round up further (say to 20). The larger the assumed SD the larger the sample size required, but the safer you will be because you have less chance of finding out that the SD in your sample is actually higher than you assumed, which could mean you won’t then achieve your desired precision level. Note: this rule of thumb only applies to variables that are, at least approximately, normally distributed. See the following paper for an improved but slightly more complicated approach to estimate SDs: https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-135\n\nTherefore, we will use 18 mmHg.\n\n3. What is the minimum level of precision that you want your confidence intervals to have?\nThis should be based on practical considerations, such as what level of precision will be useful for users of the results of the study such as clinicians, health administrators, and policy makers etc. Here we will assume that our result would only be judged to robust and useful by clinicians if our confidence intervals are a maximum of +/- 2.5 mmHg. This means that whatever mean we estimate we want the confidence intervals to be no wider than that mean plus 2.5 mmHg and that mean minus 2.5 mmHg. Note: the desired confidence interval precision does not depend on the likely/assumed value of the outcome. Also note: we have defined the precision of our desired confidence interval in terms of the “half-width” of the confidence interval, which is just the upper confidence interval value minus the lower confidence interval value (i.e. the confidence interval range) divided by 2. You will also often see this half-width referred to as the “margin of error”: https://en.wikipedia.org/wiki/Margin_of_error\n\nTherefore, we will set our confidence interval half-width to 2.5 mmHg.\n\n4. What response rate do you expect?\nIt is very rare to achieve a 100% response rate, so typically you should use previous values in the literature, if they exist, along with any pilot data, and past experience, to decide on a likely response rate. This should be a conservative/safe assumption, i.e. round down a “sufficient” extent, because experience shows researchers typically overestimate the response rate. SPSS does not allow you to automatically adjust the sample size for the response rate so we have to do this manually after we get our estimated sample size (assuming 100% response rate), but we can do this very easily as follows:\n\nSample size adjusted for &lt;100% response rate = desired sample size (i.e. the result from the calculation, which assumes a 100% response rate) / assumed response rate as a proportion.\n\nFor example, if we did our sample size calculation and we got a required sample size of 92, then if we assume we will likely only get a response rate of 90% or higher (0.9 on the proportion scale, i.e. 90/100), then we actually need to aim to collect a sample size of 92/0.9 = 103 (rounding up). So we need to approach 103 people to end up with at least 92 who participate (assuming our response rate assumption is accurate).\n\nNote: while this will ensure that you have at least 92 individuals and that you achieve your desired level of precision, if peoples’ participation is related to their values of the outcome of interest then recruiting more people won’t stop your results from suffering response bias. Increasing the sample size will increase the precision of the estimate but it can never reduce any such bias! So you can certainly have a very precisely estimated (narrow confidence intervals) but very biased result.\n\n\nTherefore, we will assume a response rate of 0.9.\n\n\n\nCalculate the required sample size\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size required to estimate a population mean with a given level of precision when using the t-distribution\n\n\nRead/hide\n\n\nFrom the main menu go: Analyse &gt; Power Analysis &gt; Means &gt; One Sample T Test. Note that this sample size tool is aimed mainly at those wishing to test whether a population mean differs from an assumed value (the null hypothesis). So while we can use it to calculate the sample size necessary to estimate a population mean to a given level of precision it’s a little awkward as we have to add in some assumptions that we are actually not interest in, and in other software for this type of sample size calculation we aren’t forced to make.\nIn the Power Analysis: One-Sample Mean tool window that appears click on the blue-rimmed Precision box at the top right. Enter our desired minimum half-width for our confidence interval (2.5 mmHg) into the Specify the half-width of confidence interval box. Once entered the Add button should appear turn from grey to blue and you should then click it to add the half-width into the box. Then click Continue.\nThen back in the main window add our assumed population standard deviation for the outcome (18 mmHg): look for the Population standard deviation box about half way down and enter the value 18.\nThat’s all we need to specify for our confidence interval based sample size calculation. However, as I said above though the tool is a bit awkward for our purposes because it forces us to also enter assumptions required for also estimating the sample size necessary for a hypothesis test, where we would be testing whether our sample mean differs from some assumed population mean. Therefore, we need to fill out the assumptions for a hypothesis test as well, but be clear: these will only affect the results of the sample size calculation relating to a corresponding hypothesis test (which we are not planning to make), so we are only adding these assumptions because the tool won’t let us only calculate our confidence interval based sample size alone! So please don’t worry about these values or assumptions here. We just have to enter something to get the results we are interested in!\nTherefore, at the top in the Single power value box add any value &gt;0 and &lt;1 (it won’t accept values outside this range), e.g. 0.5. Then in the Population mean box below add any value as long as it differs from the value in the Null value box below it, which by default is 0, so e.g. add 1.\nNow, finally, we can click OK. In the results that appear ignore the first Power Analysis Table as this relates to the sample size for a hypothesis test, which we are not interested in and just adding random assumptions for. Our results are in the Sample Size Based on Confidence Interval table. We can see our required sample size in the first column headed “N”: 202. It also gives the assumed half-width (i.e. what we wanted) and the actual half-width that the sample gives us, which will be the closest value to our desired half-width that we can get given a whole number sample size.\nFinally, we need to adjust our sample size for our assumed response rate of 90%. Remember we just need to divide our sample size assuming a 100% response rate by our assumed response rate on the proportion scale:\n\n\n202 / 0.9 = 244.4.\n\n\nSo, rounding up, our final required sample size is 245.\n\n\nTherefore, assuming the expected population SD is 18, and employing the t-distribution to calculate our confidence intervals, and assuming a response rate of 90%, the study would require a sample size of 245 to estimate the population mean systolic blood pressure (mmHg) with 95% confidence intervals of width ± 2.5.\n\nRemember though, if you achieve the required sample size you will only achieve your desired level of precision if the other assumptions in the calculation are accurate, i.e. if the SD of the outcome in the sample equals the assumed SD. If the actual SD in the sample data is larger than the pre-specified expected value then the precision you achieve, i.e. the half-width of the confidence intervals around your estimated mean, will be larger than your desired level of precision. Note: the opposite is also true, i.e. you’ll get better precision than expected if the SD turns out to be smaller than expected. Also, irrespective of the level of precision, the estimated population mean will only be unbiased on average if there is no systematic difference in outcome values between non-responders and responders, and if there are no other sources of bias impacting the data. Similarly, if our sample is not taken via a probability sampling method then formally we cannot be sure that the interpretation of our result applies to the population.\n\n\n\nAdditional considerations\n\n\nRead/hide\n\nWhat if you are estimating multiple means in a quantitative survey? For example, for our scenario where we are conducting a survey on cardiovascular health we might also measure salt intake, cigarettes smoked per day, BMI etc. Then you probably have two main approaches depending on the situation. First, if there is a clear, primary research question/objective then you can base the sample size of the outcome that allows you to answer that research question/acheive that objective. For example, if the primary aim of our survey was to estimate the distribution of systolic blood pressure in our target population then we could base the sample size on this outcome alone, and essentially hope that this is also a sufficient sample size to estimate our other numerical outcomes with sufficient precision.\nSecond, if there is no clear, primary research question/objective then decide which outcomes you need to achieve a given level of precision for when estimating their distribution, and simply choose the sample size that is largest. That way you ensure that you will achieve at least sufficient precision for all your outcomes. For example, if we found we needed a sample size of 100 to estimate our systolic blood pressure outcome with sufficient precision and a sample size of 150 to estimate our salt intake outcome with sufficient precision, and these were our two key outcomes, then we would aim for a sample size of 150.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#exercise-2-sample-size-required-to-estimate-a-proportion-to-a-given-level-of-precision",
    "href": "sample-size.html#exercise-2-sample-size-required-to-estimate-a-proportion-to-a-given-level-of-precision",
    "title": "Sample size",
    "section": "Exercise 2: Sample size required to estimate a proportion to a given level of precision",
    "text": "Exercise 2: Sample size required to estimate a proportion to a given level of precision\n\nScenario\nYou work for the regional ministry of health and the ministry leaders wish to know how frequently the public primary care facilities in the region run out of one or more drugs on the essential medicines list (known as a drug stock-out). They therefore plan to conduct a cross-sectional survey of public primary care facilities in the region and record whether each facility ran out of one or more drugs on the essential medicines list within the last month of the survey or not. Therefore, the outcome is binary (yes/no) and is most naturally summarised as a proportion/percentage. The ministry of health want a clear answer on this issue so they want to be fairly certain of the proportion of public primary care facilities across the whole region that have experienced such a drug “stock-out” within the last month. After some discussion it is agreed that you will try to give them an answer to this percentage within ± 5 percentage points.\n\n\nSample size assumption inputs\nWe will calculate the sample size required to estimate the proportion of stock-outs so that the confidence intervals for the proportion are of a maximum desired width. To make the calculation there are also three assumptions/inputs to consider.\n1. What confidence level do you want for your confidence interval?\nThe convention is a 95% confidence interval, and unless you have a good reason to choose otherwise use this.\n\nTherefore, we will use 95%.\n\n2. What is your expected prevalence/proportion/percentage?\nRemember, these are all equivalent measures but on potentially different scales, and you can covert between proportions and prevalences/percentages (prevalences are usually given as percentages, but can be given as proportions) by multiply/dividing by 100. We’ll just refer to the percentage from here on as that’s the scale that most people prefer to use.\nNote: unlike for a mean and for technical reasons related to the assumed probability distribution that a binary variable follows we do not specify an expected level of variation in the outcome like a SD, because for a binary variable the variation is assumed to be related to the mean, i.e. to the underlying probability or proportion. Therefore, we just need to specify the assumed proportion. As with the SD, you may either base your expected proportion on prior estimates from the literature, and/or from pilot studies, or if there is no information to go on then the safest (most conservative) option is to use an expected proportion of 0.5. This is because for any given sample size the confidence intervals you obtain when estimating a proportion will be widest around a proportion of 0.5, so if you assume a proportion of 0.5 then whatever level of precision you pre-specify you will guarantee that you will achieve that level of precision (conditional on obtaining the required sample size) or greater (if the proportion turns out to be &lt;0.5 or &gt;0.5. Again, technically this is because the probability distribution of a binary variable assumes that the variance is greatest when the underlying probability (i.e. proportion) is 0.5, and declines proportionally for values &lt;0.5 or &gt;0.5.\nHowever, an assumption of a proportion of 0.5 can be very conservative and result in a much larger sample size than might be necessary even if you are “playing it safe”. Therefore, it’s usually best to play around with this assumption based on your best estimate/guess and see how conservative you can go while still requiring a feasible sample size. For example, drug stock-outs are thought to be fairly rare and unlikely to occur in the previous month in more than 5 percent of facilities on average. Therefore, it doesn’t make sense to assume a proportion 0.5 as it’s very unlikely that you’d be this wrong, but to be conservative it is agreed you will assume a higher percentage of 10% to be the true frequency.\n\nTherefore, we’ll assume an outcome proportion of 0.1 (10%).\n\n3. What is the minimum level of precision that you want your confidence intervals to have?\nThe same considerations apply when choosing your desired half-width as for the case when estimating a mean, i.e. consider the practical implications and requirements of your results.\n\nTherefore, as discussed above we’ll assume we want a confidence interval that goes from 0.05 to 0.15, so a width of 0.1 or a half-width of 0.05 on the proportion scale.\n\n4. What is your expected response rate?\nAs discussed for the estimating a mean situation if you expect &lt;100% response rate you should adjust the sample size for the assumed response rate.\n\nTherefore, again we’ll assume a response rate of 90% (0.9).\n\n\n\nCalculate the required sample size\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size to estimate a single proportion with a given level of precision\n\n\nRead/hide\n\n\nFrom the main menu go: Analyze &gt; Power Analysis &gt; Proportions &gt; One Sample Binomial Test.\nClick the blue-rimmed Precision box at the top right. We now have a diverse set of different approaches we can select. These relate to the different approaches to calculating confidence intervals we can take for proportions. As you can see people have developed lots of different methods. When computing confidence intervals for proportions we can assume a normal distribution. This would be the “Wald” method. When the proportion is not too close to 0 or 1, e.g. &gt;0.1 and &lt;0.9, so called normal-approximation confidence intervals for proportions will be fairly accurate. However, if the proportion is close to 0 or 1, e.g. &lt;0.1 or &gt;0.9, then this, particularly if the sample size is small, e.g. &lt;30, then this approach can lead to biased confidence intervals that do not include the true population value at the specified rate (e.g. 95% of the time). Hence why statisticians have developed many different approaches that try to give more accurate confidence intervals. There’s not a great deal of guidance in the literature about which approach/approaches are best, but some evidence suggests the “Wilson Score” method often does well, so let’s just go with that one and not worry.\nTherefore, click the Wilson Score tick box. Then in the Specify the half-width of confidence interval box add our desired half-width of 0.05. The Add button should then turn blue and you should then click it to add our desired half-width to the box. Then click Continue.\nWe then need to specify the assumed proportion we will be estimating. As discussed above, we assume this will be no greater than 0.1 (10%), so we will assume this upper value to be safe. Therefore, in the Population proportion: box add 0.1.\nAgain, that’s all we need to do for our confidence interval based sample size, but unfortunately SPSS forces us to also specify assumptions for if we were calculating the sample size necessary when testing whether a proportion differed from some null hypothesis value. Therefore, in the Single power value: box, which doesn’t apply to our confidence interval based sample size, add any number &gt;0 or &lt;1 (again, it won’t accept values outside this range), e.g. 0.5.\nThen, finally, you can click OK.\nIn the results that appear again ignore the Power Analysis Table and look at the Sample Size Based on Confidence Interval table. In the first column under heading “N” we can see that our estimated required sample size is 141, and we can see the desired half-width we aimed for and the closest the calculation could get for a whole numbered sample size.\nFinally, let’s adjust for our assumed response rate:\n\n\n141 / 0.9 = 156.6.\n\n\nSo, rounding up, our final required sample size is 157.\n\n\nTherefore, assuming that the sample and population percentage of facilities experiencing a drug stock-out within the last month is 10% or lower, and assuming a response rate of 90% in the survey, then the study would require a sample size of 157 to estimate the percentage of stock-outs such that the 95% confidence intervals of the estimate were at most ±5 percentage points (i.e. go from at most 5% to 15%).\n\nNote that this assumes you are estimating your population proportion based on 95% confidence intervals calculated via the “Wilson Score” method we selected. You can see how to estimate a proportion using this method (or any of the methods listed in the sample size tool) in SPSS in the “Population description” section of the website.\nAlso remember that achieving the level of precision implied by the sample size calculation depends on whether the estimated proportion in the sample data is equal to the expected proportion and on any other assumptions such as the true response rate being equal to your assumption. If the estimated proportion is further from 0.5 than your expected proportion (e.g. 0.07) then you will have better precision than your pre-specified level of precision, assuming your response rate is equal or better than the assumed rate, and similarly if your response rate is better than your assumed rate you will have better precision than your pre-specified level of precision, assuming the estimated proportion is equal to or further from 0.5 than your expected proportion. And vice versa, i.e. your precision will be worse than the pre-specified value if the estimated proportion is closer to 0.5 than the expected proportion and/or if the response rate is worse than the assumed rate.\nThen as with the sample size for a mean, the same considerations apply around biases. Increasing the sample size will always increase the precision of your estimates, but it will never affect the influence of any study bias!\n\n\n\nAdditional considerations\n\n\nRead/hide\n\nSee the “Additional considerations” section in the “Estimating a mean” section above for a discussion of some useful guidance when facing common additional considerations.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#exercise-3-sample-size-required-to-test-whether-two-independent-means-are-different",
    "href": "sample-size.html#exercise-3-sample-size-required-to-test-whether-two-independent-means-are-different",
    "title": "Sample size",
    "section": "Exercise 3: Sample size required to test whether two independent means are different",
    "text": "Exercise 3: Sample size required to test whether two independent means are different\n\nScenario\nYou work for a research NGO in a country where public health facilities have reported increasing numbers of patients with cardiovascular diseases over the last decade. You have received funding to carry out a descriptive study using a cross-sectional survey design with a focus on exploring how systolic blood pressure (mmHg), as a proxy for cardiovascular disease, varies on average between different key socio-economic groups, particularly smokers and non-smokers, which is the key comparison you will base the sample size on. Analytically, we would estimate the difference using either an independent t-test or a linear regression of the outcome with a binary/categorical variable for the groups being compared (assuming the standard errors/confidence intervals/p-values for the coefficients are based on the t-distribution).\nThe goal of the study is to inform policy for public health facilities, in terms of which types of patients to focus most resources on. A difference of 5 mmHg or greater in systolic blood pressure is considered to be clinically important for patient health outcomes.\n\n\nSample size assumption inputs\nThere are more assumption inputs to consider when taking a hypothesis testing approach to sample size calculations, but several of the inputs are typically fixed at conventional levels - although this should never mean that you just mindlessly select those levels, there’s always room for thought!\n1. What alpha (α) level or level of significance do you want?\nThe lower this is set the less likely the resulting hypothesis test is to generate a type I error or a false positive result on average (or in the long-run), assuming you achieve the required sample size and that all other sample size assumption inputs are accurate. However, the lower this is set the larger the sample size required holding all other assumption inputs constant, so there’s a trade-off with the sample size. By convention the level of significance is usually set to be 0.05 or 0.01, i.e. the levels at which we commonly determine statistical significance: when P≤0.05 or less commonly P≤0.01. Again this is probably because it’s a nice round number and assumed to be reasonable trade-off. Unless you have a good reason to change this and know what you are doing then leave this as the default 0.05.\n\nTherefore, we’ll leave the level of significance as its default at 0.05.\n\n2. What power (1-β) do you want?\nThe higher this is set the more likely it is that the resulting hypothesis test will correctly reject a false null hypothesis, if the hypothesis is indeed false and the mean difference in the sample data is at least as large as the one assumed (see below), on average (or in the long-run), and assuming you achieve the required sample size and that all other sample size assumption inputs are accurate. However, again there is a trade-off: the larger this is set the larger the sample size required. By convention this is set at either 0.8 or less commonly 0.9. Again this is probably because it’s a nice round number and assumed to be reasonable trade-off. There’s absolutely no reason not to aim for a higher level of power though if you can afford the resulting sample size, and you can play around with this input and see how it affects your sample size.\n\nWe’ll leave the power as its default at 0.8.\n\n3. What is the expected difference in your outcome between the two independent groups (i.e. the expected difference in the two group means) and the expected variation (measured as the standard deviation) in the outcome?\nThis is where we pre-specify the target difference that we want to be able to detect, if it exists, which is a difference of 5 mmHg in our scenario.\nThe expected standard deviation is the expected pooled standard deviation, i.e. assuming the outcome has the same standard deviation in both groups. As with the confidence interval approach to estimating a mean we can select this based on values in the literature, pilot data, and/or using the rule of thumb previously discussed. If you expect the standard deviation to be different in each group then just use the bigger of the two standard deviations as the pooled standard deviation will always be smaller than this, i.e. it’s a conservative/safe approach. Let’s assume our expected shared or pooled standard deviation is 10 mmHg, based on a conservative rounding up of values from pilot work and related literature.\n\nTherefore, we’ll use the expected difference between means approach and enter the expected difference as 5 (mmHg) and the expected shared or pooled standard deviation as 10 (mmHg), to ensure we obtain a sample size based on our target difference. Note that the direction of the expected difference doesn’t matter. We’d get the same required sample size for a difference of +5 or -5.\n\nIf the true difference in the target population is greater than this assumption then we’ll have more power on average than we expect.\n4. What is the ratio of group sizes that you expect?\nFor any given total sample size, if the sample size in each of the two groups differs then you will not have the expected power that you pre-specified, and your power reduces as the ratio of group sizes gets further from 1 (i.e. the group sizes become increasingly different). Therefore, if you expect that you will not have equal group sizes you can account for this by specifying the expected group size ratio. We will just assume for our exercises below that we will have equal group sizes for simplicity, but be aware of this important consideration if you were carrying out a sample size calculation for a real study. As usual, make sure your assumption is conservative.\n\nTherefore, we’ll assumed an expected group size ratio of 1, i.e. an equal ratio.\n\n5. What is the expected response rate?\nAs with the estimating a mean tool the independent group means tool doesn’t have an option to automatically adjust for the expected response rate and so we’ll have to do it manually.\n\nWe’ll assume a response rate of 0.9 (90%).\n\n\n\nCalculate the sample size\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size required when comparing two independent means via the independent t-test\n\n\nRead/hide\n\n\nFrom the main menu go: Analyze &gt; Power Analysis &gt; Means &gt; Independent-samples T Test.\nIn the Single power value: box enter our desired power: 0.8 (the most common convention).\nIn the Group size ratio: enter our assumed maximum ratio for the group sizes: 1.\nIn the Population mean difference enter the minimum difference we want to be able to detect via our hypothesis test: 5 (mmHg). Again, note that the sign of the difference is irrelevant for this type of sample size calculation, so we can just think in terms of the absolute value of the difference and enter 5 and get the same results.\nIn the Population standard deviations are: area we can specify either an assumed pooled standard deviaiton for each group or specify separate assumptions for each group. As above we’re assuming a common/pooled standard deviation so leave the Equal for two groups button selected and in the Pooled standard deviation box enter our assumption for the pooled standard deviation: 10 (mmHg).\nIn the Test Direction area we will leave the Nondirectional (two-sided) analysis button checked, because we don’t only want to test whether there is a difference between the groups in one direction. We want to test whether there is a difference in either direction.\nFinally, we will also leave the Significance level: box as it is, specifying the level of significance (or alpha) as 0.05 (the most common convention).\nNow click OK.\nIn the Power Analysis Table that appears we can see in the first two columns the required sample size per group (under the “N1” and “N2” columns) of 64. So the overall required sample size is 64 + 64 = 128. The other columns report some of the other key assumptions we made, such as the desired power and level of significance at which we would reject the null hypothesis.\nHowever, we would finally need to adjust this initial sample size for the assumed response rate:\n\n\n128 / 0.9 = 142.2\n\n\nAs this is an odd number when rounded (143) we would add another 1 to be conservative.\n\n\nTherefore, we need to sample and recruit 144 / 2 = 72 individuals, assuming 50% are smokers and 50% are non-smokers, to ensure we have an 80% chance (our power) of detecting a difference between each groups’ mean systolic blood pressure of 5mmHg or greater, on the assumption that such a difference (or greater) exists in the population, when carrying out a null-hypothesis significance test based on an independent t-test with a two-sided p-value and a significance level of 5%.\n\n\nThat’s quite a complicated interpretation. Another way to think about it is that if we did our study an infinite number of times, each time calculating the p-value for the null-hypothesis that the difference between the two means we obtain = 0, and rejecting that null-hypothesis when the p-value is ≤0.05, then if the true difference in mean systolic blood pressure in the population between smokers and non-smokers is ≥5mmHg (in either direction), then on average 80% of the time (in 80% of our repeated studies) we would get a p-value ≤0.05 and correctly reject the null-hypothesis.\n\nStill confused? Again, I’m afraid it’s a pretty complex series of concepts without an easy interpretation, and you just have to keep coming back to it until it starts to become clearer.\nAgain, this calculation assumes you will be carrying out an independent t-test of the difference between your group means, either via a classical independent t-test or within the context of a linear regression model.\nHowever, as for the previous sample size calculations, the accuracy of this interpretation depends entirely on the underlying assumption that there is no bias in the study. If, for example, the 90% response rate is correct, but the 10% who don’t participate when approached are all smokers, even if the true difference between smokers vs non-smokers is that smokers have systolic blood pressure values that are on average &gt;5mmHg higher than non-smokers we would have a power &lt;80% to detect this difference because the observed difference in our sample would be biased downwards. That is, even if we recruit 72 participants per group we might actually only have a 10% chance of detecting a statistically significant difference between the two groups’ mean systolic blood pressures, because of the influence of this bias (known as differential non-response).",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#exercise-4-sample-size-required-to-test-whether-two-independent-proportions-are-different",
    "href": "sample-size.html#exercise-4-sample-size-required-to-test-whether-two-independent-proportions-are-different",
    "title": "Sample size",
    "section": "Exercise 4: Sample size required to test whether two independent proportions are different",
    "text": "Exercise 4: Sample size required to test whether two independent proportions are different\n\nScenario\nYou work for a research NGO in a country where public health facilities have reported increasing numbers of patients with cardiovascular diseases over the last decade. You have received funding to carry out a descriptive study using a cross-sectional survey design with a focus on exploring how smoking tobacco (measured as a binary yes/no self-reported outcome) varies on average between different key socio-economic groups, given it’s causal links to cardiovascular disease, particularly women and men, which is the key comparison you will base the sample size on.\nBinary outcomes are naturally summarised as proportions/percentages, and a point prevalence is simply the proportion/percentage of individuals (or other units) that have a characteristic of interest at a certain point in time. Therefore, we can view our comparison of interest for our analysis as being a comparison between the proportion/percentage or point prevalence of smokers among women compared to the proportion/percentage or point prevalence of smokers among men. Analytically, there are various, slightly different, methods by which you can compare independent proportions, and it’s not always clear which method sample size calculations are based on. However, the results shouldn’t be too different.\nOptions include a chi-squared test of independence, which is what we will assume we will be analysing the data with. Other pptions include an independent z-test, which is essentially a t-test but for validity with binary outcomes assumes a large (e.g. &gt;30) sample size per group and neither group having a proportion near 0 or 1. Another option is a Fisher’s exact test. Or you could also use a logistic regression of the outcome with a covariate for the groups being compared, potentially with additional covariates if there was a clear rationale for adjustment.\nUnlike when you are calculating a sample size when comparing independent means, as we’ll discuss further below, there is a complicating issue. It is not just the difference in the outcome between the two groups that affects the sample size but also the value of the outcome (i.e. the assumed proportion) in each group. This is due to how the variance is calculated, as it depends on the mean or proportion itself.\nFor our scenario though we’ll assume that we have good data on the existing point prevalence of smokers among patients (i.e. the proportion of patients who smoke) in public health facilities, and that this is no greater than 0.3 (30%) among men. We’ll also assume that based on consultations with clinicians and health officials in your country it has been agreed/decided that the smallest difference in the prevalence of smoking which is considered clinically meaningful/important, and therefore worth detecting to inform policy/resource allocation, is a difference of 0.1 (10 percentage points), i.e. assuming the prevalence is 30% for men and 20% for women.\n\n\nSample size assumption inputs\nSeveral of the assumptions are identical to those for the comparing two means sample size calculation, and so we will not explain them again.\n1. What alpha (α) level or level of significance do you want?\n\nWe’ll leave the level of significance as its default at 0.05.\n\nSee the relevant description in the comparing two means section above if you need a reminder about this parameter.\n2. What power (1-β) do you want?\n\nWe’ll leave the power as its default at 0.8.\n\nSee the relevant description in the comparing two means section above if you need a reminder about this parameter.\n3. What is the expected difference in your outcome between the two independent groups (i.e. the expected difference in the two group proportions)?\nAs mentioned earlier we must be explicit about the outcome proportion expected in each group to set our minimum target difference we want to detect. As with the sample size for the mean difference, the difference we specify here should be the minimum target difference: the minimum difference we want to be able to detect, should it exist. Many studies instead base their sample size on the difference they expect (hope!) to see, but this inevitably is usually overly optimistic given the resource and monetary costs associated with a larger sample size. Therefore, such an optimistic approach often leads to wasted effort and money, because the sample size turns out to be too small to produce useful results.\n\nWe’ll specify the expected proportions for each group explicitly, based on the minimum difference we want to be able to detect and our assumed existing information. We will therefore specify the assumed percentage of smokers for men as 30% and the expected percentage smokers for women as 20% to ensure we obtain a sample size based on our minimum target difference. If the true difference we find is greater than this then we’ll have more power than we expect.\n\nNote: it doesn’t matter which we round we specify these group percentages. As you’ll see in SPSS we can specify group 1 as 0.2 and group 2 as 0.3 or vice versa.\nNote: in the sample size calculation we will assume that the hypothesis test we will use will be the chi-squared test of independence, which we will cover in section 7.1. Chi-square test of independence.\n4. What is the ratio of group sizes that you expect?\nSee the description in the comparing two means section above.\n\nAgain we’ll assume we have no reason to believe that either group will be larger than the other and so we’ll assume a group size ratio of 1.\n\n5. What is the expected response rate?\n\nAgain, we’ll assume a response rate of 0.9 (90%).\n\n\n\nCalculate the sample size\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size required when comparing two independent proportions via the chi-square test of independence\n\n\nRead/hide\n\n\nFrom the main menu go: Analyse &gt; Power Analysis &gt; Proportions &gt; Independent-Samples Binomial Test.\nIn the Power Analysis: Independent-Sample Proportions tool window at the top in the Single power value: box enter our desired power: 0.8 (the most common convention).\nLeave the Group size ratio: as 1.\nThen enter the outcome proportion values for each group that specify the minimum difference between the groups that we want to be able to detect. For Proportion parameters for group 1: enter 0.2 and for group 2: enter 0.3. As mentioned above though, we could equally specify group 1 as 0.3 and group 2 as 0.2. It makes no difference.\nWe will leave the Significance level at 0.05 (the most common convention).\nAnd we will leave the Test Method as the Chi-squared test, with the Standard deviation is pooled box ticked (as default) but we will also tick the Apply continuity correction box, which makes our estimate a bit more conservative but more likely to give us our desired power.\nWe will also leave the Test Direction again as Nondirection (two-sided) analysis, as we want to test whether the two groups are different, not whether there is a difference in one specific direction.\nNow you can click OK.\nAgain, we can see the required sample size for each group in the first two columns (“N1” and “N2”). Some of the other columns list some of the other key assumptions made, specifically the power and level of significance. However, we also get our minimum desired difference that we want to be able to detect expressed in some different ways: as a risk difference (i.e. 0.2 - 0.3 = -0.1), as a risk ratio (i.e. 0.2/0.3 = 0.667), and as an odds ratio ((0.2/0.8) / (0.3/0.7) = 0.583). So our required overall sample size is 313 + 313 = 626.\n\nFinally, let’s adjust the required sample size for our assumed response rate:\n\n626 / 0.9 = 695.5.\n\n\nTherefore, after rounding up to the next even number, we need to sample and recruit 696 / 2 = 348 individuals, assuming 50% are men and 50% are women, to ensure we have an 80% chance (our power) of detecting a difference in the prevalence of diabetes between the groups of 10 percentage points or more, on the assumption that such a difference exists in the population and that the difference is between men with a 30% smoking prevalence and women with a 20% smoking prevalence, via a null-hypothesis significance test based on a two-sided p-value with a significance level of 5%.\n\n\nAgain, that’s quite a complicated interpretation. Another way to think about it is that if we did our study an infinite number of times, each time calculating the p-value for the null-hypothesis that the difference between the proportions we obtain = 0, and rejecting that null-hypothesis when the p-value is ≤0.05. Then if the true difference in smoking prevalence in the population between men and women is 10 percentage points lower in women when men have a smoking prevalence of 30% or less, then on average 80% of the time (in 80% of our repeated studies) we would get a p-value ≤0.05 and correctly reject the null-hypothesis. Still confused? I’m afraid it’s a complex series of concepts without an easy interpretation, and you just have to keep coming back to it until it becomes clearer.\n\nAnalytically, I believe this calculation assumes you are carrying out a z-test to test the null hypothesis that the difference between the two proportions is 0, although SPSS does not make this clear from what I can see! A z-test is identical to an independent t-test apart from using the normal distribution, so it assumes you have a “reasonably” large sample size, e.g. at least 30 per group. Assuming this is accurate, in practice it would probably be more common to use a logistic regression model to compare the two proportions, in which case the sample size won’t be directly applicable but should be approximately accurate.\nAgain, as for the previous sample size calculations, the accuracy of this interpretation also depends critically on the underlying assumption that there is no bias in the study, and biases such as selection bias in the form of differential non-response will certainly invalidate the assumptions in the calculation.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#instructions",
    "href": "sample-size.html#instructions",
    "title": "Sample size",
    "section": "Instructions",
    "text": "Instructions\nIn the “Exercises” folder open the “Exercises.docx” Word document and click on the Planning sample sizes heading in the contents and follow the instructions, then check your answers with the model answers below.\nReveal the below if you want a hint about the type of sample size calculations required for each exercise scenario\n\n\nRead/hide\n\n\nScenario 1: given the scenario we are aiming to calculate the sample size required to estimate a single proportion (although in reality it will apply to all the proportions we estimate in the scenario survey).\nScenario 2: given the scenario we are aiming to calculate the sample size required to detect compare two means (or more specifically we are aiming to detect a difference between two means as statistically significant, i.e. with a p-value less than some threshold, with a given level of power).\n\n\n\nSample size exercises model answers\n\n\nRead/hide\n\nExercise 1: public primary-care health facility patient satisfaction survey\nFor our survey we estimated that for any population proportion we wish to estimate (e.g. from a binary outcome or a category level from a categorical outcome with &gt;2 levels) we require a sample size of 117 (i.e. we need to try and recruit 117 facilities into the survey) to achieve a 10 percentage point level of precision (95% confidence interval ±10 percentage points), assuming the proportion we estimate is 0.5, and assuming a response rate of 80%.\nExercise 2: pilot intervention study to reduce drug stock-outs\nWe estimated that we required a total sample size of 96 health facilities, i.e. 48 per group, to detect a difference in the mean number of essential drug stock-outs during the three-month study period (the primary outcome) in the intervention group compared to the comparison group of -15 or greater, assuming a standard deviation of 25, based on a hypothesis test of the difference in the primary outcome between the two groups (assuming the outcome is t-distributed), with a level of significance of 0.05 and a power of 0.8. We also assumed a follow-up rate of 95%.\nRemember to adjust for the response rate, and then to add 1 if you end up with an odd number to allow you to divide by 2 for the group sizes.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#reporting-sample-size-calculations-in-methods-sections",
    "href": "sample-size.html#reporting-sample-size-calculations-in-methods-sections",
    "title": "Sample size",
    "section": "Reporting sample size calculations in methods sections",
    "text": "Reporting sample size calculations in methods sections\nFor any methods and results reporting, such in a paper or report, you should explain what your study sample size was and how you calculated it, including all the assumptions that went into it. You should also report where any assumptions came from, i.e. what they were based on/how you chose them, unless they were chosen by convention, such as a 95% confidence interval or a level of significance of 5%. There is no set sequence in which you have to report the different assumptions/inputs used in your sample size calculation, but the below examples are one way you might do it. Lastly, you don’t have to explicitly state whether you are reporting a sample size calculated via a confidence interval or hypothesis testing based approach as this should be clear from the assumptions reported.\nIn the below examples we will not report any assumptions about the response rate or group size ratio, but if you make an assumption for these other than 100% or 1 respectively this should also be reported, and the basis for these assumptions justified. Also, the assumption values are just for illustration and not real!\n\nConfidence interval based sample size calculation\nWhen estimating a mean you simply need to report the expected standard deviation, the level of precision or margin of error, and the confidence level (this is often not presented, presumably because the assumption is 95%, but why not be clear?). You should also explain where your assumption of the expected standard deviation and level of precision came from.\nExample methods reporting text for a confidence interval based sample size calculation for a survey of systolic blood pressure:\n\nWe estimated that we required a sample size of 100 to estimate the mean of our primary outcome of systolic blood pressure (mmHg) with a level of precision (95% confidence intervals) at most ± 5 mmHg, assuming a standard deviation of 10. We based our assumption of the expected standard deviation on data from our previously reported pilot study (reference), which we rounded up from 7 to 10 to be conservative. Our level of precision was chosen based on consultations with relevant clinicians and health officials about what level of precision they required for usable results.\n\nWhen estimating a proportion you simply need to report the expected proportion, the level of precision or margin of error, and the confidence level. You should also explain where your assumption of the expected proportion and level of precision came from.\nExample methods reporting text for a confidence interval based sample size calculation for a survey of hypertension prevalence:\n\nWe estimated that we required a sample size of 100 to estimate the proportion of individuals with hypertension, which we assumed to be 0.3, with a level of precision (95% confidence intervals) where the lower limit was at most 0.25 and the upper limit was at most 0.3. We based our assumption of the expected proportion on data from our previously reported pilot study (reference) of 0.2, which we rounded up by 0.1 towards the most conservative value of 0.5 to be conservative. Our level of precision was chosen based on consultations with relevant clinicians and health officials about what level of precision they required for usable results.\n\nTake care, when reporting the confidence interval ranges for your proportion/percentage that you are clear whether they are on an absolute scale (probably the easiest to understand and not misinterpret) or a relative scale.\n\n\nHypothesis testing based sample size calculation\nFor a hypothesis test based sample size calculation for a difference between two independent means you simply need to report the expected difference in means (or if you prefer the expected mean in each group) and the expected pooled (or common) standard deviation, plus your pre-specified level of significance/alpha and the power, and that you are using a two-sided hypothesis test (unless of course you are not). You should also explain where your assumption about the expected difference in means (i.e. the target difference) and the expected pooled standard deviation came from. Lastly, although it’s often not done you should explain what distribution you are assuming your outcome follows\nExample methods reporting text for a hypothesis test based sample size calculation for a study comparing the effectiveness of an intervention at reducing systolic blood pressure in an two-group comparison RCT:\n\nWe estimated that we required a sample size of 100 to detect a reduction in our primary outcome of systolic blood pressure (mmHg) between our intervention and control arms that is at least 5 mmHg, based on a two-sided hypothesis test (assuming the t-distribution). This assumes a pooled standard deviation of 10 mmHg, the standard level of significance of 0.05, and a power of 0.8. Our target difference for detection was chosen based on consultations with relevant clinicians and health officials about what size of impact would be required for the intervention to be considered worth funding and scaling up. Our expected standard deviation was chosen based on values from relevant previous studies (references), which we rounded up from 7 to 10 to be conservative.\n\nFor a hypothesis test based sample size calculation for a difference between two independent proportions you simply need to report the expected proportion for each group (or equivalently the expected proportion in the reference/control group and the absolute or relative expected difference in the proportional outcome, but this is arguably less easy to follow), plus your desired level of alpha and power, and that you are using a two-sided hypothesis test (unless of course you are not). You should also explain where your assumption about the expected difference in proportions (i.e. the target difference) came from.\nExample methods reporting text for a hypothesis test based sample size calculation for a study comparing the effectiveness of an intervention at reducing the prevalence/proportion of individuals having hypertension in an two-group comparison RCT:\n\nWe estimated that we required a sample size of 100 to detect a difference in the proportion of individuals with hypertension at study follow-up where we expect the proportion with hypertension in the intervention group is 0.2 and the expected proportion in the control group is 0.3, based on a two-sided hypothesis test. This assumes the standard level of significance of 0.05 and a power of 0.8. Our target difference for detection was chosen based on consultations with relevant clinicians and health officials about what size of impact would be required for the intervention to be considered worth funding and scaling up, with the expected proportion of individuals with hypertension in the control arm based on existing routine clinical data rounded up from 0.25 to 0.3 (towards the most conservative assumption of 0.5) to be conservative.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#estimating-sample-sizes-when-testing-for-differences-between-means-or-proportions-where-the-outcomes-are-paired",
    "href": "sample-size.html#estimating-sample-sizes-when-testing-for-differences-between-means-or-proportions-where-the-outcomes-are-paired",
    "title": "Sample size",
    "section": "Estimating sample sizes when testing for differences between means or proportions where the outcomes are “paired”",
    "text": "Estimating sample sizes when testing for differences between means or proportions where the outcomes are “paired”\nWe will not look at these sample size scenarios/approaches or practice them as they are not commonly needed, but they are for when your study design involves comparing means or proportions between two groups where those groups are not independent. For example, if you are comparing the change in systolic blood pressure (mmHg) in the same individuals where their systolic blood pressure is measured at two separate times, or where you are comparing the proportion of individuals with diabetes where that diagnosis is made at two separate times within the same group of individuals. Most of the assumptions that go into these calculations are exactly the same as for the calculations we’ve already covered and the rest you should be able to work out or get help with if you ever need to use them, which is not likely.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  },
  {
    "objectID": "sample-size.html#adjusting-for-clustering",
    "href": "sample-size.html#adjusting-for-clustering",
    "title": "Sample size",
    "section": "Adjusting for clustering",
    "text": "Adjusting for clustering\nWe will not look at adjusting for clustering beyond saying that if you believe this issue applies to your study you should seek advice from a statistician/researcher experienced with making the necessary adjustments. What is clustering? As an example, if you collect data on pupils within different schools to look at test scores then those pupils within the same schools are likely to have correlated test scores. This is due to differences at the school level, such as difference in the overall quality of teaching, the socio-economic circumstances of the schools’ catchment areas, whether they charge fees etc. Hence, you don’t have the same amount of statistical information as for a true simple random sample, because pupils are not independent when they come from the same school. Standard sample size calculations, such as those we’ve looked at, assume your sample data are independent. They would assume that two randomly selected pupils from the same school are no more or less likely to have similar outcome values, such as test scores, than two randomly selected pupils from separate schools. Rarely will this be the case.\nTherefore, unless you adjust for the “level of clustering” in your outcome you won’t achieve the level of precision or power that you expect to get from a given sample size even if all the other assumptions are accurate. There are different ways of measuring the “amount of clustering” in an outcome, and it’s a more advanced topic beyond the scope of this introductory course that you should seek assistance with if you need to carry out such a sample size calculation in the future.",
    "crumbs": [
      "5. Sample size calculations"
    ]
  }
]
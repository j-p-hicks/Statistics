[
  {
    "objectID": "6-describing-a-dataset.html",
    "href": "6-describing-a-dataset.html",
    "title": "",
    "section": "",
    "text": "In the last section we saw how to prepare a raw dataset for analysis. Once you have prepared your dataset for analysis the first step in any data analysis is to describe your dataset. This is done for two distinct reasons, although the methods used as the same and the results produced may be used for both purposes.\n\nIn the first stage researchers usually just describe all the variables in their data on their own, producing what are known as univariate descriptive statistics (where univariate means “single variable”). Then in the second stage they may often look at relationships between variables, with bivariate analyses looking at simple relationships between just two variables (“bivariate”) at a time. They may also use multivariate analyses to look at relationships between three or more variables at a time, but this is much less common.\n\nNote: there is often some cross-over between this stage and the data preparation stage, because as we saw then we also use descriptive statistics to help explore our variables and highlight anomalous (i.e. potential errors) or missing values when preparing and cleaning our data.\nThe main reasons for initially doing descriptive analyses are:\n\nFirst, this allows you understand your variables (e.g. their values and distributions) and the relationships between them, which helps you plan your inferential analyses, particularly if they are exploratory rather than pre-planned.\nSecond, this allows you to describe what the sample “looks” like in terms of its key characteristics. These results often come in the first table (table 1) of a results paper. E.g. in the following exercise we will describe the key bio-medical and socio-demographic characteristics of the patients sampled. Or in a health facility study you might have data that allows you to describe a range of key facility characteristics, such as the number of beds, the number of staff, frequency of drug stock outs etc. Describing the sample’s characteristics then allows readers to understand “who” was in the study and what target population(s) the results can be generalised to. E.g. in our study the descriptive statistics for age show that all participants were aged 18 to 60, and that most of the participants were between their early 20s to late 50s. Therefore, clearly the results can only be formally and robustly generalised to individuals from the same setting who are also between 18 to 60, and probably most robustly to those individuals between their 20s and 40s, i.e. not to individuals from the same setting who are adolescents (very young) or geriatrics (very old). And when you start trying to generalise results beyond your setting and context you need to be very careful and think very carefully about how valid those generalisations are.\n\nNote: when descriptively exploring variables to understand your data and plan your analyses it is recommended to make extensive use of not just numerical descriptive statistics but also graphs, because we can easily understand and process information visually. However, graphs are not typically used in papers for describing samples, mainly due to space constraints. The most commonly used graph to describe a single numerical variable is a histogram, and the most commonly used type of graph to describe a single categorical variable is a bar chart.\nIf you are still unclear on the difference between descriptive statistics and inferential statistics, or want a brief refresher, you can read the below information.\n\n\n\n\nRead/hide\n\nIt’s important to be very clear on the distinction between descriptive and inferential statistics. Descriptive statistics summarise key statistical characteristics of variables or relationships between variables but only as they apply to your sample. Strictly speaking they do not allow you to make robust generalisations about the characteristics of variables or the relationships between variables in a target population, such as the population from which the sample was selected. For example, we often want to describe the typical value of numerical variables via the median or mean, such as the typical age of individuals in our sample, but the median or mean. we calculate cannot robustly tell us the typical age of individuals in the population we selected our sample from.\nTo describe the likely value of our statistical summary in the target population requires inferential statistics such as (most commonly) confidence intervals and p-values. For example, confidence intervals are usually presented around a relevant point estimate, which will usually equal the corresponding descriptive statistic. However, the point estimate is now taken as the single best estimate of the unknown value of the summary statistic in the population, which we call a “population parameter”. The confidence intervals then provide a range of values that indicate, to a given level of confidence, where the unknown population parameter’s value is likely to lie.\nThis terminology can be a source of confusion particularly for survey-type studies, where we often say we are aiming to describe the characteristics of a given target population, but what we mean is we are going to use inferential statistics (typically confidence intervals around our point estimates) to say something about the unkown characteristics of our target population (i.e. to describe their likely characteristics). However, we are not implying that we are going to use descriptive statistics alone to do this, as descriptive statistics (i.e. point estimates such as means and percentages) on their own don’t allow us to say anything robust about the likely characteristics of our target population.\nTo illustrate the difference: if we have a list of all post-graduate students of the University of Leeds and we take a simple random sample of 100 of those students and measure their blood pressures (mmHg) we can calculate the mean blood pressure for the sample exactly (i.e. the mean blood pressure of the sample would be a descriptive statistic describing the typical value of blood pressures in the sample). Similarly, if we were able to measure all of the students’ blood pressures we could then also calculate the exact mean blood pressure for the target population, i.e. all UoL post-graduate students, which would also be a descriptive statistic describing the target population. But in practice such a census is not possible. Therefore, if we want to say something about the likely mean blood pressure in the target population, i.e. all post-graduate students at the UoL, based on our sample alone we need to use inferential statistics. For inferring characteristics about single variables we typically calculate the relevant sample statistic from the sample, which is just identical the descriptive statistic, and is the best single or “point estimate” of the likely parameter of interest (e.g. the mean) in the target population (assuming a random sample, or more technically a probability sample). For our example this would be the mean blood pressure (mmHg). Then to allow us to make inferences/generalisations to our target population, that we took our sample from, we could estimate confidence intervals (usually 95% CIs) around that sample statistic. In other situations, for inferring relationships between variables we might estimate the difference between two variables or the correlation or a regression coefficient, which again are all sample statistics and on their own can be used as descriptive statistics, but for inference we would then also compute the relevant confidence intervals, and maybe also compute p-values where there are useful null hypotheses that we can test about those statistics.\n\n\n\n\n\n\nAim: create a table of descriptive statistics to describe the key socio-demographic and health-related characteristics of a sample.\n\n\nFirst, load the “SBP data final.sav” SPSS dataset. This has all the errors removed from the “SBP Excel data.xlsx” dataset we looked at last session, and all the SPSS-specific variable properties, like variable labels and value labels etc, have been updated. We will be describing the key socio-demographic and heath-related characteristics of the individuals in the dataset, who are our sample.\nNext, in the MSc & MPH statistics computer sessions practical files “Exercises” folder also open the “Exercises.docx” Word document and scroll down to Descriptive statistics table. Your goal is to complete the empty table by calculating the values of appropriate descriptive statistics for each of the variables in the table (i.e. the key characteristics of the sample).\n\nNote: the footnotes to the table are typical of such tables in papers and allow us to just present a single column of descriptive statistics. However, you could of course present the values in other ways, as long as it is clear what the statistics are. Once you’ve completed your table you can compare it to the completed one at the end of this section to see how you did.\nBelow is an explanation of the different types of descriptive statistics that are appropriate to use to describe numerical or categorical variables. Use this information to plan what types of descriptive statistics to calculate for each of the characteristics in the table, based on the type of variable that characteristic is measured by. In the dataset the variables corresponding to the characteristics in the table should be self-evident, but to avoid confusion they are (characteristic then variable name):\n\nSystolic blood pressure = sbp\nHypertension = htn\nAge = age\nSex = sex\nSocio-economic status = ses\nSalt = salt\nBMI = bmi\nUsed an ACE inhibitor over the last 3 months = ace\n\n\n\n\nThe typical approach is that if the variable is approximately normally distributed you calculate the mean and the standard deviation (SD), but if the variable is skewed you calculate the median the interquartile range (IQR) (or possibly the range, but this is completely dependent on the most extreme values in the variable so it’s very sensitive to atypical values). This applies whether the variable is continuous or discrete. In tables the convention is to present the mean and then in brackets after the mean the standard deviation, or the median and then in brackets after the median the standard deviation. E.g. for a mean of 10 mmHg and a SD of 2.5 you would write: 10 (2.5). The units can either be given after the mean or just included in the variable description like in the table.\n\n\n\n\n\nSimply calculate the frequency (count) and the percentage (or proportion, but percentages are typically used) for each category level. E.g. for the variable sex calculate the frequency and the corresponding percentage of individuals who are male and female. This would similarly apply to a binary variable.\n\nPresent values just to 1 decimal place. We have completed the first two variables for illustration.\n\n\n\nVideo instructions: calculate common descriptive statistics\nWritten instructions: calculate common descriptive statistics\n\n\nRead/hide\n\nCategorical variables\n\nTo calculate descriptive statistics for the categorical variables we just need to produce frequency tables for each variable and extract (i.e. copy) the counts for each category level and the corresponding percentages. Simple! From the main menu go to: Analyze > Descriptive Statistics > Frequencies. Then in the Frequencies tool window add all categorical variables to the Variable(s): box. You can either drag each variable across one-by-one by clicking and holding down the left mouse button, or you can click on each one and then click the button in between the two boxes to move it over (or to move it back), or you can select multiple variables and move them at the same time in the same way by first holding down ctrl and then clicking on as many variables as you want to before moving them. Note: this is the typical behaviour of these types of variable boxes in SPSS and you will spend quite a lot of time moving variables so it’s good to try the different methods. Once you have moved all the categorical variables in the table over to the Variable(s): box just ensure the Display frequency tables box at the bottom of the tool is ticked and then click OK. After a moment in the output window you should see the tables appear (one for each variable). You can now copy the relevant values from the Frequency and Percent columns into the table.\n\nNumerical variables\n\nNow for numerical variables. First we need to check the variables’ distributions to see whether they are approximately normal or skewed. There are statistical tests that can be used to formally test whether a variable’s distribution differs “significantly” from a normal distribution, but as they are based on p-value thresholds the result depends strongly on the sample size, and with a big enough sample size you are essentially guaranteed that a variable will fail the test and be classed “non-normal”. However, many statistical tests that rely on normality are quite robust to modest (or sometimes worse) departures from normality, i.e. they still work well with slightly skewed variables and have much more power than equivalent non-parametric tests. Therefore, it’s better to judge normality by eye using histograms even if this seems unscientific or less robust than using a statistical test.\nSo let’s see how to create histograms for numerical variables to check their distributions. In the main menu go: Graphs > Legacy Dialogues > Histogram. Then add the sbp variable into the Variable: box and tick the Display normal curve box just below the Variable: box. This adds a curved line to the histogram which shows what the distribution of values would look like assuming the data were normally distributed. If the histogram follows this fairly well we can assume approximate normality. Now just click OK. Do this for each numerical variable in turn and examine the graphs. What do you see? sbp, age and bmi all appear to approximate a normal distribution well, but salt displays some slight right skew. Therefore, let’s present the mean and SD for sbp, age and bmi, and the median and IQR for salt. Go: Analyze > Descriptive Statistics > Explore. Then in the Explore tool window add each numerical variable to the Dependent List: box. Then click the Statistics button and ensure the Descriptives box is ticked and click Continue. Then back at the main Explore tool window in the Display options at the bottom ensure the Statistics option is selected and then click OK. You’ll then get several tables produced in the output window, one for each variable, with lots of descriptive statistics listed. Simply copy the relevant statistics (mean/median and SD/IQR) from them and add them to your Table 1.\nRepeat the above processes for each characteristic, given the appropriate descriptive statistic you are calculating, and complete the table.\n\n\n\n\n\n\n\nRead/hide\n\n\n\n\nCompleted Table 1 for SBP data\n\n\nIf you have any glaring errors or strange differences try and work out why, and if you can’t then ask for help!"
  },
  {
    "objectID": "1-website-information.html",
    "href": "1-website-information.html",
    "title": "",
    "section": "",
    "text": "Before going any further please download the datasets and exercises files that we will be using throughout these sessions. These files can be downloaded as a single “zip” file by clicking here.\nOpen the zip file (double click on it) and move the Datasets and Exercises folders in the zip file to a suitable personal folder (e.g. create a “Statistics” folder on your M: drive folder). You can just drag each folder in the zip file (once opened) into the folder you create, You will need to access the files in the Datasets and Exercises folders throughout these sessions, so it’s important you have easy access to them.\nIf you are using your own Windows-based laptop and the zip file won’t open you may need to install an unzipper. You can download the free Winrar unzipping software from:\nwww.win-rar.com\nMac users should be able to unzip zip files with the in-built MacOS unzipping software.\n\n\n\nThroughout this website we use buttons like the one below to reveal/hide text, such as optional information, or the answers to exercises:\n\n\nRead/hide\n\nNow you can read the hidden text. You can also click the Read/hide button again to hide the text.\n\n\n\n\nThis website primarily aims to provide you with information about the aim of each computer practical session, and instructions on how to carry out the specific tasks and exercises for that session, which are intended to help you learn some useful and powerful data analysis skills. We also try to give you some guidance on how to apply these skills to real-world studies, primarily focusing on how to understand and interpret the results you will produce. We also aim to show you how to try and deal with some of the most common challenges faced when working with real-world data.\nYou may therefore feel the notes contain too much detail. However, we would argue they really only cover the basics! Data analysis is a huge topic, and you can never stop learning and improving your skills if you wish. We do try to limit the information presented to just the most critical, and we usually hide more in-depth or optional content via Read/hide buttons, which you can then choose whether to read at all or at a later time.\n\n\n\nEach computer practical session covers one or more related topics covered in a previous lecture, and there is one or more corresponding sections on the website for you to work through in the session. For example, in this first computer practical session we will be working through section 2. Sampling & sample size sub-section 2.1. Probability sampling. We will tell you at the start of each session which section(s) or sub-sections you are expected to work through. You can see all the sections listed on the left of the website, and you can use those links to move between sections.\nThe idea is that you will hopefully be able to work through each section of the website on your own, or with other students. However, when you get stuck you can obviously ask us for help! Each section is designed to provide you with some background information on one or more specific methods within the broader topic covered in that section, followed by a practice “scenario”. Each scenario is a hypothetical (made-up) situation (e.g. you need to select a sample of health facilities for a survey) that describes a realistic example of a situation where you might need to use the method(s) covered. Corresponding to the scenario is some artificial but realistic data (which you should have now downloaded - if not go back to the top of this page). You are then provided with detailed instructions on how to carry out the relevant method(s) (e.g. take a stratified random sample of facilities from the facility list) using a specific piece of software and the example data. Where the section covers more than one specific method these will be listed as “Method 1: …” etc. Then finally there will usually also be one or more closely related exercises for you to complete, using the skills you have just been shown.\nNote: once you are in a section you can navigate between the background information, the scenario, instructions on the method etc using the sub-menu that will appear on the right side of the website.\nWe provide step-by-step video-based and text-based instructions on how to carry out each method using the relevant software program. The video- and text-based instructions are identical, so you can choose whether to watch a video or read instructions in order to understand how to do everything you need to do.\nNote: with the text-based instructions we refer to the names of menus, tools, buttons etc as they appear or are named in the different pieces of software using highlighted text like this.\nPlease note that all SPSS instructions relate to SPSS version 27, which is the version that I had on my computer when making the videos. This may not be the version that you are using either on a campus computer or your own computer. However, SPSS typically changes little from version to version, especially when it comes to standard methods of data analysis. So although things might look a little different on your computer compared to a video there shouldn’t (hopefully) be any issues in following the instructions.\n\n\n\n\n\nRead/hide\n\nMSc Statistics for Health Sciences students have to complete a “summative assignment”, which accounts for 100% of your module mark. This assignment will require you to directly apply the skills you learn in these sessions. Below we describe this in more detail. If you are a MPH Health Systems Research Methods student then your module assignment is quite different and doesn’t require you to directly apply the skills learned here, although they should be helpful in your assignment. Please see your module handbook for details of your assignment.\nFor MSc Statistics for Health Sciences students: in summary, when analysing data for the summative assignment we only expect you to use methods that we will cover in these sessions. For further details please see the module handbook.\n\n\n\n\n\n\nRead/hide\n\nStudents often find learning and doing statistics to be hard, sometimes very hard! Unfortunately this is to be expected and there are no short-cuts or secrets. Unless you are a genius then like most people, including ourselves, to really begin to understand statistics more deeply and to be able to confidently and correctly analyse typical datasets independently will take a lot of time and effort. However, we firmly believe that it is perfectly possible for any motivated student to get to any level of skill and understanding they wish to reach if they are simply prepared to put in the necessary effort and practice, and the rewards can be huge. Being able to correctly run and interpret even simple statistical analyses gives you skills that are highly valuable in the world of research, and which are increasingly valuable and sought after in the wider world. So please don’t despair when things seem difficult or impossible. We’ve all felt that way, but like any new skill it really does gets easier and easier with practice, and it can even be enjoyable (no really, being able to take a dataset and generate insights about the world can be a really exciting and enjoyable process!)."
  },
  {
    "objectID": "10-paired-t-test.html",
    "href": "10-paired-t-test.html",
    "title": "",
    "section": "",
    "text": "The information and instructions given here will be kept brief, as once you understand when to use the paired t-test rather than the independent t-test there’s not much more to know because the interpretation is very similar to the independent t-test.\nThe paired t-test allows us to compare whether the mean of an outcome differs between two groups when observations within those two groups are correlated. This is most typically the case when the groups represent measurements taken from individuals or other entities (units of observation) at two points in time. For example, if you were interested in the effect of an intervention on blood pressure you might have measured systolic blood pressure in a group of individuals before the intervention and then repeated the measurements after applying the intervention. Less clearly you would also have paired observations when collecting outcome measures from pairs of spatially or geographically related entities. For example, if you were interested in HIV rates within villages located along a major highway and were specifically looking at whether villages on the side of the road where traffic is travelling from the capital city have higher HIV rates than villages located on the other side of the road where traffic is travelling to the capital city. You could then compare pairs of villages that are on opposite sides of the road. Clearly though, adjacent villages will likely have correlated HIV rates compared to villages that are further away, and so your observations would be correlated or paired.\nIn summary, there’s no fool proof way to work out if your data are “paired” or correlated, but if you think carefully you should be able to work out what is likely based on the study design and subject matter knowledge.\nTherefore, if you have paired data and your research question is about whether there are differences between the two groups making up the pairs then you can use the paired t-test. The paired t-test is actually largely just the independent t-test but applied to the differences between the pairs of observations (e.g. the difference between the systolic blood pressure measured before an intervention compared to after an intervention in each individual in a study), rather than applied to two groups of observations. Let’s see how to do the analysis. Note: the way you would store such data for use in a paired t-test is to have two outcome variables: one for each member of the paired observations.\n\n\n\n\nLoad the “AB paired.sav” SPSS dataset.\n\nThis contains real data from a cluster randomised controlled trial done by Chinese partners in China and Canada with support from Joe Hicks. The research question was whether a multi-component complex intervention could reduce inappropriate prescribing of antibiotics by primary care providers to children (aged 2-14) in Chinese primary care facilities. However, this dataset just contains data from the intervention arm before and after the intervention. There are two variables: apr_base and apr_end. apr_base contains the antibiotic prescription rates (actually proportions not true rates) for primary care facilities before the intervention was applied in the intervention arm, and the apr_end contains the antibiotic prescription rates for the same primary care facilities after the intervention had run for six months in the intervention arm. Hence, each row contains data from the same primary care facility, and therefore the outcomes are correlated: they are pre and post measurements from the same facilities. The antibiotic prescription rates are more specifically the facility-level proportion of prescriptions issued to children (aged 2-14) for upper respiratory tract infections that contain one or more antibiotics. Therefore, broadly speaking these will be inappropriate and a lower rate is more desirable. We’ll use a paired t-test to explore the relationship between the antibiotic prescription rate and the intervention period (pre or post intervention) as a proxy for evidence of the intervention’s potential effectiveness, and then you can try a similar analysis by yourself.\n\n\n\n\n\nTechnically a paired t-test assumes a continuous outcome variable, but as long as the following two assumptions are satisfied it’s fine to use a discrete outcome with a paired t-test.\n\n\n\nSee above for a discussion of how to understand if your data are suitable for the paired t-test. However, the paired t-test also assumes that pairs of observations are independent from each other. For example, if you recorded blood pressure measurements from individuals before and after an intervention the observations would be paired or correlated within individuals, but if individuals were also clustered within family grouping then there would also be correlations between pairs of individuals (as family members would likely have correlated before and after values), and the data would violate this assumption of the paired t-test.\n\n\n\nCompute the difference between each pair of observations and plot them on a histogram to check for approximate normality. In SPSS you can use the Compute Variable tool that we used earlier in the data preparation section to do this easily. You just need to enter the command: “var1 - var2” (without quotes) or vice versa, where var1 and var2 are your paired outcome variables. Or you can also do it in Excel by creating a new column from the difference between your paired outcomes. You should know how to plot a histogram now, but refer back to the “Step 1: check the assumptions of the independent t-test” section earlier for a reminder if needed.\n\n\n\n\n\nFrom the main menu go: Analyse > Compare Means > Paired Samples T Test.\nAdd apr_base as Variable1 and apr_end as Variable2 then click OK.\n\n\n\n\n\nThe results are largely the same as for the independent t-test, so refer back to the “Step 4: understand the results tables and extract the key results” section in the “Inferential analysis 1: the independent t-test” section to review what the results in the tables mean if you need reminding. However, a few things are a little different. Your descriptive statistics for each group are in the Paired Samples Statistics table, and you get the (presumably) Pearson correlation between the two groups in the Paired Samples Correlations table. Then your inferential results are in the Paired Samples Test table. There are now no equal variances assumed/not assumed version of the results, and we can see the key results are the Mean (i.e. mean difference between the reference group [entered into Variable1] and the comparison group [entered into Variable2], the associated 95% confidence intervals of this mean difference, and if you want it the two-tailed p-value (Sig. (2-tailed)), which tests the hypothesis that the mean difference = 0. You can ignore any “effect size” table.\n\n\n\n\nFor a report we would say something like:\n\nMethods: explain that you used a paired t-test and justify why.\n\n\nResults: at baseline the mean facility-level antibiotic prescription rate was 0.82. After the six-month post-intervention follow-up the mean facility-level antibiotic prescription rate was 0.4. There was therefore a mean change in the antibiotic prescription rate from baseline to six-month follow-up of 0.42 (95% CI: 0.3, 0.55).\n\n\nNote: take care interpreting the direction of change. If we compared the six-month follow-up group to the baseline group the change would be -0.4, which could be easily mistaken for a reduction when it’s actually an increase. Just be clear on the mean of each group and what comparison is being made, i.e. which group mean is being subtracted from which.\n\n\nDiscussion: interpret the direction and size of the difference in terms of the implications for practice and policy. Is the difference “statistically significant”, i.e. can we make a clear inference that there even is a difference on average between the groups, and if so is it a small difference, a medium difference, a large difference etc in terms of what is being measured and the implications for practice and policy?\n\n\n\n\nThe limitations are the same as discussed in the “Limitations” section of the “Inferential analysis 1: the independent t-test” section.\n\n\n\nOpen the simulated “MDR-TB LTFU.sav” dataset. It represents data collected from an uncontrolled pre-post (or before-after) study comparing the number of patients being treated for multiple drug resistant Tuberculosis (MDR-TB) who were lost to follow-up during a month before and after an intervention was implemented that aimed to reduce loss to follow-up. The unit of observation is the health facility, and the dataset contains three variables:\n\nfacility_id = generic facility ID.\nltfu = the number of patients lost to follow-up during the pre or post intervention period.\ntreatment_period = the treatment period (pre or post).\n\nNote: pre-post data are often stored like this in “long” format, with the outcome variable containing repeated measures for each unit of observation and a variable indicating which time period those observations come from. Therefore, you will need to rearrange the dataset into two outcome columns (pre and post) where the observations from each separate facility are on the same row (this should be easy enough to do once you look at the data).\n\nThen via the process outlined above use a paired t-test to analyse the relationship between the treatment period (i.e. in theory the effect of the treatment) and the number of MDR-TB patients lost to follow-up per facility.\nExtract the mean difference and confidence intervals around this estimate.\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Paired t-test: the relationship between an intervention and MDR-TB patient loss to follow-up.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: sample size and group outcome means. Also be sure to include sufficient details about the outcome variable and the comparison made, including the type of analysis used, and of course the key inferential results. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\nExample results reporting text\n\n\nRead/hide\n\nUsing a paired t-test I analysed the change in the number of MDR-TB patients lost to follow-up during the month before and after an intervention to reduce loss to follow-up in MDR-TB health facilities. Out of a total sample size of 10 health facilities the mean number of patients lost to follow-up before the intervention was 7.9 and after the intervention was 5.3. This represented a mean difference of 2.6 (95% CI: 1, 4.2) more patients lost to follow-up in the month before the intervention was implemented. Therefore, the intervention was associated with a clear and statistically significant reduction in the number of patients lost to follow-up in the month following its implementation compared to the previous month. However, the key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an uncontrolled pre-post study like this), particularly unmeasured time-varying confounding variables that may have impacted on patient loss to follow-up (such as performance bias [https://catalogofbias.org/biases/performance-bias/]). Therefore, this is likely to represent a biased estimate of the independent relationship between the intervention and the number of patients lost to follow-up."
  },
  {
    "objectID": "11-chi-sq-independence.html",
    "href": "11-chi-sq-independence.html",
    "title": "",
    "section": "",
    "text": "While the independent and paired t-tests allow us to analyse the association between a numerical outcome and a binary independent variable, the chi-square (or chi-square) test of independence allows us to analyse whether there is an association between a categorical outcome (with any number of category levels) and a categorical independent variable (again with any number of category levels), in terms of whether the relative frequency of observations across category levels in the outcome differs depending on the category level of the independent variable. It’s probably called the chi-square test of independence because it uses the chi-square distribution and the inferential part of the test is a hypothesis test where the null hypothesis is that no association exists between the outcome and independent variable, i.e. that they are independent.\nFor example, assume we are interested in whether there is an association between hypertension (a binary outcome) and socio-economic status, defined as having three levels: low, medium and high. Here for our purposes of illustration we don’t define these levels further, but in a real study they would be based on some, usually multivariate, computation/analysis. Statistically, we would be interested in whether the relative frequency (i.e. proportion/percentage) of hypertension differed depending on whether individuals were classed as being of low, medium, or high socio-economic status. This type of analysis could then be done using a chi-square test of independence (also called Pearson’s chi-square test after the inventor of the method, or the chi-square test of association), but note that the two categorical variables can have any number of levels.\nThe chi-square test of independence is still quite popular for such situations, which is why we are covering it, but it does have some big limitations and we’ll see later how to use a much more useful approach is logistic regression. Note: although the p-value calculated for the test is based on the chi-square distribution the test itself does not assume the underlying data follow any specific distribution and so it is technically a non-parametric test.\n\n\n\nUsing the “SBP data final.sav” dataset we’ll analyse whether there is an association between individuals’ hypertension status (hypertension/no hypertension), based on having a systolic blood pressure <140 mmHg or ≥140 mmHg, and their sex (asumed to just be male/female), and then you can try a similar analysis by yourself.\n\nLoad the “SBP data final.sav” SPSS dataset.\n\n\n\n\n1. Both the outcome and the independent variable should be categorical (either ordinal or nominal), each having at least two levels\nThis is clearly the case.\n2. Observations should be independent\nAs with the independent t-test observations should not be related/correlated, such as would be the case if observations are collected from entities within clusters, such individuals clustered within families, health facilities or other institutions etc. Again, the only way to verify this is from a full understanding of the study design and data collection processes. Here we can assume this holds because the data come from a simple random survey of separate individuals.\n3. Ideally no cell has an expected frequency less than 5 and no cells have expected values of 0\nWe can only check this assumption from the results once we’ve run the test. As we’ll see below cells are just the cross-classified category levels between the outcome and independent variable. For example, if the outcome is hypertension (hypertension/no hypertension) and the independent variable is sex (female/male) then you would have four cells: yes-female, yes-male, no-female, and no-male. We’ll also see below what the “expected” values are.\nNote: this is not really a formal rule and other researchers provide other thresholds such as no expected frequency less than 10. However, when any expected cell frequency is less than 5, particularly when there are few category levels in both outcome and independent variable (e.g. with a “2 x 2 contingency table” where both variables just have two levels), some researchers recommendation applying “Yates’s correction” (named after its creator) that aims at correcting the error introduced by assuming that the discrete probabilities of frequencies in the table can be approximated by a continuous distribution (chi-square). However, this has been shown to result in overly conservative hypothesis tests and so other researchers prefer to use Fisher’s Exact test, which is what we will mention here later. Remember you can always do a sensitivity analysis: run both versions and present both and interpret in terms of any differences.\nHowever, if any cells have expected values of 0 you must merge the relevant category level in the independent variable with one or more (logically chosen) category levels to ensure no expected values are 0. For example, if our outcome was hypertension status (hypertension/no hypertension) and our independent variable was socio-economic status (low/medium/high) and we found that our “hypertension status = no, socio-economic status = low” cell had an expected frequency of 0 you could merge the low socio-economic status level with the medium socio-economic status level and see if that solved the problem.\n\n\n\nVideo instructions: run the chi-square test of independence\nWritten instructions: run the chi-square test of independence\n\n\nRead/hide\n\n\nLet’s run the test. First, load the “SBP data final.sav” dataset. From the main menu go: Analyze > Descriptive Statistics > Crosstabs. Then in the Crosstabs tool add the htn variable to the Row(s): box and the sex variable to the Column(s): box. Note: you can put either variable in either box and it doesn’t make any difference to the actual chi-square test of independence p-value, which is the only inferential statistic we get, but the cross-tabulated frequencies and percentages will just be presented “the other way around” from how I will produce and describe them, and the way I choose seems most logical to me, but you may disagree. Then click the Statistics button and tick the Chi-square box at the top-left, and then also tick the Phi and Cramer’s V box under Nominal (we’ll explain what these are later), then click Continue.\nLet’s also edit the options for the descriptive tables that SPSS will produce, so we can see the observed and expected counts and frequencies (as percentages). Click the Cells button and under Counts ensure Observed and Expected are ticked, and under Percentages ensure Row, Column and Total are ticked.\nLet’s also produce some clustered bar charts to help us interpret the results visually, which SPSS helpful allows us to do via the tool by ticking the Display clustered bar charts box.\nFinally click OK.\n\n\n\n\n\n\nIn the output window you’ll see four tables and a bar chart. The first table titled “Case Processing Summary” just tells us how many “cases” were included or excluded in the analysis. A case represents an observation, which in our dataset represents a separate participant who had data on both categorical variables. So for example if a participant lacked data for either their hypertension status or their sex (or both) they would be excluded from the analysis. We can see none (0%) of the observations (participants) had to be excluded due to missing values.\nWe will skip the second table for now and next look at the third table down titled “Chi-Square tests”. This primarily gives us the inferential result for our chi-square test of independence, which is just in the form of a p-value from a hypothesis test that you interpret along side the point estimates of the outcome frequencies in each group of the independent variable. Note: in addition to the chi-square test of independence we also get (without asking) results from some other related tests that we won’t consider further, and in the final row of the table we get the total number of valid (i.e. non-missing) cases again. Just look at the top row (Pearson Chi-Square) for the results from our chi-square test of independence.\n\nWhat does it all mean?\n\n\nChi-Square tests section columns explained\n\n\nValue\n\nThis gives us the test statistic which is the chi-square (χ2) test statistic. It is used to compute the p-value, but you can ignore the value of the test statistic itself.\n\ndf\n\nThis is the degrees of freedom (df) for the test/p-value. It represents a measure of how many pieces of statistical information (observations) were freely available for the test. You can ignore this.\n\nAsymp. Sig. (2-sided)\n\nThis is our chi-square test of independence two-sided p-value. “Asymp. Sig. (2-sided)” stands for asymptotic significance (2-sided). This two-sided (or two-tailed) part means the p-value is computed on the assumption that the association between the outcome and independent variable could be in either direction, while the “asymp.” part means that the rest assume an infinite sample size. The infinite sample size assumption is obviously not true but may be a reasonable and useful approximation. What represents a “reasonable sample size” isn’t easy to work out, but rough rules of thumb (a “rule of thumb” is an English language saying for a rough guide) usually say at least >30 observations and no cell with <5 observations.\nAs you can see our chi-square test of independence p-value is 0.000 (i.e. <0.001 as if you remember SPSS does not give exact p-values once P<0.001). This is lower than the standard 0.05 threshold for claiming “statistical significance”.\n\nWe can actually ignore the remaining two columns: “Exact Sig. (2-sided)” and “Exact Sig. (1-sided)”. This is because as they don’t apply to our test, only if you were interested in the results of “Fishers Exact test”.\nNote: see the footnotes to the table. One reads “0 cells (0.0%) have expected counts less than 5…”, which means that this assumption of the test was not violated and so we can interpret our results.\n\n\nNow let’s come back to the second table titled “Hypertension (SBP >= 140 mmHg) * Sex (male/female) Crosstabulation” where this information comes from and understand it.\n\n\nHypertension (SBP >= 140 mmHg) * Sex (male/female) Crosstabulation table explained\n\n\nThis is a cross tabulation showing us a range of statistics for each cell, where each cell is defined by one of the unique combinations of the two categorical variables’ levels (i.e. with two binary categorical variables we have 2x2 = 4 cells: htn = yes and sex = male, htn = yes and sex = female, htn = no and sex = male, and htn = no and sex = female). In addition the total value of each statistic is presented within each level of each categorical variable across the levels of the other categorical variable (e.g. the total for htn = no across both sex = male and sex = female). Within each cell we have the following statistics/values:\nCount\n\nThe number of observations (participants) in each cell.\n\nExpected Count\n\nThe expected number of observations calculated using the formula: (row total x column total) / overall total. This calculated the expected number of observations per cell if there was no association between the two variables.\n\n% within Hypertension (SBP >= 140 mmHg)\n\nThis is the % of observations (relative frequencies) within each level of hypertension for the relevant level of sex.\n\n% within Sex (male/female)\n\nThis is the % of observations (relative frequencies) within each level of sex for the relevant level of hypertension.\n\n% of Total\n\nThis is the % of observations (relative frequencies) within each cell out of the total number of observations.\n\n\n\nSo it’s quite a complicated table to say the least to look at initially, but it’s crucial for interpreting the results of the test fully. In particular we need to look at the relative frequencies within each level of one of the categorical variables for the levels of the other variable to understand the strength and direction of possible associations between the variables, as reported in the example results reporting text above. These are the “% within Hypertension (SBP >= 140 mmHg)” and “% within Sex (male/female)” values. Let’s work through both sets of relative frequencies. For ease of understanding we have reproduced the table below. If yours looks different then it’s not necessarily wrong but you may have put the variables in a different way around, so you may wish to go back and follow the instructions above more carefully to recreate the results as they look below.\n\n\n\nChi-square test of independence crosstabs table\n\n\nBecause of the way the table is laid out (with sex along the top and htn down the side) it makes most sense to look at the results from the “point of view” of the htn variable, looking at the relative frequency of men and women for each level of htn. So to do this with the above table we are looking across the rows at the “% within Hypertension…” values. First look at the row outlined in solid red for htn = no. This tells us that out of the individuals who were htn = no 47.9% were men and 52.1% were women. Next look at the row outlined in dashed red for htn = yes. This tells us that out of the individuals who were htn = yes 69.9% were men and 30.1% were women. Therefore, there appears to be a strong association between hypertension status and sex, with men being more likely to be hypertensive than women, i.e. as we “move” from one level of htn to the other the relative frequency of sex varies substantially from roughly 50%:50% to roughly 70%:30%. We can also look at these results from the “point of view” of the sex variable, but now because of the way the table is laid out it makes more sense to look down the sex level columns or reproduce the table with the variables the other way around. Look at the values outlined in solid blue in the male column. These show us that of the individuals who were men 67.9% were not hypertensive and 32.1% were. Finally look at the values outlined in dashed blue in the female column. These show us that of the individuals who were female 84.2% were not hypertensive and 15.8% were. So again it indicates the same direction and strength of association, because it’s the same data, but looking from “the point of view” of the other variable. I find looking from the “point of view” of the assumed outcome variable more logical and intuitive (i.e. the first “view”) because it’s the assume causal relationship, but you may disagree.\nLastly we come to the fourth table “Symmetric Measures”. This presents the Phi and Cramer’s V statistics and their associated p-values. The statistics both measure the strength of the association between the two variables in the analysis, and both go from 0 (no association) to 1 (“perfectly” associated). The associated p-value shows you, assuming the Phi/Cramer’s V value is 0, how likely you were to observe a value at least as great as the one observed due to sampling error alone. These statistics can help you interpret how strong any association is, but they are difficult to interpret. When both variables have two levels then they are equivalent in interpretation to a linear correlation coefficient with a simple and useful interpretation (the strength of association on a 0-1 or 0-100% scale), but when there are more than two levels in one or both variables there’s no clear intuitive interpretation!\nTherefore, I would recommend firstly looking at the p-value to see if there is any reasonable evidence for an overall association between your two categorical variables (i.e. based on the standard P ≤ 0.05 threshold), and the looking at the relative frequencies (% of observations within different levels of one variable for each level of the other) as we have done above to judge where any associations might exist, in what direction and how strong they appear to be.\n\n\n\nIn our methods section we would explain that we used a chi-square test of independence and justify why we did so. Then in a results section we could say something like:\n\nAmong males 67.9% (201/296) of individuals had hypertension (systolic blood pressure ≥140 mmHg) while among females 84.2% (219/260) of individuals had hypertension. A chi-square test of independence showed that this represented a statistically significant association between hypertension and sex (P<0.001).\n\nThis allows readers to understand the direction and possible size of associations between the relative outcome frequencies in each of the independent variable groups, and the associated inferential hypothesis test p-value result, which loosely speaking gives us a sense of how confident we can be in concluding that the exact observed association in the sample represents the association that exists in the target population.\n\n\n\nThe main limitation of the chi-square test of independence is that we do not get any confidence intervals on our raw measures of effect size: the relative frequencies (i.e. the raw measure of the direction and strength of the association). We only get an overall p-value that tells us, assuming there is no association between the two categorical variables, how likely we are to have observed an association (in either direction) at least as great as the one we have observed due to sampling error alone. So all it can indicate is if there is likely to be an overall association between the two variables given the uncertainty in the data. It tells us nothing about the direction or size of any association, or which levels it involves.\nWhen both variables have two levels, like with our example, we know any association must be between both levels of each variable. However, when there are more than two levels in one or both variables it’s even less clear. For example, assume we looked at the association between the htn and ses variables. If we found the p-value for a chi-square test of independence was < 0.05 we could conclude there was evidence for an overall association. However, we could get a significant p-value if the relative frequency of hypertension was similar for two levels of ses and only differed for the remaining level, or we could get a significant p-value if the relative frequency of hypertension was different among all three levels of ses. Therefore, all we can do is conclude whether there is evidence for an overall association based on the p-value, and then cautiously interpret the relative frequencies (measures of effect) to see which levels, what direction and what size of associations appear to exist. As you can see this is much, much less robust and satisfactory than having confidence intervals for our raw measures of effect size.\n\n\n\nWhat if one or more of our expected cell counts are <5? Then we can use a similar test that can deal with this problem (but is typically less powerful when it’s not an issue) called the Fisher’s Exact test. To include this test in our chi-square results table when we go to the Crosstabs tool add in the variables as above but now click the Exact button and then tick the Exact button (you can leave the Time limit per test as it is). Then click OK and in the Chi-Square Tests table you will see an additional row called Fisher’s Exact Test. Again all we get is a two-sided p-value (“Exact Sig. (2-sided)”) based on the test statistic (“Value”) given the degrees of freedom (“df”), which as always just tells us, assuming the null hypothesis is true, how likely we were to have observed data reflecting a relationship that differed from the null hypothesis at least as greatly as that observed.\n\n\n\nUsing the “SBP final data.sav” dataset and via the process outlined above use a chi-square test of independence to analyse the relationship between BMI and ACE inhibitor usage during the past three months, with the hypothetical research question being is BMI related to ACE inhibitor usage?\n\nFirst convert the BMI variable into a binary variable based on BMI values <30 kg/m² and those ≥ 30 kg/m².\nRun the chi-square test of independence using the new categorical variable BMI (<30 kg/m² or ≥ 30 kg/m²) and ace.\nHint: enter your new categorical BMI variable into the Row(s): box in the Crosstabs tool and then in the Crosstabulation table look across the top and bottom halves of the table. For example, the top half of the table will, depending on how you’ve coded your BMI categorical variable, include the count (frequency) and % of individuals who reported (yes) and did not report (no) using ACE inhibitors during the past month within the relevant BMI group. You can then compare the frequency and % of individuals reporting usage between the two BMI groups, along with the chi-square p-value result in the following table.\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Chi-square test of independence: the relationship between BMI and ACE inhibitor usage.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: overall sample size and frequency and proportion/percentage of individuals with low and high BMI who reported using ACE inhibitors during the past three months. Also be sure to include sufficient details about the type of analysis used, and of course the key inferential result. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\nExample results reporting text\n\n\nRead/hide\n\nI analysed the relationship between BMI, defined/grouped as <30 kg/m² or ≥30 kg/m², and ACE inhibitor usage during the past three months. Out of a total sample size of 529 individuals, among individuals with reported BMI values <30 kg/m² 4.7% (20/429) reported using ACE inhibitors during the past three months while among individuals with reported BMI values ≥ 30 kg/m² 7% (7/100) reported using ACE inhibitors during the past three months, but this difference was not statistically significant (P = 0.3) based on a chi-square test of independence. Therefore, there was no clear evidence for a relationship between BMI when grouped as <30 kg/m² or ≥30 kg/m² and ACE inhibitor usage frequency during the past three months in this target population. However, the key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an observational cross-sectional study like this). Therefore, this is likely to represent a biased estimate of the independent relationship between BMI (as defined/grouped here) and ACE inhibitor usage within the past three months in this target population."
  },
  {
    "objectID": "12-chi-sq-goodness.html",
    "href": "12-chi-sq-goodness.html",
    "title": "",
    "section": "",
    "text": "The chi-square goodness-of-fit test\nThere is another chi-square based test usually called the chi-square goodness-of-fit test. We will not practice this test here as it’s far less commonly required/used than the chi-square independence test. In brief though, the chi-square goodness-of-fit test is a nonparametric test for single variables that is used to analyse whether the distribution of cases (units of observation) in a single categorical variable follows a known or hypothesised distribution. For example, whether the proportion of males and females in a sample follows the distribution of males and females in a given target population, or a distribution that is “hypothesised”, such as the proportion of males compared to females that we expect to have a certain characteristic/disease etc.\nA good overview of the test with instructions of how to use it in SPSS can be found here: https://statistics.laerd.com/spss-tutorials/chi-square-goodness-of-fit-test-in-spss-statistics.php"
  },
  {
    "objectID": "13-paired-categorical-test.html",
    "href": "13-paired-categorical-test.html",
    "title": "",
    "section": "",
    "text": "McNemar’s test\nLike with the paired t-test there is an alternative to the chi-square test of independence for the situation where you want to compare a binary outcome between two groups (i.e. an independent variable with two levels) but where those two groups are paired. This test is known as McNemar’s test. A typical scenario where this test might be applicable is where you have a binary outcome, e.g. a covid test (positive/negative), with the outcome measured/recorded from the same set of individuals at two points in time, e.g. before/after an intervention. We will not look further at this test or practice its use in this course, but a good overview of the test with instructions of how to use it in SPSS can be found here:\nhttps://statistics.laerd.com/spss-tutorials/mcnemars-test-using-spss-statistics.php"
  },
  {
    "objectID": "14-linear-regression.html",
    "href": "14-linear-regression.html",
    "title": "",
    "section": "",
    "text": "If necessary please read the below guide to key terminology and concepts before reading further as we will be using these terms when discussing linear regression and the modelling process in SPSS without further explanation.\n\nModel = Loosely speaking the outcome variable and the set of independent variables that you are analysing via linear regression, plus their functional form (see below). Note: although simpler analyses like the independent t-test are referred to as “statistical tests”, creating the false illusion of a fundamental difference from regression “modelling”, they have the same underlying mathematical form. Simply put, parameteric statistical tests like the independent t-test are models too.\nFunctional form/model parameterisation = In what mathematical form you add independent variables to your model. Practically speaking this is whether your model assumes 1) a simple linear relationship between a given independent variable and the outcome (also called a “main effect”), or 2) an interaction between a given independent variable and one or more other independent variables and the outcome, or 3) a non-linear relationship between a given independent variable and the outcome (although many other functional forms are possible).\nModel building = The process by which you decide which independent variables to include in your model.\nCoefficient or parameter or (model) term = The point estimate measuring/indicating the direction and size of the relationship between a given independent variable and the outcome that your linear regression estimates.\nFocal relationship = A relationship between a given independent variable and an outcome that is the focus of the analysis and research question, such as the effect of smoking status on systolic blood pressure.\nConfounder or confounding variable = With respect to a focal relationship, say the effect of smoking status on systolic blood pressure, a confounder or confounding variable is a “common cause” of both the independent variable of interest (smoking status) and the outcome (systolic blood pressure). For this example a plausible confounder might be “stress level”, because it’s very plausible that stress can induce people to take up smoking or restart smoking and that stress can increase blood pressure on its own.\nCompeting exposure = With respect to a focal relationship, say the effect of smoking status on systolic blood pressure, a competing exposure is a cause of the outcome but is not caused by the independent variable of interest and does not cause the independent variable of interest. For this example a plausible competing exposure might be the presence of a certain gene that predisposes individuals to high blood pressure, whilst having no effect on their likelihood of taking up smoking or remaining a smoker, because it would, on average, lead to higher blood pressure in individuals but not affect their likelihood of smoking.\nSufficient adjustment set = With respect to a focal relationship, say the effect of smoking status on systolic blood pressure, a sufficient adjustment set is a set of variables that, if fully “conditioned on” (i.e. adjusted for by including in a regression model), will provide an unbiased estimate for the causal effect of the independent variable of interest by adjusting for all sources of confounding.\n\n\n\n\nMultiple linear regression is used to analyse the relationship(s) between one numerical outcome variable and one or more independent variables that can be numerical or categorical or a mixture of both. When you have one numerical outcome and one numerical or categorical independent variable this is sometimes referred to as “simple linear regression”, but there’s no qualitative difference from multiple linear regression, and we’ll often just refer to “linear regression” from here on. Note: when we have one outcome variable and multiple independent variables it is a multiple linear regression not a multivariate linear regression. Multivariate refers to analyses of more than one outcome variable.\nLinear regression modelling is therefore an extremely flexible and powerful method of analysis. Linear regression models are used for a variety of purposes, but primarily they are used for prediction and causal inference. Prediction models (actually more often using logistic regression) are typically used for diagnosis or prognosis, and are usually developed using more or less “theory free” and automated methods, where data are primarily used to build the model. However, most research in the health sciences focuses on attempting to understand causes of outcomes, such as does smoking cause lung cancer? This has historically been what “statistical inference” tended to mean, at least in practice. In the context of robust randomised controlled trials it is generally accepted that causal effects can be clearly identified, and you can certainly use regression models to this as appropriate to the data. However, in the context of observational studies it is generally accepted to be much more challenging to clearly identify causal effects, primarily due to the challenges of accurately modelling the processes generating your outcome of interest and avoiding various source of bias such as confounding and other less well known biases.\nBroadly speaking, in the context of observational studies the historical, and still dominant, approach to trying to understand causal effects has been the following: 1) collect some data on an outcome of interest and a range of independent variables thought to be possible causes or confounders, 2) use a mix of theory and data-driven choices to build a regression model that is the “best” in some sense, usually at minimising the unexplained variation in the outcome, and 3) interpret all the independent variables as causal effects in relation to the outcome, at least implicitly. I say implicitly because the typical approach in research using observational studies, given the limitations of observational designs at robustly and clearly identifying causal effects, has been to refer to the relationships identified between independent variables and outcomes as “associations” or another similar term. In such studies researchers usually also state that causation cannot be inferred from the study given its limitations, but then in practice still treat the identified relationships as causal, which is clearly not a very satisfactory approach!\nFor a while now though a field know as “causal inference” has been developing and promoting methods for less biased analysis of causal research questions in observational studies, as it has become increasingly clear that existing methods and approaches have some substantial (and often hidden and/or counterintuitive) biases. Most of the learning and approaches from this field area beyond the scope of this introductory course, but we will try and incorporate some of the key ideas and findings below, while acknowledging the gaps.\nVery simply speaking, particularly in the context of observational studies, research has shown that data-driven automated model selection processes typically lead to biased models that cannot identify and accurately estimate causal relationships, while also producing inferential results that have falsely inflated precision and power. Research has also shown that it is typically not valid to interpret the results of a regression model in terms of all the independent variables in the model reflecting causal relationships. This is known as the “Table 2 fallacy”.\nInstead, within a given study every focal relationship of interest, say the relationship between stress and blood pressure, should be analysed using a separate model containing its own set of independent variables (other than the independent variable of focal interest), which may or may not differ from the set of independent variables used when analysing a different focal relationship of interest within the data, say the relationship between BMI and blood pressure. Broadly speaking, each model for every separate focal relationship of interest should contain a “sufficient adjustment set” of independent variables that suitably adjust for all key sources of confounding of that relationship, and this set of variables should be chosen based on theory and plausible/sensible decisions, not via a data-driven automated process.\nIn this practical and course we will not go further into these aspects of causal inference, and please be clear that this is just a very simplistic overview of a few of the key principles of causal inference as it relates to analysing observational data using regression models, but there is much more to learn and many details are not covered! However, we will take some of these key principles forward. Specifically, for this linear regression and the following logistic regression practical sessions we will assume we are aiming to make causal inferences about the likely causes of variation in systolic blood pressure and hypertension status respectively using the SBP data, which we assume to have come from an observational study (specifically a cross-sectional design). We will be interested in the assumed causal effects of all the available independent variables. We will not use any form of data-driven automated process when building our model, but we will select a single set of independent variables that we assume represents the sufficient adjustment set of variables for each variable in that set. That is, we will interpret the relationship of each independent variable in our single model as though it was an estimate of a possible causal effect between itself and the outcome based on a single model, i.e. we will assume that every independent variable in our model is a focal relationship of interest. However, please bear in mind that while this practice is extremely common in the scientific literature, as discussed above in practice this would almost certainly be committing the “Table 2 fallacy”, and it’s just for the purposes of keeping things simple that we won’t try and create sufficient adjustment sets for every focal relationship.\n\n\nIf you are likely to carry out quantitative research in your dissertation and/or later career I would strongly advise that as well as putting time and effort into learning statistics, i.e. analysing data, that you put time and effort into learning about causal inference and put into practice as many of the recommended approaches as you can. Although it’s still somewhat of an emerging field learning from it is being adopted rapidly by epidemiology and the health sciences, and understanding at least the basic principles and key recommendations will really benefit your research and expertise.\nSome very useful causal inference references are:\nHernan and Robins’ causal inference book. This is an excellent and comprehensive book. It gets quite technical in places but there’s not necessarily a way to simplify things further. If you want to do robust causal inference it’s not a simple or easy thing unfortunately.\n\nhttps://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/\n\nFree online course about using directed acyclic diagrams (DAGs) to select variables for models. DAGs are a brilliant and relatively easy to use tool for thinking through what independent variables constitute a sufficient adjustment set for a given focal relationship when planning an analysis of data relating to that focal relationship. They are highly recommended by causal inference practitioners, and are literally just diagrams - no maths involved.\nhttps://www.edx.org/course/causal-diagrams-draw-your-assumptions-before-your\nTable 2 fallacy paper.\n\nhttps://academic.oup.com/aje/article/177/4/292/147738\n\nWhy step-wise selection methods commonly used to build regression models are bad.\n\nhttps://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df\n\n\n\n\nLinear regression can also be used to look at interactions between different variables (e.g. how the relationship between smoking status and systolic blood pressure changes based on sex), and to model non-linear relationships (e.g. the relationship between age and blood pressure, which is typically fairly flat at young ages and then increases through middle age before flattening out). In practice most processes/phenomena interact and are non-linear to a greater or less extent, but it is often a reasonable simplifying assumption to treat variables as independent (non-interacting) and relationships as linear, unless the interactions and/or non-linear relationships are clearly strong or a key part of your research question. Therefore, as these issues are complicated and often not required when analysing maybe most data simple datasets we will not look at or practice the techniques needed to model interactions and non-linear relationships.\nNote: the “linear” part of linear regression refers to the fact that in the linear regression model the coefficients or parameters are “linear”, i.e. the model is just a simple sum of those coefficients:\n\ny = α + β1X1 + β2X2 + … + βnXn\n\nWhere y is the model predicted outcome value, α is the intercept, the βs are the independent variable coefficients and the Xs are the independent variable values.\n\n\n\n\nWe wish to understand all the main likely causes of variation in individuals’ systolic blood pressure using the data collected in the SBP final dataset. We’ll use a linear regression model/analysis to look at this question. Refer to the “The datasets” section within the “Introduction to SPSS” section for full details if you need a full reminder, but briefly this dataset contains data on 556 individuals who were (hypothetically speaking!) sampled in a cross-sectional survey, and had data collected on their systolic blood pressure plus certain socio-demographic (age, sex, socio-economic status) and health characteristics (BMI, salt intake and ACE inhibitor usage).\nAs discussed above we will assume, based on existing theory/evidence and plausible, critical thinking, that all our measured independent variables are all likely causes of variation in systolic blood pressure and collectively simultaneously act as a sufficient adjustment set when interpreting each independent variable’s relationship with the outcome causally. That is, we will just create one linear regression model containing all the independent variables and assume that this model represents the same sufficient adjustment set for each independent variable (i.e. when interpreting the effect of each independent variable causally we will assume all other variables in the model are confounders of that relationship that require adjusting for to obtain an unbiased estimate of the focal causal effect). In reality if we were doing this we should start by deciding which focal relationships of interest we were interested in, which may well not be all of our collected independent variables, and for each focal relationship of interest we should define a sufficient adjustment set which may well differ from the sufficient adjustment set of other focal relationships of interest. This would typically be a big and time consuming process requiring a lot of careful critical thought and research/subject matter knowledge.\n\nLoad the “SBP data final.sav” dataset.\n\n\n\n\nVideo instructions: explore the data for a linear regression\nWritten instructions: explore the data for a linear regression\n\n\nRead/hide\n\nAlthough we are not taking a data-driven approach to model building, which often involves extensive exploration of patterns in your dataset to largely/wholly determine your model building along with automated model selection processes, it is always advisable to conduct some focused exploration of your dataset to understand the data and the guide certain decisions in your model building process. Note: this would follow your data preparation stage, where you would already have a good sense of the key characteristics of each variable, such as the type of data it contains, the range of values present etc.\nSpecifically, although you should not typically use your dataset to guide which independent variables are included in your model (and as discussed earlier for any given focal relationship that should be based on theory and plausible hypotheses about what constitutes a sufficient adjustment set), most researchers would recommend thoroughly exploring you data in terms of: 1) the distribution of your outcome and independent variables, to ensure you understand them well and are taking a suitable modelling approach (e.g. linear regression rather than another type of regression), and 2) what functional form of model makes good sense given the relationships between your independent and outcome variables, primarily whether there are any clear/strong non-linear relationships or interactions, although we will not look at exploring interactions in this practical as they are beyond the scope of this course.\n\n\nTo understand the distribution of your variables you can use histograms for numerical variables and bar charts for categorical variables. Let’s run a histogram for our outcome variable.\n\nFrom the main menu go: Graphs > Legacy Dialogues > Histograms. Add sbp into the Variable: box, tick the Display normal curve box and click OK. What do you see?\n\nWhat does the distribution of the outcome look like?\n\n\nRead/hide\n\nThere appears to be a slightly odd “gap” near the mean, but overall the variable appears to pretty closely follow a normal distribution.\n\nNext let’s look at a bar chart for our ses variable.\n\nFrom the main menu go: Graphs > Legacy Dialogues > Bar. Then click the Simple option and Define. Add ses to the Category axis: box, tick the % of cases option and click OK. What do you see?\n\nWhat does the distribution of the socio-economic status variable look like?\n\n\nRead/hide\n\nMost participants were of low socio-economic status, with successively smaller proportions being of medium and high socio-economic status.\n\nYou can use these two types of graphs to explore the distribution of all the variables. In a real analysis you would certainly want to do this, but for the sake of time you may want to move on now that you know how to do this.\n\n\n\n\n\nFirst let’s look at relationships between the outcome and numerical variables (i.e. bivariate relationships) to understand whether it’s reasonable to assume simple linear relationships for your numerical independent variables or whether any need to be modelled via the addition of non-linear terms to the model. We’ll just look at age and we’ll use a scatterplot.\n\nFrom the main menu go: Graphs > Legacy Dialogues > Scatter/Dot. Then select the Simple option and click Define. Add the sbp variable into the Y Axis: box and age into the X Axis: box then click OK. What do you see?\n\nWhat does the relationship between age and systolic blood pressure look like?\n\n\nRead/hide\n\nIt’s hard to say much other than there’s no clearly strong linear or non-linear relationship. However, this is a good example of why you can’t always interpret bivariate graphs very easily, because when there is a lot of variation in the data it can easily hide weaker relationships, because as we’ll see later there is evidence of a relationship here.\n\nWhat if we had seen a clear non-linear relationship? There are two main options within a regression modelling framework:\n\nConvert your numerical independent variable into a categorical variable.\nInclude additional “polynomial” terms of the relevant independent variable. This just means that as well as including, say, age in the model you include age² or possibly higher-order terms as well.\n\nOption 1 is often the best choice because although it might not model the relationship as well as option 2 it provides results that are easier to interpret. If you ever need to do this as always you should think carefully and critically about what cut-points to use when converting your numerical variable to a categorical variable. There aren’t necessarily clear “rules” about this, but within the framework we’ve discussed it would be most consistent to choose cut-points based on theory rather than driven by what the sample data suggest are key cut-points.\n\n\n\nNow let’s look at the relationship between categorical variables and the numerical outcome. To do this we can use bar charts or boxplots. Boxplots are probably better as they provide more information within the plot, although you can present much of the same information on a barchart if you know how using statistical software like R. Why would we want to do this, given you cannot have a non-linear relationship between a categorical variable and a numerical outcome? The main reason would be to inform us whether any levels within categorical variables might be suitable for merging/pooling together if necessary, and to understand whether the variance within category levels is approximately equal, which is a key assumption of linear regression.\n\nLook at the figure below to understand how to interpret boxplots:\n\n\n\n\nBoxplot explained\n\n\nLet’s look at the relationship between the outcome and the categorical variable sex with a boxplot:\n\nFrom the main menu go: Graphs > Legacy Dialogues > Boxplot, then select Simple and click Define. Add the sbp variable into the Variable: box and sex into the Category Axis: box and click OK. What do you see?\n\nWhat does the relationship between sex and systolic blood pressure look like?\n\n\nRead/hide\n\nIn our sample males clearly tend to have moderately higher values of SBP than females on average, but both sexes have a broadly similar spread (i.e. approximately equal variance) of SBP values. We can also see that systolic blood pressure values in both groups appear to follow a fairly normal distribution, which you can tell this by the fact that the box plots are quite symmetrical, unlike the explanatory boxplot image above where the data would clearly be right skewed due to the larger spread of higher values. Asymmetry indicates a non-normal, skewed distribution with a higher spread of observations indicating right-skew and a lower spread of observations indicating left skew. You can always plot a histogram of the outcome within each group to understand this more clearly.\n\nAgain, you can now use these two types of plots to explore the relationships between all the other independent variables and the outcome if you wish, but there’s no real need to do this for this practical so feel free to move on."
  },
  {
    "objectID": "14-linear-regression.html#step-2-run-the-linear-regression-model",
    "href": "14-linear-regression.html#step-2-run-the-linear-regression-model",
    "title": "",
    "section": "Step 2: run the linear regression model",
    "text": "Step 2: run the linear regression model\nVideo instructions: run the linear regression model\nWritten instructions: run the linear regression model\n\n\nRead/hide\n\nRemember linear regression allows you to look at relationships between a numerical outcome variable and any number of numerical or categorical independent variables, assuming all the assumptions of the method are met (we’ll check these out shortly). So let’s see how we build, run and estimate our linear regression model in SPSS.\n\nFrom the main menu go: Analyze > General Linear Model > Univariate. Note: a univariate general linear model is essentially another name (less commonly used) for (multiple) linear regression, although confusingly SPSS also has various “regression” modelling tools as well that produce different linear regression model but with slightly different options available or (somewhat pointless) restrictions compared to this tool. One of the main benefit of the General Linear Model - Univariate tool is that you can add categorical variables without first converting them to dummy variables (see later for an explanation of what this means).\nNext in the Univariate tool we add our outcome variable sbp to the Dependent Variable: box. Then we add our independent variables. SPSS has separate boxes for numerical and categorical independent variables, so we add our numerical variables (bmi, salt and age) into the Covariate(s): box (you can highlight them all – hold shift while left-clicking – and add them together), and our categorical variables (sex, ses and ace) into the Fixed Factor(s): box (a factor is another term for a categorical variable). We can ignore the Random Factor(s): box and WLS Weight: box (see the help if you want to understand what they are for).\n\nNext we need to edit a number of options. Most importantly we need to specify the “functional form” of our model or how the model is “parameterised”. Essentially we can either add all variables as simple “main effects”, which means the independent linear effect of each variable is estimated, or we can add additional terms to create interactions between two or more variables, and/or we can add non-linear terms for one or more variables. However, as interactions and non-linear terms are beyond the scope of this course we will just assume all our variables have simple linear relationships with our outcome.\n\nTherefore, click on the Model tool button, then under Specify Model select the Custom option, then select all variables in the Factors & Covariates: box (to do this click on the top variable, then click and hold shift before clicking on the bottom most variable). Next (with all variables still selected) under the Specify Model section click the Build terms button, then under the Build Terms(s) Type: section click on the drop-down menu and change it to Main effects. This is where you could create interaction terms between two or more terms/variables in the model if you wished, but we’ll just stick with main effects. Therefore, now click the right facing arrow beneath this menu which will add all selected variables as main effects terms into the Model: box. Ensure that the Include intercept in model box is ticked and click OK.\nNext click the Save button and under Predicted Values tick the Unstandardized box, and then under Residuals tick the Unstandardized box, and then under Diagnostics tick the box for Cook’s distance. This tells SPSS to save the unstandardised predictions and residuals, and values for “Cook’s distance”. We will explain these later.\nLastly click the Options button, and then under Display tick the Parameter estimates box and click Continue and then OK. The output window will then pop up with the results, but first…"
  },
  {
    "objectID": "14-linear-regression.html#step-3-check-the-assumptions-of-linear-regression",
    "href": "14-linear-regression.html#step-3-check-the-assumptions-of-linear-regression",
    "title": "",
    "section": "Step 3: check the assumptions of linear regression",
    "text": "Step 3: check the assumptions of linear regression\nBefore we look at the results that appear in the output window we must first check whether we can treat the results as robust and valid. Our results are only valid if the assumptions of the linear regression model have been met/hold, i.e. if they have not been violated. Below we’ll go through the assumptions and how to check them, which is more complicated than for the simpler statistical tests we ran previously.\n1. Continuous outcome\nTheoretically the outcome variable must be continuous, but like with t-tests this can be relaxed and you can safely use linear regression for discrete outcome variables as long as the other assumptions hold.\n2. Independent observations\nTechnically this means that the residuals or model errors (variation not explained by the model) of one observation should not be correlated/related (be able to predict) to the residuals of other observations. As with the t-tests you should be able to understand from your study design whether you have independent observations or not. There are two main reasons for non-independent observations. 1) You have outcome measurements on your units of observation at more than one point in time (repeated measures) that are all included in the outcome variable. 2) You have outcome measurements on your units of observation that are nested within a larger cluster, such as patients within facilities, where patients from the same facility are going to be more similar and have correlated outcomes compared to patients from different facilities. We know our study design (simple random sampling of participants) ensures our observations are independent, so we don’t need to worry about this assumption further. What if your data are not independent? See below.\nDealing with non-independent observations in linear regression modelling\n\n\nRead/hide\n\nThere are sophisticated and powerful ways of dealing with problems of non-independence, but we don’t go into them here. However, a simple solution for non-independent observations due to having multiple measurements across time is to just use observations from one time point only as your outcome (if this makes sense), or to take the average across all time points (if this makes sense), or to take the difference between your first and last time points and use these change scores as your new outcome (if this makes sense). And a simple solution for having non-independent observations due to clustering is to calculate summary values of the outcome based on all observations within each cluster. For example, if your outcome is a numerical variable, such as SBP, and you are looking at patients within facilities, then you can calculate the mean SBP across all patients in each facility, and then use the facility-level mean SBP as your outcome. For binary categorical variables (e.g. hypertension – yes/no) you can select one level (e.g. hypertension = yes) and calculate the proportion or percentage of observations in that level per cluster. For categorical variables with >2 levels you have to create separate summary percentage variables for each level.\n\n3. Normally distributed residuals\nRemember that in linear regression the residuals or model errors are simply the differences between each observed outcome value and the model predicted value (based on the linear regression equation). Technically the assumption here is “multivariate normality of residuals or errors”. In practical terms this just means checking that the residuals are (approximately) normally distributed, which luckily is easy to do. Linear regression assumes normally distributed errors, and if this assumption is violated then the resulting confidence intervals and p-values for coefficients can be biased (usually too narrow and too small respectively).\nWhen we ran our linear regression using the General Linear Model – Univariate tool we told SPSS to calculate the “unstandardised” residuals for each observation and save them as a new variable, which SPSS will have called RES_1. To check whether the residuals are approximately normally distributed we could use a statistical test, but again this is sensitive to sample size and even if violated doesn’t necessarily mean our results won’t be robust, so it’s best to use a histogram.\n\nFrom the main menu go: Graphs > Legacy Dialogues > Histogram. Then add the RES_1 variable into the Variable: box, tick the Display normal curve box and click OK. What can you conclude?\n\nHow are the residuals distributed?\n\n\nRead/hide\n\nThe residuals appear to be reasonably approximately normal so we can safely assume this assumption has not been violated in our model. If the residuals were not approximately normally distributed then we can try to transform the outcome to increase the normality of their distribution. You can use the same methods as discussed and practiced in the “Inferential analysis 2: the independent t-test applied to a skewed outcome” practical, specifically see the “Step 2: transform the outcome” section.\n\n4. Linearity of the relationships between the residuals and all numerical variables\nAlthough you should have already confirmed the nature of the bivariate relationships between sbp and the numerical independent variables during your pre-modelling data exploration phase, once we are dealing with a multiple linear regression model it can be the case that once you adjust for a certain variable the adjusted relationship between another numerical independent variable and the outcome becomes non-linear. If this is the case the un-modelled non-linearity will be reflected in the residuals and cause bias in the inferential results (confidence intervals and p-values). Therefore, we must check whether the residuals show any non-linear patterns (curved trends) when plotted against the values of each independent numerical variable.\n\nWe will use a scatterplot. From the main menu: Graphs > Legacy Dialogues > Scatter/Dot. Then select Simple Scatter and click Define. Add the RES_1 variable into the Y Axis: box and the age variable into the X Axis: box and click OK. What do you see?\n\nWhat is the relationship between the residuals and age?\n\n\nRead/hide\n\nThere is no clear non-linear trend as the residuals are scattered fairly evenly and linearly across values of age.\n\nIn a real data analysis you should repeat this process for the other numerical independent variables in the model, but feel free to move on now if you wish because there are also no issues with the remaining variables. If you had found any non-linear relationships involving numerical independent variables then see the above “Bivariate exploration” section for a discussion of some possible solutions.\n5. Homoscedasticity: constant variance of the residuals across model predicted values\nThe technical name for this assumption is homoscedasticity, but in practical terms this just means that there should be no systematic pattern or change in the amount of variation (i.e. spread) in the residuals across the model predicted values (also called the model fitted values). If the residual variance changes with predicted values (most commonly increasing at higher predicted values) then this is known as heteroscedasticity, and this can again lead to confidence intervals and p-values for coefficients can be biased (usually too narrow and too small respectively). Again there are statistical tests available to “test” for this, but we will use graphical methods. Again this is very easy to check: we just plot the model predicted values against the model residuals and hope to see “random noise”.\nSee the following three figures for an illustration of some examples of clear heteroscedasticity and homoscedasticity:\n\n\n\nIllustrations of example heteroscedasticity and homoscedasticity residual patterns\n\n\nNote: these are in an idealised/very clear form. You will often see some tapering of points at either end of the main spread of points because there is usually less data at these places, but this is not usually something to worry about.\nLet’s now produce the necessary plot and check for violation of this assumption:\n\nFrom the main menu go: Graphs > Legacy > Scatter/Dot, then select Simple Scatter and click Define. Add the RES_1 variable into the Y Axis: box and the PRE_1 variable into the X Axis: box and click OK. Note: you can also get SPSS to make this graph via the Options menu in the General Linear Model Univariate tool, but it is a lot smaller and harder to see when made that way. What can we conclude?\n\nIs the homoscedasticity assumption violated or not?\n\n\nRead/hide\n\nThe spread of the residuals looks fairly even across all values of the model predictions for the outcome, so we can safely assume this assumption has not been violated.\n\nWhat if there had been a clear pattern in the residuals when plotted against the model predicted values?\nHow to deal with heteroscedasticity\n\n\nRead/hide\n\nThe first thing to do would be to try and find out why this was. This is usually either caused by 1) having an outcome that covers a very large range of values, because typically the variance will be greater at larger values, even if the model is correctly specified, 2) having an outcome that varies more at higher/lower values of a numerical independent variable and/or varies differently between one or more categorical variable levels for whatever (genuine/real world) reason, or 3) having an incorrectly specified model, which means your model is either missing important and necessary non-linear and/or interaction terms, or is missing one or more independent variables that are important in explaining variation in the outcome.\nIdentifying scenario 2) and 3) involves exploring the relationship between the residuals and all independent variables in turn to try and identify the culprits, and careful thinking about whether any key independent variables may be missing from your sufficient adjustment set. While identifying scenario 1) involves ruling out scenario 2) and 3) and having an outcome with a large range of values (typically spanning a number of orders of magnitude).\nHow to solve this? If the problem appears to be due to scenario 3) you may be able to identify the culprit missing variable or missing functional form and update the model to solve the problem. If the problem involves scenario 1) or 2) then a simple and often sufficient solution is to use a linear regression model with “robust standard errors”, also known as “heteroskedasticity-consistent standard errors” or “Huber-White (robust) standard errors” (after the two inventors of the method). While we will not discuss or practice this solution further you can get an overview of how to run the method in SPSS here:\nhttps://www.ibm.com/support/pages/can-i-compute-robust-standard-errors-spss"
  },
  {
    "objectID": "14-linear-regression.html#step-4-consider-additional-possible-issues",
    "href": "14-linear-regression.html#step-4-consider-additional-possible-issues",
    "title": "",
    "section": "Step 4: consider additional possible issues",
    "text": "Step 4: consider additional possible issues\nThere are two additional common problems with linear regression models that, while not violations of any assumptions, can cause serious problems in obtaining valid/unbiased results.\n1. No serious multicollinearity\nMulticollinearity refers to the situation when one or more independent variables in your linear regression model are correlated with each other. If the correlation is weak, as exists between most independent variables, it’s not a problem, but when the correlation becomes very high this can cause problems with model estimation potentially giving you worthless results. We can test whether there is any substantial multicollinearity between our independent variables using the variance inflation factor (VIF) measure, which measures how much each variable inflates the estimates of its variance (standard error) due to its collinearity with one or more other independent variables. As a rough rule of thumb VIF values >10 are likely to be problematic for your results are need attention.\nTo get SPSS to estimate the VIF values for our independent variables we must re-run our model using the Regression tool. Unfortunately this tool does not allow us to include our categorical variables in their current form, and they must be manually converted into the necessary separate “dummy coded” or “indicator” variables for each level of each categorical variable (the General Linear Model - Univariate tool does this automatically “behind the scenes”).\nWhat are dummy variables? To allow linear regression to model the effects of categorical variables we can use dummy variables. In the most common and simple coding scheme (see here for more on other coding schemes: https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis-2/) we choose one level to be the reference against which the effects of all other levels are compared. Very briefly this level is then “omitted” and the mean of the linear regression model (when all other variables are set to 0) is the mean for the reference level. The remaining levels then have a dummy variable created for them, where the dummy variable takes the value of 1 if the observation is from that level or a value of 0 if the observation is from a different level. Dummy variables therefore act like “switches” that turn on or turn off the effect of each level when appropriate. Therefore, we must create dummy variables ourselves. To save time I have already done this.\n\nTherefore, to calculate the VIF values in the main menu go: Analyze > Regression > Linear. Then add sbp to the Dependent: variable box, and age, salt, bmi, sex_f_dummy, ses_m_dummy, ses_h_dummy and ace_dummy to the Independent(s): box (there is no dummy for sex = male or ses = low as these are taken to be the reference levels, i.e. the mean expected/predicted value of SBP when all independent variables have the value 0). Then click the Statistics button and tick the Collinearity diagnostics box. Click Continue then OK.\n\nIn the output window look for the “Coefficients” table. On the right hand side of this table you will see the two columns under “Collinearity Statistics” are labelled “Tolerance” and “VIF”. VIF is just 1/tolerance. Based on the rough rule of thumb what can we conclude?\nWhat can we conclude about multicollinearity in our model?\n\n\nRead/hide\n\nThere are no concerning signs of excessive multicollinearity for any independent variables. Great!\n\nWhat if you do find excessive multicollinearity in your model?\nHow to deal with excessive multicollinearity\n\n\nRead/hide\n\nWhat if one or more variables had very high VIF scores? Before doing anything we can actually ignore high VIF scores under some circumstances. 1) When they are only for “control” variables, i.e. variables whose effects we are not interested in interpreting but where we believe they need to be in our model because of our theory about the likely causal influences on our outcome. 2) Where the VIFs are for interaction or polynomial terms, because this is to be expected. 3) The variables with high VIF values are dummy variables for a categorical variable with three or more levels. When the reference level has an increasingly small proportion of the observations in it then the remaining levels will have increasingly large VIFs. You can avoid this by setting the reference category as a larger level. However, if the variable(s) with large VIF values aren’t covered by these situations then you need to consider why they have a high VIF. It means they are highly correlated with one or more other independent variables, which usually means they are essentially measuring the same thing. For example, weight and waist circumference will often be highly correlated. Therefore, in such a situation you can simply remove whichever one of the variables makes most sense for your research question.\n\n2. No extremely influential observations\nSometimes single or multiple observations (individuals in our “SBP data final.sav” dataset) can have an excessive influence on the results, i.e. the coefficients and/or confidence intervals and p-values of your linear regression model, especially in smaller datasets. What this means is that including or excluding these observations from the analysis can change the results and potentially your overall interpretation of the results substantially. This is obviously not a good situation when the results are so sensitive to one or a few observations. If such observations exist in your model then you need to explore them and try and understand why they are so influential and whether they should be retained in the model. Observations can be influential in two main ways:\n\nOutliers: observations can have a large residual value, i.e. an outcome value that is unusually larger or small given its independent variable values.\nLeverage: observations can have values for one or more independent variables that are unusually large or small compared to all other values for that independent variable.\n\nSeparately or together these two processes can give rise to observations that are highly influential. There are quite a few ways to explore these issues, but for time and simplicity we will just look at one widely used approach: the Cook’s distance (D) statistic. Without going into details for each observation in a dataset Cook’s D measures “how much” the values in a regression model change when that observation is removed from the model. Cook’s D starts at 0 with higher values indicating more influential observations. Various rules of thumb have been proposed for judging when a value of Cook’s D indicates that the observation should be looked at, but these may fail, and it is simpler and probably more robust to just judge (based on the type of graph we will produce below) whether any observations have a value of Cook’s D that is relatively much greater than all the other Cook’s D values. We already told SPSS to calculate Cook’s D as a new variable in the General Linear Model Univariate tool when we selected the Cook’s distance option in the Save options.\nThe easiest way to explore which observations appear to have excessively large values for Cook’s D is to create a scatterplot of Cook’s D against the observation ID variable (which is just a simple count from the first to the last observation).\n\nRemember in the main menu we go: Graphs > Legacy Dialogues > Scatter/Dot. Then select the Simple Scatter option and click Define. Then add the Cook’s D variable COO_1 to the Y Axis: box and the id variable to the X Axis: box and click OK.\nRemember you can interact with an SPSS graph by double clicking on it. We can then click twice on an observation to highlight it alone with a yellow circle, which allows you to then right click and select “Go to Case” to see that observation in the Data View. What do you see on the graph?\n\nInterpreting Cook’s D values\n\n\nRead/hide\n\nTwo observations appear to have clearly relatively much higher values for Cook’s D than all the other observations. Interacting with the graph we can explore these observations, which have ids of 478 and 520.\n\nWhat do you notice about the outcome and/or independent variable values for these observations?\nWhat do you notice about these influential values?\n\n\nRead/hide\n\nObservation id 520 has a very large value for BMI of 41, but otherwise appears normal, so this is likely driving its influential power. While observation 478 has a very large value for the outcome, with a SBP of 175, but otherwise has fairly “normal” values for its independent variables although they are of high socio-economic status and taking an ACE inhibitor, both of which (as we’ll see) are associated with lower values of SBP in the sample, so this is likely driving its influential power.\n\nWhat should we do? Generally unless you can be certain that an observation is influential due to an error in the outcome or an independent variable then you should not make any changes to these values nor should you exclude the observation from your primary analysis. However, a simple, transparent and widely recommended approach is to conduct a sensitivity analysis by removing such observations from the dataset (i.e. create a copy of the dataset and then delete them entirely) and re-running your analysis to see if the results change substantially. If the results do not change in any important way then you should report this lack of change following the removal of the observations, but include the sensitivity analysis results in an appendix etc so readers can verify the truth of this. If the results do change substantially then it makes most sense to report both sets of results in the main paper and interpret accordingly, i.e. be clear that the conclusions change depending on whether such extreme observations are included or not. Either way you must be transparent and open about their effects."
  },
  {
    "objectID": "14-linear-regression.html#step-5-understand-the-results-tables-and-extract-the-key-results",
    "href": "14-linear-regression.html#step-5-understand-the-results-tables-and-extract-the-key-results",
    "title": "",
    "section": "Step 5: understand the results tables and extract the key results",
    "text": "Step 5: understand the results tables and extract the key results\nSo now that we have verified that our results are robust, and the assumptions of the model hold, we can finally interpret our results. This is the really interesting and exciting part of any analysis! As you’ll have filled the output window with lots of graphs from the assumptions checking you may wish to re-run the linear regression again. Either way in the output window the results are presented in three tables.\nThe first table “Between-Subjects Factors” isn’t that useful (assuming we understand our data well), and just shows the number of observations in each level of each categorical variable. The second table “Tests of Between-Subjects Effects” shows an “ANOVA” table. This can be used to understand the “statistical significance” of each term (but only the overall term for categorical variables, not each level) in relation to how much variation it accounts for in the outcome variable. However, this is arguably of little value when our final table “Parameter Estimates” (which SPSS doesn’t provide by default!) provides us with both an estimate of the “statistical significance” (i.e. the p-value) of all terms including categorical variable levels, but also much more usefully it gives us the linear regression coefficient (i.e. the estimated direction and size of the relationship) and its 95% confidence interval for every independent variable (or more specifically every term) in the model. Therefore, we will largely ignore the second table (apart from coming back to one piece of information it provides that should really just be in the “Parameter Estimates” table), and just focus on the “Parameter Estimates” table.\nSo what does it all mean?\n\n\nParameter estimates table columns explained\n\n\nParameter\n\nEach row indicates which term in the model the following results apply to. Terms are either the intercept or (in our case) main effects of independent variables, or with more complicated models they may include interaction terms and/or non-linear terms like polynomial terms. For numerical variables this means one row per variable. However, because each level of a categorical variable is actually treated as a separate “dummy variable” (coefficient) as standard in a linear regression model each categorical variable level has its own row.\n\nB\n\nB stands for “betas”, because in the linear regression model when represented mathematically the coefficients are usually represented by the Greek letter beta. The betas are more commonly referred to as the linear regression parameter estimates or coefficients. They tell us the best estimate (point estimate) of the direction (positive or negative) and size of the relationship between each parameter in the regression model and the outcome variable.\nFor all numerical independent variables they represent the expected mean change in the outcome variable for every 1-unit increase in the independent variable.\nFor categorical variables it’s a bit more complicated. There are many different ways of looking at categorical variable effects, but the most common is called dummy coding, and this is the default presentation in SPSS and most (probably all) stats software. With dummy coding one level in the categorical variable (e.g. female in the variable sex) is set as the reference level. Then the coefficients for the other level(s) represent the expected mean difference in the outcome variable between each level and the reference level (e.g. male compared to female). Unfortunately (for no obvious reason) SPSS’s univariate general linear model tool does not display value labels for categorical variables in the “Parameter Estimates” table and so all you see are the numerical codes (you’ll have to check the value labels in the Variable View if you can’t remember the value coding). By default SPSS sets the level with the highest value as the reference level. You will notice that the reference level always has a coefficient value of 0, with a superscript letter “a” linking to a footnote explaining that “This parameter is set to zero because it is redundant”, i.e. it’s the reference level.\nA note on the intercept. You may be wondering what the “Intercept” parameter represents? In a simple linear regression with just one independent variable this corresponds to the Y-intercept (hence the name), i.e. where the linear regression line crosses the y-axis (and the independent variable or x-value is 0). In a multiple linear regression this represents the expected/model predicted mean value of the outcome when all numerical independent variables have a value of 0 and all categorical variables are at their reference levels. Therefore the intercept will rarely have any useful interpretation (e.g. it assumes BMI = 0) and is usually ignored as a necessary but informative structural part of the model (unless you centre your variables, which we will not be looking at here).\n\nStd. Error\n\nThis is the standard error of the coefficient, which is an estimate of the sampling variability of the coefficient in the target population. This is used when calculating the 95% confidence intervals and p-value, but you can ignore these and just interpret the 95% confidence intervals and (if you wish) p-values.\n\nt\n\nThis is the t-statistic for the coefficient and is used when calculating the confidence intervals and the p-value associated with the coefficient. You can calculate 95% confidence intervals and p-values assuming normally distributed data, but using the t-distribution is more conservative (safe) for small sample sizes and is equal to assuming normally distributed data at large sample sizes. You can ignore these and just interpret the 95% confidence intervals and (if you wish) p-values.\n\nSig.\n\nThis is the two-tailed (although now it doesn’t mention that explicitly!) p-value associated with each coefficient. Again, assuming the “true” value of the coefficient in the target population is 0, the p-value represents the probability of observing a coefficient at least as great as that observed (positively or negatively as it’s a two-tailed p-value) due to sampling error alone.\n\n95% Confidence Interval (Lower Bound and Upper Bound)\n\nThese are the lower and upper 95% confidence intervals for the coefficient based on the t-distribution. As usualy, loosely speaking they represent a range of values that we can be reasonably confident contain the “true” coefficient that exists in the target population.\n\n\n\nThen lastly if we go back to the “Tests of Between-Subjects Effects” table and look at the footnotes we see one footnote: “a. R Squared = .594 (Adjusted R Squared = .588)”. In a linear regression model the R² value represents the proportion (or % if you multiply it by 100) of variation in the outcome variable that is explained by the model, i.e. by all the terms of variables in the model. However, whenever you add a term to a model the R² value will increase even if the term is a random number variable, and has no true explanatory power for the outcome. Therefore, it is better to use the adjusted R2 value which makes a correction for the number of variables in the model. Note, R² values in health sciences are rarely as high as the one seen here, which is due to the artificial nature of the data.\nTherefore, typically we are just interested in the key descriptive statistics about the sample (number of observations and missing values, which are more easily obtained via separate descriptive analysis of the dataset), and in terms of the inferential results we want the parameter or coefficient estimates, their associated 95% confidence intervals (and possible their associated p-values), and often also the R² value of the model."
  },
  {
    "objectID": "14-linear-regression.html#step-6-report-and-interpret-the-results",
    "href": "14-linear-regression.html#step-6-report-and-interpret-the-results",
    "title": "",
    "section": "Step 6: report and interpret the results",
    "text": "Step 6: report and interpret the results\n\nNumerical variables\nFirst we’ll look at how to interpret the results relating to the numerical independent variables and their relationship with the outcome statistically and then how to interpret that result in practical terms (sometimes referred to as the results’ statistical and practical significance).\nStatistical interpretation\n\nFor numerical independent variables linear regression coefficients represent the model-predicted mean (or more loosely the average) change in the outcome (i.e. in units of the outcome) for every 1-unit increase in the independent variable’s units, while holding the effect of all other independent variables constant, i.e. they measure the mean independent relationship. Note: this change does not depend on the value of the independent variable, i.e. the same relationship is assumed to exist across the full range of values that the independent variable can take in the sample data, but it should not be considered to hold if you were considering values of the independent variable outside of the range of values seen in the sample data.\n\nLet’s take age as an example. Looking at the parameter estimates table the point estimate (i.e. the single best estimate) of the regression coefficient is 0.21. This means that, based on the model, the point estimate of the relationship between age and systolic blood pressure in the target population is that for every 1-unit increase in age, i.e. for every year older a participant is, the model predicts that an individual’s systolic blood pressure will increase by a mean of 0.21 mmHg, whilst holding the effect of all other independent variables constant (i.e. this estimated relationship is independent of the effect of all other independent variables in the model).\nHowever, how sure can we be about the true direction and size of the regression coefficient (i.e. the relationship of interest) in the target population given our sample size and the sampling error in the sample? This is what our confidence intervals help us to estimate. Remember, formally they provide a range of values that, hypothetically speaking, if we were to repeat our study and analysis many times (technically an infinite number of times), would contain the true value of the regression coefficient in the target population X% of the time, where the true value of the regression coefficient would be the value of the regression coefficient that we would get if we measured every individual in our target population and ran the model, and X% is the confidence level (typically 95%). Informally and more loosely speaking a 95% confidence interval around a regression coefficient gives us a range of values that we can view as likely including the true value of the regression coefficient that exists in the target population (but we can’t say within that range which values are more/less likely).\nTherefore, in the parameter estimates table we can see that the 95% confidence intervals for the regression coefficient for age are 0.03 and 0.39. Consequently, we can be reasonably confident that the true value of the regression coefficient in the target population is between 0.03 and 0.39. And so because the 95% confidence intervals are fairly narrow we can conclude that we have obtained a reasonably accurate estimate of the likely relationship between age and systolic blood pressure in the target population. However, as always this is assuming there is no bias in the results, which in a real study is very unlikely, and therefore in a real study we must consider all likely sources of bias and their likely impacts when assessing the inferential results!\nPractical importance\nNow we know how to interpret the result statistically how do we interpret its real-world practical importance? For example, what can we conclude about its importance clinically and for public health programmes? These are complicated questions that have no simple answers and different people will have differing views depending on their views of the evidence (result) and the wider context. Most importantly you need to think carefully and critically and have strong subject matter knowledge to make robust interpretations and suggestions/recommendations. However, we can give some guidance about things to consider. You should clearly consider whether individuals or other units of observation can have their values of the independent variable of interest moved or not, and what this implies for what clinical practice and public health programmes etc can or cannot do about that the characteristic represented by that independent variable. For example, an individual cannot alter their height, while their age changes but out of their control, but their weight can be affected by themselves or outside influences (e.g. interventions).\nFor numerical independent variables we must then also consider what range of values most individuals (or units of observation) take in the study/sample. This is because the impact of a 1-unit increase in an independent variable can add up if that variable can increase by many units. For example, our regression coefficient for age is 0.2 (95% CI: 0, 0.4) (rounding up for ease subsequent example calculations), and the age range in the study/sample was ~40 years. Therefore, across our age range, which most individuals will “experience”, based on the 95% confidence interval ranges for age, age is expected to account for a mean increase in individuals’ systolic blood pressure of between 0 x 40 = 0 mmHg and 0.4 x 40 = 16 mmHg. Note: unless strongly justifiable you should not interpret your results outside of the range of the independent variables in your sample, e.g. you should not try and interpret the age relationship as holding for individuals aged between 70 and 80 as no such individuals were in the study.\nTherefore, it might be reasonable to say that while every extra year makes little difference to most individuals’ systolic blood pressure, across forty years of their life it may result in a clinically important increase, which might therefore be of importance when considering relevant clinical guidelines and public health programmes. However, as noted earlier practical interpretations are complicated and you should consider all relevant contextual issues. For example, an increase of 16 mmHg or more systolic blood pressure may not make any clinical difference for a health individual, but a smaller increase might be very clinically important for someone with cardiovascular disease.\n\n\nCategorical variables\nStatistical interpretation\n\nWith the standard dummy coding of categorical variables (https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis-2/) linear regression coefficients represent the model-predicted mean (or more loosely the average) difference in the outcome (i.e. in units of the outcome) between the categorical level (or group) of interest and the reference or comparison level (or group), while holding the effect of all other independent variables constant, i.e. they measure the mean independent relationship. The choice of which categorical level is set as the reference group is up to you.\n\nLet’s take socio-economic status as an example. Looking at the parameter estimates table as this is a categorical variable there are separate rows for each of the levels of ses. First we can see that the highest value of ses (3), which codes for high socio-economic status, has been set as the reference group and has a regression coefficient of 0 (“ses=3” has a B or regression coefficient of 0). This means that the regression coefficients for the other socio-economic status groups represent the difference in the outcome in relation to/compared to the high status group.\nLook at the coefficients for “ses=1” and “ses=2”. Here we can see that participants with a low socio-economic status (ses=1) have a point estimate regression coefficient of 7.91. This means that, based on the model, the point estimate of the mean difference (or relationship) in systolic blood pressure between individuals in the low socio-economic status group compared to individuals in the high socio-economic status group, in the target population, is 7.91 mmHg, whilst holding the effect of all other independent variables constant (i.e. this estimated relationship is independent of the effect of all other independent variables in the model). To clarify, this means that the model estimates that in the target population individuals in the low socio-economic status group have systolic blood pressure values that are on average 7.91 mmHg higher than individuals in the high socio-economic status group, while holding the effect of all other independent variables constant. We would interpret the result for the medium socio-economic status group similarly.\nThen as with the numerical independent variables we of course need to look at the 95% confidence intervals to judge the likely direction and size of relationship in the target population, given our sample size and sampling error (i.e. our point estimate is not necessarily very precise). From the parameter estimates table we can see that the 95% confidence intervals for ses=1 are 4.85 and 10.97. Therefore, we appear to have a reasonably precise/clear estimate of the effect of low socio-economic status compared to high socio-economic status, and it appears that being of low socio-economic status compared to high socio-economic status is independently associated with slightly higher systolic blood pressures on average.\nPractical importance\nAs with numerical independent variables interpreting the practical or real-world importance is complicated and requires careful and critical thought and strong subject matter knowledge. Similar to interpreting the practical importance of numerical independent variable results you should consider things like the relative proportions of individuals or other units of observation in your different categorical variable levels, and how plausible it is that individuals or other units of observation can move or be moved between category levels, say via interventions. Again you should also not interpret the results for category levels not in your sample, or for comparisons between category levels that you did not formally make.\n\n\n“Non-significant” results\nNote: you should present and discuss all results, i.e. “non-significant” results too, which may have equal if not more importance in relation to your research question. However, take care. A common mistake in interpreting inferential results is to treat a non-significant result as indicating “no difference” or “no relationship”, when actually the correct interpretation is that it provides no clear evidence of a difference or a relationship, but there may well be a real difference or a relationship in the target population but it’s just that it may be too small for you to have had much chance of detecting it with your dataset. In terms of whether a difference/relationship exists in the target population, there are actually typically three possibilities when you find a non-significant difference/relationship in your sample dataset:\n1/. There is really no (or negligible) difference/relationship in the target population.\n2/. There is a difference/relationship in the target population but it is too small for you to have any real chance of detecting it with your sample size (i.e. you lacked statistical power), no matter how many times you repeated the study.\n3/. There is a difference/relationship in the target population and given its size and your sample size you had a good (e.g. >80%) chance of detecting it, but due to sampling error or other sources of bias you did not find any evidence of it in your single dataset. If you repeated the study lots of times though you’d find statistically significant evidence for it more often than not.\nThere are analysis methods (e.g. “power calculations”) to help you get an idea about which of these possibilities is most likely, but as they are beyond the scope of this introductory course and actually not commonly used we will not look at them further.\n\n\nRegression tables\nWhen reporting the results from a linear regression model it is common to use a table to present the results, given there are typically a large number of results of interest. The following tips/recommended best practices may be useful when creating regression results tables:\n\nSeparate 95% CI lower and upper values with a comma, not a dash (-), otherwise negative values can be ambiguous or hard to read.\nYou can refer to the reference level in other ways, but the above is a simple and clear approach.\nP-values <0.001 are usually referred to as such without further decimal place values.\nAlways include all units for numerical variables.\nAlways ensure the outcome variable is clearly mentioned (here in the title) along with the units it’s measured in.\nInclude the adjusted R² value.\n\nIn the exercise below you can fill in a regression results table “template” that incorporates these features, so you can then use this template/format to guide your future presentation of regression results.\n\n\nMethods\nAs usual in a methods section you should clearly explain why you used a linear regression analysis and exactly what you did, including how all the variables were coded/what units they were in, if you modified any variables, how you dealt with any missing data etc."
  },
  {
    "objectID": "14-linear-regression.html#exercise-multiple-linear-regression",
    "href": "14-linear-regression.html#exercise-multiple-linear-regression",
    "title": "",
    "section": "Exercise: multiple linear regression",
    "text": "Exercise: multiple linear regression\n\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Multiple linear regression: the causes of variation in systolic blood pressure. Then follow the instructions in the exercise.\n\nExample completed linear regression results table\n\n\nRead/hide\n\n\n\n\nLinear regression results table\n\n\n\nExample text interpreting the relationship between BMI and systolic blood pressure in terms of its statistical and practical importance\n\n\nRead/hide\n\nThere was a clear and precise, statistically significant relationship between BMI (kg/m²) and systolic blood pressure (mmHg) in the linear regression model, indicating that for every 1-unit increase in BMI there is a mean increase in systolic blood pressure of 4.25 mmHg (95% CI: 3.89, 4.6) independent of all other independent variables. Given that the range of BMI in the study was 23, the relatively narrow confidence intervals for this relationship indicate that variation in BMI has the potential to have a very substantial impact on variation in systolic blood pressure in the target population, and given BMI is potentially variable by individuals and interventions this has clearly important clinical and practical implications. For example, this result suggests an increase in BMI of just 10 could potentially lead to an increase of between 39 mmHg (3.9 x 10) and 46 mmHg (4.6 x 10) in an individual’s systolic blood pressure on average, implying this should be considered when developing treatment guidelines and public health programmes.\n\nExample text interpreting the relationship between sex and systolic blood pressure in terms of its statistical and practical importance\n\n\nRead/hide\n\nThere was a clear and precise, statistically significant relationship between sex (male/female) and systolic blood pressure (mmHg) in the linear regression model, indicating that compared to females males had systolic blood pressure values that were on average 10.03 mmHg (95% CI: 7.77, 12.3) higher, independent of all other independent variables. Therefore, considering that the mean difference is likely to be between 7.77 mmHg and 12.3 mmHg this appears to be a potentially important clinical difference that should be considered when developing treatment guidelines and public health programmes (for example, by educating males on their higher risk of having higher systolic blood pressure).\n\nExample text discussing the key limitations of the study and the issues that must be considered when critically interpreting the results\n\n\nRead/hide\n\nCare must be taken when interpreting the results because the study was an observational cross-sectional study with therefore very limited ability to make robust causal inferences, and because the validity and accuracy of the results depends on how well the study data and model have accurately captured the causal relationships of interest and all important confounding variables of those causal relationships, and on how much bias there was in the study."
  },
  {
    "objectID": "14-linear-regression.html#next-steps-optional",
    "href": "14-linear-regression.html#next-steps-optional",
    "title": "",
    "section": "Next steps (optional)",
    "text": "Next steps (optional)\n\nIf you have time why not run the sensitivity analysis for the extremely influential observations? Simply remove them from the dataset (go to the Data View and right click on their observation number on the left of the screen to highlight the relevant row, then select “Clear”), re-run the model and compare the results to those above.\nConfirm to yourself that an independent t-test is equivalent to a simple linear regression with one binary independent variable: using the “SBP final data.sav” dataset run an independent t-test analysing the difference/relationship between sex and systolic blood pressure, and then run a linear regression with systolic blood pressure as the outcome and sex as the sole independent variable. You should see that the coefficient estimate from the linear regression matches the mean difference estimate from the t-test, and the 95% confidence intervals and p-values match as well. Underlying the independent t-test is a model that is equivalent to a simple linear regression!"
  },
  {
    "objectID": "15-logistic-regression.html",
    "href": "15-logistic-regression.html",
    "title": "",
    "section": "",
    "text": "Like multiple linear regression multiple binary logistic regression allows us to analyse the relationships between any number of continuous and/or categorical independent variables and an outcome, but unlike multiple linear regression the outcome in multiple binary logistic regression must be binary. Like with linear regression “simple logistic regression” may be a term used to refer to logistic regression with one independent variable only, while “multiple linear regression” refers to including more than one independent variable, but there’s no qualitative difference, and we’ll just refer to “logistic regression” from here on. Also, as touched on in the lecture there are different versions or extension of binary logistic regression like multinomial logistic regression for categorical outcomes with any number of category levels, but as these are more advanced we will not be looking at them further. Therefore, because “binary logistic regression” is far more commonly used/seen than any other form of logistic regression we’ll just refer to it as logistic regression from here on, which is common practice (i.e. if you see “logistic regression” in the literature you can assume it’s referring to binary logistic regression).\nYou can actually analyse binary outcomes using linear regression, and you will sometimes see such analyses in the literature (sometimes called a “linear probability model”) and you can often get reasonably useful (i.e. reasonably accurate/unbiased) results. However, generally this is not recommended because of the often substantial disadvantages and problems with this approach. For example, the model predictions may range outside 0-1 (the only values your outcome can take in reality when viewed as a probability of the event occurring), and your inferences are likely to be biased because the assumptions of linear regression cannot be met in such a situation, producing biased confidence intervals and p-values. Although logistic regression is more complex mathematically the basic idea of creating a linear model (where the terms are additive) with numerical or categorical independent variables is the same, although the interpretation of the terms differs substantially.\n\n\n\nWe will use almost exactly the same scenario as for the linear regression practical, using the SBP final dataset again. However, now we wish to understand the likely causes of variation in individuals’ hypertension status (rather than their systolic blood pressure), or more generally the likelihood of having hypertension, where we define hypertension as having a systolic blood pressure ≥140 mmHg, using the data collected in the SBP dataset. Refer to the “The datasets” section within the “Introduction to SPSS” section for full details if you need a full reminder, but briefly this dataset contains data on 556 individuals who were (hypothetically speaking!) sampled in a cross-sectional survey, and had data collected on their systolic blood pressure plus certain socio-demographic (age, sex, socio-economic status) and health characteristics (BMI, salt intake and ACE inhibitor usage).\n\n\n\nWe will use an almost identical modelling process to that used in the linear regression practical, loosely following a causal inference inspired approach, but in practice for simplicity building one model and interpreting all the independent variables causally. As discussed in the “Overview with a focus on causal inference from observational data” section in the “Inferential analysis 5: multiple regression” section, we will assume, based on existing theory/evidence and plausible, critical thinking, that all our measured independent variables are all likely causes of variation in individuals’ hypertension status, which collectively simultaneously act as a sufficient adjustment set when interpreting each independent variable’s relationship with the outcome causally. That is, we will just create one logistic regression model containing all the independent variables and assume that this model represents the same sufficient adjustment set for each independent variable (i.e. when interpreting the effect of each independent variable causally we will assume all other variables in the model are confounders of that relationship that require adjusting for to obtain an unbiased estimate of the focal causal effect). In reality if we were doing this we should start by deciding which focal relationships of interest we were interested in, which may well not be all of our collected independent variables, and for each focal relationship of interest we should define a sufficient adjustment set which may well differ from the sufficient adjustment set of other focal relationships of interest. This would typically be a big and time consuming process requiring a lot of careful critical thought and research/subject matter knowledge.\n\nLoad the “SBP data final.sav” dataset.\n\n\n\n\nVideo instructions: explore the data for a logistic regression\nWritten instructions: explore the data for a logistic regression\n\n\nRead/hide\n\nThe same basic reasoning applies to our data exploration process as with the linear regression modelling process, so refer back to the “Step 1: explore the data” section in the linear regression practical if you want a reminder of the theory/justification for the following data exploration process.\nIn practical terms though we would examine the distribution of each variable on its own using the same methods discussed in the linear modelling practical, but when examining the relationships between the outcome and each independent variable we would have to use different graphical approaches.\n\n\nSpecifically, as we have a binary outcome when exploring the relationship between numerical independent variables and our binary outcome we have to use a form of barchart, where we first create a “binned” or grouped version of our numerical independent variable (i.e. a new variable which takes a single value for a range of values of the original independent variable).\nLet’s see how to do this for bmi. First create the binned independent variable.\n\nFrom the main menu go: Transform > Visual Binning. Add bmi to the Variables to Bin: box and click Continue. At the top of the window that apppears look for the Name: fields and in the editable field called Binned Variable: give your new binned variable a name, e.g. bmi_bin. Then at the bottom right click on the Make Cutpoints… button. In the middle of the tool window click the Equal Percentiles Based on Scanned Cases option and in the Intervals - fill in either field are click on the Number of Cutpoints: box and enter a suitable number of cutpoints. It’s hard to know what is a suitable number but typically the more bins the better, as you can see relationships at a finer scale, but for more bins you need more data (sample size). For bmi let’s choose 15, but you can always remake this variable with more/fewer bins as needed. Therefore, enter 15 in this box and then click Apply at the bottom. Then click the Make Labels button just below the Make Cutpoints… button and then click OK. Click OK on the information box that appears.\n\nNow we can plot the mean of our binary outcome, which is equivalent to the proportion/percentage, for every bin we’ve just created, using a type of barchart, although we’ll use a “histogram” tool to create it.\n\nFrom the main menu go: Graphs > Chart Builder. If an information box appears saying “Before you use this dialogue…” just click OK. Now in the Gallery tab at the middle left click on Histogram and then double click on the leftmost picture of a graph (Simple Histogram) to add it to the plot. Next drag the htn variable from the Variables: box onto the Y-Axis? dotted box on the chart image. Similarly, drag the bmi_bin variable onto the X-Axis? box. Look to the right of the tool window at the Element Properties tab in the Statistics box. You should see a drop down menu under Statistic:. This should already read “Mean”, but if not click on it and select mean. This tells SPSS to calculate the mean of the Y-axis variable, htn, for each value of the X-axis variable, which are now repeated when in each bin. Now just click OK.\n\nWhat can you see?\nWhat does the barchart of BMI vs hypertension show?\n\n\nRead/hide\n\nThere appears to be a fairly linear increase in the proportion of individuals with hypertension as BMI values increase.\n\n\n\n\nFor exploring the relationship between categorical independent variables and binary outcomes we can just use a normal barchart where the Y-axis is the proportion of the outcome in each category level on the X-axis. Let’s see how to do this for sex:\n\nFrom the main menu go: Graphs > Chart Builder. If an information box appears saying “Before you use this dialogue…” just click OK. Now in the Gallery tab at the middle left click on Bar and then double click on the top leftmost picture of a barchart (Simple Bar) to add it to the plot. Next drag the htn variable from the Variables: box onto the Y-Axis? dotted box on the chart image. Similarly, drag the sex variable onto the X-Axis? box. Look to the right of the tool window at the Element Properties tab in the Statistics box. You should see a drop down menu under Statistic:. This should already read “Mean”, but if not click on it and select mean. This tells SPSS to calculate the mean of the Y-axis variable, htn, for each value of the X-axis variable, which are now repeated when in each bin. Now just click OK.\n\nWhat can you see?\nWhat does the barchart of sex vs hypertension show?\n\n\nRead/hide\n\nMen appear to have roughly twice the proportion of hypertension cases compared to women.\n\nIn a real analysis you would explore all bivariate relationships this way, but as with the linear regression practical feel free to move on once you are happy with the process.\n\n\n\nUnlike with linear regression we have one more data exploration process that we must do. When looking at relationships for a logistic regression model an important issue to look out for is so-called “sparse data”, which means we have small (or empty) “cell sizes”, which really means few (or no) observations of one of the levels of our binary outcome (e.g. either htn = yes or htn = no) in one or more levels of one or more categorical variables (e.g. if say all men were classed as htn = yes). This is explained in more detail during the “Additional potential problems” section below, but briefly if there are zero observations of one outcome level within one or more independent categorical variable levels that will cause the model to fail. If there are just a few you may see very large biased coefficients and confidence intervals. How few is few? There’s not set number, but <5 is often a cause for concern.\nWe can explore this issue using cross tabulations of the outcome variable against the categorical independent variables.\n\nGo: Analyze > Descriptive Statistics > Crosstabs. Add htn to the Row(s): box and sex, ses and ace to the Col(s): box then click OK.\n\nThis will give us three tables, one for each categorical variable. Look at each cross-tabulation cell to see the number of observations in that cell, i.e. each possible combination of the independent categorical variable’s level and the outcome’s level, which for sex vs htn would include 1) male-yes, 2) male-no, 3) female-yes, and 4) female-no. What can you see?\nWhat does the cross-tabulation show?\n\n\nRead/hide\n\nThere appear to be reasonable numbers of observations of both levels of the outcome for all levels of sex and ses, but ACE = yes has only three htn = yes observations. See the “Additional potential problems” for more discussion of the possible problems this might cause."
  },
  {
    "objectID": "15-logistic-regression.html#step-2-run-the-logistic-regression-model",
    "href": "15-logistic-regression.html#step-2-run-the-logistic-regression-model",
    "title": "",
    "section": "Step 2: run the logistic regression model",
    "text": "Step 2: run the logistic regression model\nVideo instructions: run the logistic regression model\nWritten instructions: run the logistic regression model\n\n\nRead/hide\n\nWe will use SPSS’s Generalized Linear Model tool to build our logistic regression model. The generalised linear modelling framework is a more comprehensive and coherent way of (mathematically) representing the full range of possible regression type models, of which linear regression and logistic regression are specific types. SPSS somewhat confusingly and redundantly also has a Binary Logistic tool under the Regression menu, which will also create a logistic regression model, but like the Linear (Regression) tool in the Regression menu it is also more restrictive because it won’t accept categorical variables without us first converting them to separate dummy variables, and it also does not offer all the options available in the Generalized Linear Model tool.\n\nFrom the menu go: Analyze > Generalized Linear Models. This will open the Generalized Linear Models tool which has multiple tabs along the top of the window.\nIn the first tab Type of Model look for the “Binary Response or Events/Trials Data” set of options and select Binary logistic. This tells SPSS we want to create a logistic regression model from among the many other possible generalised linear models (note: linear regression is also a specific type of generalised linear model, and could be run from this tool via the “Scale Response” Linear option).\nThen click on the Response tab and add the htn variable to the Dependent Variable: box. Then click on the Reference Category… button and under “Reference Category” select First (lowest value), which sets the lowest value in the htn variable, 0, as the reference category (i.e. htn = no as the reference category) and click Continue (without recoding the outcome variable by changing this option we could make our results refer to the likelihood of not having hypertension if this suited our needs better).\nThen click on the Predictors tab and add the categorical variables sex, ses and ace into the Factors: box and the numerical variables age, salt and bmi into the Covariates: box.\nThen click on the Model tab and highlight all the independent variables in the Factors and Covariates: box (click on the top one, hold down shift and then click on the bottom one). Then under the “Build Term(s)” options ensure the Type: is set to Main effects (this should be the default) and click the right facing arrow to add the variables into the Model: box.\nWe will ignore the Estimation tab and just accept its defaults (this is where you can alter how the model is estimated).\nNext click on the Statistics tab and in the “Print” options tick the Include exponential parameter estimates box.\nWe will ignore the EM Means tab and just accept its defaults (this is where you can tell SPSS to calculate certain predicted outcomes given certain contrasts between categorical variable levels other than those displayed by default via dummy coding, i.e. where each level is compared to a reference level).\nNext click on the Save tab and tick the Cook’s distance option to save a variable of the Cook’s distance values for each observation in the dataset. We’ll come back to this, but it provides a measure of influence of each observation on the values of the regression coefficients.\nFinally click OK.\n\nAs with the linear regression practical, before interpreting the results we should check the assumptions of the model are not violated, and if they are make any necessary changes before re-running the model."
  },
  {
    "objectID": "15-logistic-regression.html#step-3-check-the-assumptions-of-the-logistic-regression",
    "href": "15-logistic-regression.html#step-3-check-the-assumptions-of-the-logistic-regression",
    "title": "",
    "section": "Step 3: check the assumptions of the logistic regression",
    "text": "Step 3: check the assumptions of the logistic regression\nUnlike with linear regression logistic regression has fewer assumptions and they are not all easily checked. Specifically, logistic regression no longer assumes linearity in the relationship between the independent variables and the outcome, nor does it assume normality or homoscedasticity in the residuals. However, it does have some assumptions as discussed below.\n1. Binary outcome\nSelf-explanatory! However, note that we can code the variable either way around depending on whether we want our results in relation to the likelihood of observing a given “event” or the “non-event”. For example, for our hypertension variable we could obtain results in relation to the likelihood of having hypertension or of not having hypertension if we coded the outcome as 1 = hypertension or 1 = not hypertension respectively, or equivalently we can use the relevant SPSS model building options as explained in the “Step 2: create the logistic regression model” section to change the reference level.\n2. Independent observations\nAs with linear regression logistic regression assumes observations are independent of one another, which we can assume for our study data here as it came from a simple random (cross-sectional) sample. However, if our study design implies this is not the case then we need to either use a method that can cope with this (beyond the scope of this course) or modify our data.\nThe alternatives aren’t exactly the same as for linear regression though. For longitudinal (multi-time point) data we can again use logistic regression if we just use data from one time point. However, unlike with linear regression we cannot calculate a change value between two time points and analyse that with logistic regression because outcomes of 0-0 and 1-1 at two time points will both give us the same change score, but mean very different things. Similarly if we have nested or clustered observations, such as patients within clinics, we cannot calculate summary outcome values for each cluster, such as means, and use logistic regression as the values will no longer be binary. We may however calculate the proportion of events per cluster and, assuming the assumptions are met, use linear regression to analyse this cluster-level summary (proportion) outcome. Like with linear regression there are multi-level logistic regression models that can explicitly model clustered (or multi-level) data.\n3. Linearity of relationships between the log-odds and numerical independent variables\nThis is similar to the linear regression linearity assumption, but differs in a key way. With logistic regression the model estimates the predicted log-odds of the outcome rather than the actual value of the outcome (0/1), also called the logit transformation of the outcome, for each observation (e.g. individual), given/conditional on the values of the independent variable(s) for that observation. The logistic regression model assumes that there is a linear relationship between any numerical independent variable(s) and the log-odds of the outcome variable. In theory we can test this by looking at the relationship between the residuals and the predicted/fitted values or numerical independent variables, but the resulting plots can be difficult to interpret usefully as you can still get curved patterns but due to the nature of logistic regression not have a model that has violated this assumption. See the following for more discussion:\n\nhttps://stats.stackexchange.com/questions/121490/interpretation-of-plot-glm-model\nhttps://stats.stackexchange.com/questions/45050/diagnostics-for-logistic-regression\n\nHence, we won’t produce such plots here and instead recommend that you think very carefully about the model you are building in terms of whether you have allowed for any strong/important non-linear relationships and/or interactions that evidence suggests may be present."
  },
  {
    "objectID": "15-logistic-regression.html#step-4-consider-additional-problems",
    "href": "15-logistic-regression.html#step-4-consider-additional-problems",
    "title": "",
    "section": "Step 4: consider additional problems",
    "text": "Step 4: consider additional problems\n1. Adequate sample size\nAlthough you should always consider your sample size when conducting any statistical analysis logistic regression requires more data than a hypothetically equivalent linear regression to achieve the same level of precision in its estimates, because there is less statistical information in an outcome than can only take two values. There are various rules of thumb. A common one is having at least 10 observations (e.g. individuals) that have the least frequent outcome (i.e. that either have the event or do not have the event of interest, depending on whether having the event or not is more common in the sample) for each independent variable in your model. For example, if you have three independent variables and the overall proportion of your least frequent outcome, say not having hypertension, is 0.1, then you would need a minimum sample size of (10 x 3) / 0.1 = 300. All such rules are just rough guides, and it doesn’t mean you can’t build a model with less data, but they give you a good idea about the likely amount of data you need for decent precision. If you do have a much smaller sample size than these guides you’ll typically find you have very wide confidence intervals for all your coefficients, making meaningful inference difficult, and it increases your chances of getting “sparse data” or “complete separation” (see next).\n2. No complete separation/sparse data\nComplete separation refers to the situation when one or more levels of one or more categorical variables levels have 100% 1s or 100% 0s observed for the outcome (e.g. every male had “hypertension” or did not have “hypertension”). In such a situation a standard logistic regression model cannot estimate the coefficient or standard error for that categorical variable level because there is no statistical variation in the outcome. The solutions are: 1) remove the entire variable, or 2) if possible recode the variable so that you “collapse” or merge two or more category levels together so that all levels have at least one (ideally more) of each possible observation (i.e. a 1 or a 0), but the newly merged category levels must make logical sense for this to be a viable solution.\nSparse data refers to the situation where there are very few 1s or 0s observed in the outcome variable in one or more levels for one or more categorical variable levels (also referred to in this context as “cells”, e.g. the male and female cells for the variable sex). In such a situation with a standard logistic regression model the estimated coefficients and/or standard errors for that categorical variable level will often be very large and biased.\nWe checked the cell counts for each level of the outcome in the data exploration stage and found there were only 3 htn = yes observations in the ace = yes cell. There are rules of thumb for judging if a cell has sparse data, but really it’s often easiest to just run the model and look at the results and if problems have occurred you will see extremely large (positive or negative) coefficient(s) and standard errors/confidence intervals. You can also check the cell counts for sex and ses if you wish, and you should do this for all categorical variables in a real data analysis situation, or you can move on as we know from earlier data exploration that they are fine.\n3. No serious multicollinearity\nAgain if there is substantial multicollinearity between two or more independent variables this can cause serious problems with the model’s ability to accurately estimate the results. As we are only looking at correlations between the independent variables when assessing multicollinearity we can just use the Linear Regression tool to calculate the VIF values again, but there is no need to repeat this process if you don’t want to as the results will be identical to those from the linear regression practical VIF checking (because we have the same set of independent variables). Refer to the “Step 4 consider additional possible issues” section the linear regression practical for more details if needed.\n4. No extremely influential observations\nLastly we again need to check that there are no observations that have an excessively influential effect on the results of the logistic regression model. We can again use a version of the Cook’s distance statistic for logistic regression to check which observation(s), if removed, would change the coefficients substantially. When we ran the logistic regression via the Generalized Linear Model tool we told SPSS to calculate and save the Cook’s distance values for each observation in a new variable, so as with the linear regression let’s graph these against the observation ID. Graphs > Legacy Dialogues > Scatter/Dot and select the Simple Scatter option and click Define. Add the CooksDistance variable into the Y Axis: box and the id variable into the X Axis: box and click OK.\nWhat do you see?\nWhat do you see on the Cook’s Distance plot?\n\n\nRead/hide\n\nThere are two clearly excessively influential points. Interact with the graph to see which observations these are (double click, then click twice on one of the points to highlight it before right clicking and selecting “Go to Case”). Unsurprisingly they are the same two observations that were highly influential for our linear regression: 478 and 520. We can again examine the values of the independent variables for each one again but the conclusion would be the same. Although they have independent variable values that mean the model predicts a very low likelihood of having hypertension we have no reason to think there are any errors in any of their data, and so we must keep them in our primary analysis, but best practice would be to perform a sensitivity analysis by removing them and exploring if our results and therefore our conclusions change substantially, and then transparently reporting this."
  },
  {
    "objectID": "15-logistic-regression.html#step-5-understand-the-results-tables-and-extract-the-key-results",
    "href": "15-logistic-regression.html#step-5-understand-the-results-tables-and-extract-the-key-results",
    "title": "",
    "section": "Step 5: understand the results tables and extract the key results",
    "text": "Step 5: understand the results tables and extract the key results\nAs we are now satisfied that the assumptions for our logistic regression model are not violated we can interpret the results. You can either scroll up in your output window or you may wish to re-run the model. By default, SPSS produces a whopping 8 tables, most of which are not that useful for us.\n\n“Model information” provides some basic information about our model.\n“Case Processing Summary” provides information on how many observations were included and excluded from the model (all those with any missing outcome and/or independent variable data are automatically excluded).\nThe “Categorical Variable Information” and “Continuous Variable Information” tables provide descriptive information about all the categorical and continuous (also discrete numerical) variables included.\n“Goodness of Fit” provides information about how well our model fits, or explains, the data. As you can see there are many measures of goodness of fit, and they are of little use to us (they are primarily useful when comparing the fit of different competing models).\nThe “Omnibus Test” provides a useful overall test of goodness of fit based on the likelihood ratio. This test compares the fit of our model to the fit of an “intercept only” model, i.e. a logistic regression of the htn variable with no independent variables, just an intercept or overall mean. Clearly, we would hope that our model explains or fits the data much better than an intercept only model, otherwise this is an indication that our independent variables have very little ability to explain variation in the outcome and your model therefore tells you nothing about what influences the outcome. If the p-value (the “Sig.” column of the table) is statistically significant at the 5% level this is usually taken as evidence that the full model has a substantially better fit than the intercept only model. This is commonly reported along with the coefficients.\nThe “Test of Model Effects” is analogous to an ANOVA table and tells us which variables (but not separate categorical variable levels) are “statistically significant”, but nothing about their effect sizes or precision. It is therefore of arguably limited value.\n\nFinally, we get to the key “Parameter Estimates” table, which provides us with very similar analogous information as the linear regression parameter estimates table.\n\n\nParameter estimates table columns explained\n\n\nParameter\n\nAgain, each row is for a different “parameter” or “logistic regression coefficient”, which means a separate term in the logistic regression model. For numerical variables this means one row per variable. However, because each level of a categorical variable is actually treated as a separate “dummy variable” (coefficient) as standard in a logistic regression model each categorical variable level has its own row.\n\nB\n\nB stands for betas, because in the logistic regression model when represented mathematically the effect (or coefficient) of each variable is usually represented by the Greek letter beta. The betas are more commonly referred to as the parameter estimates or the (logistic regression) coefficients. They tell us the estimated direction (positive or negative) and size of effect each parameter (i.e. variable) in the regression model has on the outcome variable. However, with logistic regression the model parameters as originally estimated by the model are on the log-odds scale. Therefore, for all numerical independent variables they represent the expected mean change in the log-odds of the outcome variable being 1 for every 1-unit increase in the independent variable. For categorical variables with the default dummy coding the coefficients represent the expected mean difference or change in the log-odds of the outcome variable being 1 between each level and the reference level (e.g. male compared to female). Remember by default SPSS sets the category level coded with the highest value as the reference level, and the reference level always has a coefficient value of 0.\nA note on the intercept. Here the intercept now represents the log-odds or odds (for Exp(B)) of the outcome variable being 1 when the values for all numerical independent variables are set to 0, and for the reference levels of all categorical variables. Again, this will often not have a useful interpretation and is often not reported in a logistic regression results table.\n\nStd. Error\n\nThese are the standard errors for each coefficient, which estimate the sampling variability of the coefficients in the wider population. This is used when calculating the 95% confidence intervals and p-value.\n\n95% Wald Confidence Interval (Lower and Upper)\n\nThese are the lower and upper 95% confidence intervals for each coefficient based on the Wald method of calculation (essentially assuming a normal distribution).\n\nHypothesis Test (Wald Chi-Square, df and Sig.)\n\nThis section of the table provides hypothesis tests for each coefficient based on the Wald chi-square statistic and the given degrees of freedom. The p-value is in the “Sig.” column, and is again a two-tailed p-value. Assuming the true value of the coefficient is 0, this p-value represents the probability of obtaining a coefficient at least as great as that observed (positively or negatively) due to sampling error alone.\n\nExp(B)\n\nThese are the exponentiated coefficients, i.e. eβ. Trying typical exp(x) into Google where x is one of the coefficients on the log-odds scale. You will see it is now the Exp(B) value. By exponentiating the coefficients, we can transform them from the log-odds scale the odds ratio scale, which allows for more easily and intuitive interpretation. Therefore, they now have the following interpretations. For numerical variables they represent the multiplicative change in the odds of the outcome variable being 1 for every 1-unit increase in the independent variable. For categorical variables they represent the multiplicative difference in the odds of the outcome variable being 1 for each level of the categorical variable compared to the reference level. By multiplicative we mean on the multiplicative scale, so an odds ratio of 2 means the odds are multiplied by 2 (i.e. double) for every 1-unit increase in a numerical variable or compared to the reference level of a categorical variable. Similarly, an odds ratio of 0.5 means the odds are multiplied by 0.5 (i.e. halve) for every 1-unit increase in a numerical variable or compared to the reference level of a categorical variable. Therefore, on the multiplicative scale the null or no effect value is 1.\nA note on the intercept. Here the intercept now represents the odds (for Exp(B)) of the outcome variable being 1 when the values for all numerical independent variables are set to 0, and for the reference levels of all categorical variables. Again, this will often not have a useful interpretation and is often not reported in a logistic regression results table.\n\n95% Wald Confidence Interval for Exp(B) (Lower and Upper)\n\nThese are the (Wald-based) 95% confidence intervals for the exponentiated coefficients, i.e. for the estimated odds ratios.\n\nNote: typically, only the exponentiated coefficients (and their 95% confidence intervals) are presented in the results from a logistic regression model, due to them being much easier and more intuitive (but still not that intuitive!) to interpret than those on the original log-odds scale."
  },
  {
    "objectID": "15-logistic-regression.html#step-6-report-and-interpret-the-results",
    "href": "15-logistic-regression.html#step-6-report-and-interpret-the-results",
    "title": "",
    "section": "Step 6: report and interpret the results",
    "text": "Step 6: report and interpret the results\nLet’s see how we interpret some the results in the parameter estimates table. We will only look at the exponentiated coefficients, i.e. the odds ratios regression coefficients, and their confidence intervals as they are more easily interpreted and, as explained above, only the odds ratio scale results are typically presented instead of the original scale log-odds results. We’ll also keep things briefer than for the same linear regression sections as most of the concepts apply here in the same way, but we’re just dealing with ratio measures of relationships rather than differences/increases/decreases.\n\nUnderstanding and interpreting odds ratios\nOdds ratios are just ratios, like risk ratios, but when interpreting their direction and size mistakes can be easily made. First of all, unlike linear regression coefficients which can range (in theory) from negative infinity to positive infinity, odds ratios can range (in theory) from 0 to positive infinity. Because they are ratios the null value or no relationship/no effect value is 1, not 0 like for linear regression coefficients. This is because ratios are the result of dividing two numbers, so if those numbers are the same then the result is 1, whereas with linear regression the coefficients are differences, where a difference of 0 represents no difference. Therefore, odds ratios <1 represent a negative relationship and odds ratios >1 represent a positive relationship.\nAs odds ratios are ratios they are on the “multiplicative scale”, and they therefore represent factors by which one set of odds is multiplicatively related to another. For example, an odds ratio of 2 indicates that the odds of an outcome in the group of interest are 2 times the odds of the outcome occurring in the reference or comparison group. Take care though. It is not correct to say that an odds ratio of 2 indicates that the odds in the group of interest are 2 times higher than the odds in the reference group. To talk about odds ratios in terms of the odds for the group of interest being relatively higher or lower than the odds for the reference group you should first convert the odds ratio to a percentage using the following formulae:\n\nWhen the odds ratio is >1 you can calculate the % increase in odds as:\n\n\n(OR – 1) x 100\n\n\nFor example, for an odds ratio of 4.2:\n\n\n(4.2 – 1) x 100 = 320% increase in odds\n\n\nWhen the odds ratio is <1 you can calculate the % decrease in odds as:\n\n\n(1 – OR) x 100\n\n\nFor example, for an odds ratio of 0.45:\n\n\n(1 – 0.45) x 100 = 55% decrease in odds\n\nNote: you should of course refer to the upper and lower odds ratio confidence interval range when discussing results, and those values can be similarly transformed as above if desired.\nLet’s look at some examples to make this all clearer. We’ll start with categorical independent variables as these are arguably easier to interpret than numerical independent variables.\n\n\nCategorical variables\n\nWith the standard dummy coding of categorical variables (https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis-2/) logistic regression coefficients for categorical variables represent the model-predicted mean (or more loosely the average) ratio between the odds of the outcome for the categorical level (or group) of interest compared to the odds of the outcome for the reference or comparison level (or group), while holding the effect of all other independent variables constant, i.e. they measure the mean independent relationship. The choice of which categorical level is set as the reference group is up to you.\n\nLet’s take sex as an example. In the parameter estimates table the adjusted odds ratio and confidence interval for the category level male compared to the category level female was AOR = 3.53 (95% CI: 2, 6.27). Note: AOR = adjusted odds ratio, because the odds ratio is adjusted for all other independent variables in the model. Therefore, the model predicts that males have odds of having hypertension that are 3.53 times the odds of having hypertension for females, or equivalently the model predicts that males have odds of having hypertension that are 254% higher than the odds of having hypertension for females, while holding the effect of all other independent variables constant. The 95% confidence intervals then tell us that the odds ratio for having hypertension in males compared to females is likely to be between 2 and 6.27 in the target population. Therefore, we have a reasonably accurate/precise estimate of this relationship. However, as always this is conditional on the level of bias in the results.\n\n\nNumerical variables\nHere are four equivalent ways to interpret a logistic regression odds ratio coefficient for a numerical independent variable (i.e. you can use any of these so it makes sense to use the one you are most comfortable with):\n\nLogistic regression coefficients for numerical independent variables represent the model-predicted mean (or more loosely the average) ratio between the odds of the outcome for a 1-unit increase in the independent variable compared to the odds of the outcome without a 1-unit increase in the independent variable, while holding the effect of all other independent variables constant, i.e. they measure the mean independent relationship.\n\n\nOr you may find it easier to think in terms of the multiplicative change in the odds for every 1-unit increase in the independent variable: logistic regression coefficients for numerical independent variables represent the model-predicted mean (or more loosely the average) multiplicative change in the odds of the outcome for every 1-unit increase in the independent variable, while holding the effect of all other independent variables constant.\n\n\nOr you may find it easier to think in terms of the number of times the odds change for every 1-unit increase in the independent variable: logistic regression coefficients for numerical independent variables represent the model-predicted mean (or more loosely the average) number of times the odds of the outcome change for every 1-unit increase in the independent variable, while holding the effect of all other independent variables constant. Therefore, for any given odds ratio “OR” regression coefficient for a numerical independent variable you can say that “the model-predicts that the odds of the outcome change OR times for every 1-unit increase in the independent variable, while holding the effect of all other independent variables constant.”\n\n\nOr you can convert the odds ratio to a percentage change in the odds (i.e. percentage increase or decrease in the odds) using the formulae above and interpret it in those terms: logistic regression coefficients for numerical independent variables represent the model-predicted mean (or more loosely the average) percentage change in the odds of the outcome for every 1-unit increase in the independent variable, while holding the effect of all other independent variables constant.\n\nI usually prefer to use either the third or final interpretation/wording.\nNote: for all these equivalent interpretation the ratio/change/relationship does not depend on the value of the independent variable, i.e. the same ratio/change/relationship is assumed to exist across the full range of values that the independent variable can take in the sample data, but it should not be considered to hold if you were considering values of the independent variable outside of the range of values seen in the sample data.\nLet’s take salt intake (in grams/day) as an example. From the parameter estimates table we can see the odds ratio for this numerical independent variable is AOR = 0.96 (95% CI: 0.85, 1.14). Therefore, the model predicts that the odds of having hypertension change 0.96 times for every extra gram of salt consumed per day, while holding the effect of all other independent variables constant. Or equivalently the model predicts that the odds of having hypertension decrease by 4% for every extra gram of salt consumed per day, while holding the effect of all other independent variables constant. The 95% confidence intervals imply that the odds ratio in the target population is between 0.85 and 1.14, and therefore we cannot be confident whether the true relationship in the target population is positive or negative, although we can be confident that the true odds ratio is unlikely to be very large whether positively or negatively. As always though this is conditional on the level of bias in the results.\n\n\nPractical importance\nSimilar considerations apply when trying to interpret the practical importance of the results of a logistic regression analysis of a study as those discussed for a linear regression. Therefore, see the discussion about interpreting the practical importance of numerical independent variables and categorical independent variables in the linear regression practical notes above (in particular see the “Numerical variables” section).\nHowever, as shown in the lecture on logistic regression, it is also much more difficult to interpret the results of a logistic regression in terms of the real world importance for clinical/public health practice and policy etc than those from a linear regression for two reasons.\nFirst, odds ratios are relative measures of a relationship and you cannot interpret their absolute impact without knowing what the odds (or probability/prevalence) of the outcome are in the reference group. Therefore, an odds ratio of 100 might not represent much of an absolute increase in the probability of occurrence of some event if that event is rare, while an odds ratio of 2 might represent a large absolute increase in the probability of occurrence of some event if that event is common. For example, if the probability of developing a rare cancer is 0.001% then a risk factor that increased the odds of developing the cancer 100 times (i.e. an odds ratio of 100) would only result in a probability of developing the cancer 0.1%. While if the probability of developing a common cancer is 25% then a risk factor that increased the odds of developing the cancer just 2 times (i.e. an odds ratio of 2) would result in a probability of developing the cancer of 40%.\nSecond, odds ratios are ratios of odds, and for most people odds are not as intuitive as probabilities of an outcome. However, we can use the following formula to approximate the relative risk (or risk ratio), which is arguably more easily interpreted as it’s in terms of the probabilities of the outcome:\n\nApproximate relative risk = adjusted or crude odds ratio / (1 − p0 + (p0 x adjusted orcrude odds ratio))\n\nAs explained in the lecture the adjusted/crude odds ratio is the adjusted/crude odds ratio for the independent variable of interest (it will only be a crude estimate if the independent variable of interest is the only independent variable in the model), while p0 is the risk in the baseline/reference/control/comparison group. Also as explained in the lecture, in an observational study with multiple variables in a logistic regression model p0 will vary depending on both the value of the independent variable of interest and all other independent variables. Therefore, in such a situation we can try and estimate a range (lower and upper value) of plausible baseline risks to use in the calculation along with the range of adjusted odds ratios implied by the 95% confidence interval of the adjusted odds ratio. We will not look not at this further here, but be aware of the challenges with interpreting results from logistic regression models in practice, and if you plan to use logistic regression (and indeed sophisticated analyses more generally) frequently in the future then the best advice is to learn how to do them in R (free) or Stata where you can then use their functions for estimating predicted probabilities of the outcome at different values of the independent variables, making it very easy to calculate risk ratios or even better risk differences along with the actual predicted probabilities, making for much more intuitive practical interpretation of your model results!\n\n\n“Non-significant results”\nThe same advice applies when reporting and discussing non-significant results from a logistic regression as for a linear regression. Therefore, see the “‘Non-significant results’” section in the linear regression practical session for details.\n\n\nRegression tables\nThe same advice applies when reporting and presenting results from a logistic regression as for a linear regression in terms of presentation via a table. Therefore, see the “Regression tables” section in the linear regression practical session for details. However, an additional comment would that it is most common for logistic regression results table to just present the exponentiated regression coefficients, i.e. the odds ratio scale regression coefficients, rather than the original log-odds scale regression coefficients, which are harder to usefully interpret. You should always make it explicit and clear though what form of results you are presenting. You can always include these in the same table or a separate table (e.g. as supplementary materials) if you need to. Also, there are various pseudo-R²s for logistic regression, but they are often criticised for their less than ideal meaning and interpretability. Hence, unless required you do not need to present an “equivalent” R² value. However, it is usually recommended to present the likelihood ratio (chi-square) test result p-value, which compares your model to an intercept-only model, as this at least indicates whether your model explains more variation in the data than a simple intercept-only model.\n\n\nMethods\nAs usual in a methods section you should clearly explain why you used a logistic regression analysis and exactly what you did, including how all the variables were coded/what units they were in, if you modified any variables, how you dealt with any missing data etc."
  },
  {
    "objectID": "15-logistic-regression.html#exercise-multiple-logistic-regression",
    "href": "15-logistic-regression.html#exercise-multiple-logistic-regression",
    "title": "",
    "section": "Exercise: multiple logistic regression",
    "text": "Exercise: multiple logistic regression\n\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Multiple logistic regression: the causes of variation in hypertension status. Then follow the instructions in the exercise.\n\nExample completed logistic regression results table\n\n\nRead/hide\n\n\n\n\nLogistic regression results table\n\n\n\nExample statistical and practical interpretation of the relationship between BMI and hypertension status\n\n\nRead/hide\n\nThe model-predicted that the odds of having hypertension increased by 85% (95% CI: 64%, 110%) for every 1-unit increase in BMI (kg/m²), while holding the effect of all other independent variables constant. Therefore, there was a clear, statistically significant, positive relationship between BMI and hypertension status.\n\nExample statistical and practical interpretation of the relationship between socio-economic status and hypertension status\n\n\nRead/hide\n\nThe model-predicted that, on average, the odds of having hypertension for individuals in the low socio-economic group were 187% (95% CI: 28%, 543%) higher than the odds of having hypertension for individuals in the high socio-economic group, while, on average, the odds of having hypertension for individuals in the medium socio-economic group were 38% (95% CI: -42%, 233%) higher than the odds of having hypertension for individuals in the high socio-economic group. Note: as the lower confidence interval was for an odds ratio <1 it represents a percentage reduction in the odds and so you must indicate this by adding a negative symbol. You wouldn’t have to do this if both confidence intervals were reductions if you had verbally stated that the percentage changes were “reduction” or “decreases”, as doing both could be confusing (a double negative). Therefore, there was a clear, statistically significant, positive relationship between having low socio-economic status compared to high socio-economic status and the odds of having hypertension, but no clear (“statistically non-significant”) relationship between having medium socio-economic status compared to high socio-economic status and the odds of having hypertension.\n\nExample text discussing the key limitations of the study and the issues that must be considered when critically interpreting the results\n\n\nRead/hide\n\nCare must be taken when interpreting the results because the study was an observational cross-sectional study with therefore very limited ability to make robust causal inferences, and because the validity and accuracy of the results depends on how well the study data and model have accurately captured the causal relationships of interest and all important confounding variables of those causal relationships, and on how much bias there was in the study."
  },
  {
    "objectID": "15-logistic-regression.html#next-steps-optional",
    "href": "15-logistic-regression.html#next-steps-optional",
    "title": "",
    "section": "Next steps (optional)",
    "text": "Next steps (optional)\nIf you have time you can run a sensitivity analysis for the excessively influential points 478 and 520. Delete the htn values for these observations (which will exclude all their data from the model) and re-run the model.\nWhat happens to the results?\n\n\nRead/hide\n\nAlthough there are some minor changes, with the largest being for the effect of taking an ACE inhibitor, it seems fair to say there are no substantial changes to the interpretation of the results. You can therefore have greater confidence in the robustness of the primary results."
  },
  {
    "objectID": "16-complex-survey-analysis.html",
    "href": "16-complex-survey-analysis.html",
    "title": "",
    "section": "",
    "text": "Complex survey analysis as it is commonly known is simply the analysis of complex survey data using appropriately modified methods. You may also see it referred to as complex sample analysis. Despite the common name of complex survey analysis complex survey data does not have to have come from a “survey”, which doesn’t actually define a study design, although most complex survey datasets will be surveys. More generally or technically speaking, complex survey data is any dataset (i.e. sample) that has come from a probability sampling method that has any combination of the following three “complex design features”, with all three often being present together:\n\nStratification\nCluster or clustered sampling\nUnequal probabilities of selection for sampling units\n\nSuch data may have come from a single cross-sectional sample, multiple cross-sectional samples (i.e. multiple samples across time of different sampling units), or be true longitudinal data (i.e. multiple samples across time of the same sampling units).\nThis type of data is most commonly encountered/analysed in global health research when working with large-scale, typically national-scale, cross-sectional household surveys, such as the Demographic and Health Surveys (DHS: https://dhsprogram.com/), the Multiple Indicator Cluster Surveys (MICS: https://mics.unicef.org/), the WHO STEPwise Approach to surveillance Surveys (STEPS: https://www.who.int/ncds/surveillance/steps/en/), and many countries’ own household surveys. This is because large-scale household surveys almost always use all three of these key complex survey features of stratification, cluster sampling, and unequal probabilities of selection of sampling units. The reason they do this is usually for both pragmatic, logistical, and statistical/research reasons. Specifically, stratified sampling is often employed to both allow strata (typically an administrative level such as the region, often further stratified into urban and rural areas) to be independently sampled to ensure that each strata has a sufficient sample size to allow inferences to be made at the strata-level. Cluster sampling is primarily employed to reduce the logistical costs of sampling across huge geographical areas. While stratification and the multi-stage cluster sampling methods employed result in unequal probabilities of selection for sampling units (typically individuals).\nStandard methods of analysis, or more technically the maths underlying those methods, such as the independent t-test and linear regression, assume that the data being analysed come from a simple random sample. However, any of the complex survey features mentioned above result in a violation of that assumption, which if not adjusted for would result in either biased point estimates (e.g. biased regression coefficients) and/or biased standard errors of those point estimates (i.e. biased variance estimates), which means biased confidence intervals and p-values. More specifically, not adjusting for clustering typically results in falsely narrow confidence intervals/falsely small p-values, while not adjusting for stratification can mean the analysis “misses out” of the benefits of a stratified sample, which can reduce the overall variation in the outcome and produce more precise results for a given sample size. And not adjusting for unequal probabilities of selection of sampling units typically results in both biased/inaccurate point estimates of characteristics/relationships and falsely narrow confidence intervals/falsely small p-values. Therefore, it is critical that we account or adjust for any complex survey design features in our analyses to obtain unbiased results.\nLuckily there are modifications to all commonly used analytical methods, including t-tests and regression models (i.e. linear and logistic and others that we’ve not looked at). And also fortunately we don’t have to worry about the underlying maths. With statistical software like SPSS primarily all we have to do is tell the software which variables code for, or identify, the different complex design features, and then select the appropriate complex survey design version of the analysis we want to do. Then we proceed with our analysis like we would if we were analysing data from a simple random sample, i.e. using the processes outlined in the earlier practical sessions.\nIn any high quality complex survey dataset that used stratification, cluster sampling, and unequal probabilities of selection of sampling units in its sampling design, there will be a variable that indicates which strata each observation (sampling unit) comes from and a variable that indicates which cluster sampling unit each observation comes from. There will also be a third variable that contains the survey weights, which are used to adjust for the unequal probabilities of selection of sampling units.\n\n\n\n\n\nRead/hide\n\nSampling with unequal probabilities of selection of sampling units results in unrepresentative samples (i.e. unrepresentative of the target population). This may occur when using stratified sampling with disproportionate strata, or when using a multi-stage cluster sampling approach. To obtain representative results we must therefore “map” the survey sample back onto the target population. To do this survey data producers calculate/produce sampling weights. In the analysis these weights upweight (or increase the contribution) of observations from sampling units that were undersampled compared to proportion of the target population that they represent, and they downweight (or decrease the contribution) of observations from sampling units that were oversampled compared to proportion of the target population that they represent. These weights are then usually combined with two other sets of weights. One that attempts to correct for any missing observations, and one that uses any pre-existing, recent, robust data on characteristics of the target population (such as a recent census) to adjust for any remaining lack of representativeness still reflected in the sample. Once all three types of weight are combined this results in a final or ultimate survey weight variable.\n\nTherefore, once we’ve identified these three variables and “told” SPSS which variables they are we can analyse our data and largely forget/ignore the fact that we are doing a complex survey version of our chosen analysis. The type of results we get and their interpretation will, broadly speaking, not change apart from in a few areas that we will look at in this practical session.\n\n\n\n\n\nRead/hide\n\nUnfortunately, the best and most beginner-friendly textbook on complex survey data analysis, which also included examples using SPSS (as well as other statistical software) is now no longer available from the library (it was previously available online via the library). This book is: Heeringa, S., West, B.T., Berglund, P.A. (2017). Applied Survey Data Analysis (2nd ed.). Boca Raton, FL, USA: CRC Press. If you do plan on conducting complex survey analyses in your later careers I would strongly recommend buying the book, as it’s well worth it. The accompanying website also has lots of practice examples of analyses in SPSS and other statistical software. Unfortunately there are not any similar textbooks that I am aware of for complex survey analysis.\nHowever, there is a nice overview/summary of the main issues around complex survey data analysis in the following paper, by one of the authors of the book I’ve just mentioned, which is freely available here:\nhttps://pubmed.ncbi.nlm.nih.gov/18956450/\n\n\n\n\nThe following is an excellent future reference checklist of steps to undertake when preparing to analyse complex survey data in a real analysis. It is taken from the Applied Survey Data Analysis textbook mentioned previously. For this practical we will assume we have carried out steps 1-3 and step 5 to save time, and some of the issues mentioned are beyond the scope of this introductory course/session, but if you were to do complex survey analysis for a real study or later in your career you should educate yourself enough to understand all these terms and the issues they refer to (e.g. by buying and reading the recommended textbook).\n\n\nRead/hide\n\n\nReview the documentation for the data set provided by the data producer, specifically focusing on sections discussing the development of the final survey weights and sampling error (standard error) estimation. Contact the data producer if any questions arise.\nIdentify the correct weight variable for the analysis, keeping in mind that many survey data sets include separate weights for different types of analyses. Perform simple descriptive analyses of the weight variable, noting the general distribution of the weights, whether the weights have been normalized, and whether there are missing or 0 weight values for some cases. Select a few key variables from the survey data set and compare weighted and unweighted estimates of descriptive parameters (e.g., means, proportions) for these variables to understand the effect the weights have.\nIdentify the variables in the data set containing the “sampling error calculation codes” that define the sampling error calculation model [JPH: these are just the variables defining the strata and clusters]. Examine how many clusters were selected from each sampling stratum (according to the sampling error calculation model), and whether particular clusters have small sample sizes. If only a single sampling error cluster is identified in a sampling stratum, contact the data producer or consult the documentation for the data set for guidance on recommended variance estimation methods. Determine whether replicate sampling weights are present if sampling error calculation codes are not available, and make sure that the statistical software is capable of accommodating replicate weights (Section 4.2.1).\nCreate a final analysis data set containing only the analysis variables of interest (including the survey weights, sampling error calculation variables, and case identifiers) [JPH: this is optional. It’s nice to keep your dataset tidy and no bigger than necessary for speed and manageability, but it obviously doesn’t affect the analysis in anyway]. Examine univariate and bivariate summaries for the key analysis variables to determine possible problems with missing data or unusual values on the individual variables.\nReview the documentation provided by the data producer to understand the procedure (typically nonresponse adjustment) used to address unit nonresponse or nonresponse to a wave or phase of the survey data collection. Analyse the rates and patterns of item missing data for all variables that will be included in the analysis. Investigate the potential missing data mechanism by defining indicator variables flagging missing data for the analysis variables of interest. Use statistical tests (e.g., chi-square tests, two-sample t-tests) to see if there are any systematic differences between respondents providing complete responses and respondents failing to provide complete responses on important analysis variables (e.g., demographics). Choose an appropriate strategy for addressing missing data using the guidance provided in Section 4.4 and Chapter 12 [JPH: we will not look at missing data issues in this practical, but most high quality complex survey datasets, such as the DHS surveys, include weights that incorporate adjustments for missing data that, while not perfect by any means, go some way to addressing bias from missing data by simply including the weights in the analysis as you should be doing anyway].\nDefine indicator variables for important analysis subclasses. Do not delete cases that are not a part of the primary analysis subclass. Assess a cross-tabulation of the stratum and cluster sampling error calculation codes for the subclass cases to identify the distribution of the subclass across the strata and clusters defined by the sampling error calculation model. Consult a survey statistician prior to analysis of subclasses that exhibit the “mixed class” characteristic illustrated in Figure 4.4. Make sure to employ appropriate software options for unconditional subclass analyses if using TSL for variance estimation [JPH: we will look at this issue in more detail later in this practical session].\n\n\n\n\n\nThe dataset we will use here is one of the DHS’s “model datasets”, and as such it was produced by the DHS for practicing analysis of DHS survey data. It is therefore representative of the types of data and data features (e.g. complex survey features) found in DHS surveys, but it doesn’t contain any real data from any country. DHS surveys typically result in the production of a number of datasets, primarily a household-level dataset and datasets for women, men, and children, plus potentially others. You can read more about DHS datasets here: https://dhsprogram.com/data/\nHowever, we will just use the “individual recode” dataset in the SPSS .sav format (DHS helpfully makes their datasets available in other formats too). This contains data from the female respondents of the survey, who consist of all members of a selected household who are aged 15-49. This dataset is included in the “Datasets” folder of the MSc & MPH computer sessions practical files, and is called “ZZIR62FL.SAV”. The “odd” naming is due to DHS’s naming conventions, which you can read about here: https://www.dhsprogram.com/data/File-Types-and-Names.cfm\nMore specifically, we will use the dataset to answer the research question “what factors affect the time women report it takes someone from their household to reach the household’s source of drinking water?”\nWe will assume that our causal inference based model building process has led us to hypothesise that the most likely set of causal influences on the time household members take to reach the household’s source of drinking water are:\n\nThe household location (urban or rural)\nThe wealth level of the household\nThe age of the “head of the household”\n\nOf course this is just a simplified assumption to make things easier for learning purposes. Then like with our regression analyses, and unlike in a proper causal inference driven approach, we will make the simplifying assumption that we are interested in the relationship between each of these independent variables and the outcome variable of “time household members take to reach the household’s source of drinking water”, i.e. we assume all these independent variables are likely causally related to the outcome, but that together these independent variables collectively form the sufficient adjustment set for each of the separate focal relationships. Therefore, like with our regression analyses to simplify things we will just create one model containing all these variables and interpret the results as though we had, to the best of our knowledge, adjusted for all likely sources of confounding for each focal relationship.\nFirst of all though we will identify, explore, and edit as necessary the key complex design variables of the survey. We will then explore our independent and outcome variables, and then produce descriptive statistics to describe the sample’s characteristics in terms of the variables measured, as would commonly be found in the “Table 1” of a paper. Then finally we will produce our analytical model results.\n\nLoad the “ZZIR62FL.sav” SPSS dataset.\n\n\n\n\nVideo instructions: prepare and explore the data\nWritten instructions: prepare and explore the data\n\n\n\n\n\n\n\nV005 = Women’s individual sample weight (6 decimals) - numerical\nV021 = Primary sampling unit - nominal\nV023 = Stratification used in sample design - nominal\nV025 = Type of place of residence - nominal\nV115 = Time to get to water source - numerical (minutes)\nV152 = Age of household head - numerical (years)\nV190 = Wealth index - ordinal\n\n\n\n\n\n\nRead/hide\n\nAs per the checklist steps 1-3 for any complex survey analysis you must first thoroughly review the survey technical/methodological documentation to understand the design of the survey and sampling process, and to understand which variables contain the stratification, clustering, and weighting information, and to understand any modifications of those variables that are required. To save time we will assume we have done this, and that we have removed all variables from the dataset other than these design variables and the independent and outcome variables.\nTherefore, open the ZZIR62FL.SAV dataset. If a window appears with a message starting “IBM SPSS Statistics is running in Unicode encoding mode…” just click Yes. We can assume that from reading the relevant technical/methodological literature on this survey we have identified the relevant complex design variables and our variables of interest and removed all other variables from the dataset. The information on the complex design variables can be found in the “DHS Guide to Statistics” (https://dhsprogram.com/data/Guide-to-DHS-Statistics/Analyzing_DHS_Data.htm), but you should also review the survey specific literature (each DHS survey has a final report with specific methodological details relevant to that survey, along with a full copy of the questionnaire used that you should always review).\n\n\nTherefore, V005 identifies the survey weights, V021 identifies the clusters (often called primary sampling units as they are the first stage of a multi-stage cluster sample), and V023 identifies the strata. However, and this shows why you must read survey methodological literature carefully, all DHS weights are stored multiplied by 1,000,000. This is for very technical computer science related reasons about floating point errors that we don’t need to go into here, but the point is that to use the weight variable we therefore need to first divide all values by 1,000,000.\nTo do this from the main menu go: Transform > Compute Variable. Then in the Target Variable: box give our new variable a suitable name, say “survey_weight”, and in the Numeric Expression: box enter the following:\n\nV005 / 1000000\n\nThen click OK. You can now delete the old V005 variable if you wish.\nBefore we go any further let’s rename the other variables for ease of reference. We’ll use the following names:\n\nV021 = cluster\nV023 = strata\nV025 = location\nV115 = time_water\nV152 = age_hhh\nV190 = wealth\n\nAnd let’s also change the variables cluster, strata, location, and wealth to the variable type nominal so SPSS treats them as categorical variables (as they use numerical coding SPSS automatically treats them as numerical variables unless told otherwise).\nNow let’s explore the complex design variables. For a quick overview of their descriptive statistics remember you can just right click on each variable in either the Data view or Variable View and click on Descriptive Statistics. However, you should first change the strata (V023) and cluster variables. As our strata and cluster variables are both categorical this is probably sufficient, but it’s worth producing a histogram for the numerical survey_weight variable to visualise its distribution. What do the descriptive statistics and histogram show you for the three complex design variables?\nWhat do the descriptive statistics and histogram show you for the three complex design variables?\n\n\nRead/hide\n\nThere are no missing values for any of the variables. The observations are typically evenly distributed across clusters, but much less evenly distributed across strata. However, there are no concerning issues, such as a single observation in a cluster or strata, that might indicate data errors. The histogram of the weights shows a strongly right-skewed distribution with some potentially concerning extremely high weight values. If this was a real analysis it would be worth looking into these more closely, but we’ll assume they’re all fine.\n\n\n\n\nNext let’s explore each of the data variables in turn. Again for a quick univariate exploration we can just use the same process as we used to explore the complex design variables.\nWhat do the descriptive statistics and histogram show you for the four data variables?\n\n\nRead/hide\n\nThere are no missing values for location, and there is a ~40%:60% split between urban and rural located households of the women surveyed. There are 13 missing values of our outcome variable time_water, and a histogram indicates that the variable appears to have a strongly “bimodal” distribution with two distinct “peaks”, but we’ll come back this shortly. There are 6 missing values of age_hhh, and a histogram indicates that the variable is fairly normally distributed. There are no missing values of wealth, and observations are evenly distributed across it’s five levels. This is to be expected because in DHS surveys the numerical household wealth index is categorised into five wealth quintiles by evenly separating increasing values of the variable into five levels/groups.\nSo back to the outcome variable. This is a good example of why a careful data exploration is important prior to any analysis, and why understanding your survey data and methodological documentation is so important. You can access the questionnaire that the model dataset is based on here: https://dhsprogram.com/pubs/pdf/DHSQ6/DHS6_Questionnaires_5Nov2012_DHSQ6.pdf\nScroll down to question 104 “How long does it take to go there, get water, and come back?”, which is where our variable comes from.\nWhat can you see about the possible response values?\n\n\nRead/hide\n\nRespondents can provide the number of minutes it takes them to go and collect water and come back, but for those who don’t know their response is coded as 998:, which is the second peak we see on the histogram. Therefore, the distribution is not bimodal at all, we’re just not looking at only the true/non-missing values.\n\nTherefore, let’s just look at the distribution of non-missing values for our outcome. Instead of deleting all observations where the response was “don’t know” to our question of interest we can tell SPSS to treat the value 998 as a missing value for this variable. In the Variable View for the time_water row click on the cell under the column Missing and click on the box with three dots in that appears to the right of the cell. In the Missing Values box that appears click the Discrete missing values button and in the first box enter 998 and then click OK.\nNow re-run the histogram and what do you see?\n\n\nRead/hide\n\nThe distribution of values is clearly strongly right-skewed. Remember that linear regression assumes that the residuals are approximately normally distributed not the raw values of the outcome, and therefore once we adjust for all our independent variables our model may be valid. However, with such strong right-skew it looks likely that we’ll have to transform the outcome or use a different model (e.g. a valid alternative would be to dichotomise the outcome, i.e. convert it into a binary variable, and use a logistic regression, although that would throw away a lot of statistical information and result in cruder inferences). We shall see!\n\n\n\n\n\nLet’s look at the relationships between our independent variables and our outcome to check for any clearly non-linear relationships. In a real analysis we would also want to check for the presence of strong interactions, ideally that we would expect from theory, but that is beyond the scope of this practical and course.\nAs previously for numerical independent variables we use scatter plots.\n\nFrom the menu go: Graphs > Legacy Dialogues > Scatter/Dot, then select the default top-left Simple scatter option and click Define. Then add age_hhh and water_time to the appropriate X Axis: and Y Axis: boxes respectively and click OK.\n\nIt’s hard to see much of a relationship there, and certainly no clear non-linear relationship.\nThen for the two categorical independent variables again we can use box plots.\n\nFrom the main menu go: Graphs > Legacy Dialogues > Boxplot, then select Simple and click Define. Add the water_time variable into the Variable: box and location into the Category Axis: box and click OK.\n\nIt’s also hard to see much of a clear relationship here, but the median time for urban looks slightly higher than the median time for rural, but clearly there’s a lot of variation in both category levels. If you repeat the graph but for wealth what do you see? Again, personally I can’t see any clear relationships here."
  },
  {
    "objectID": "16-complex-survey-analysis.html#step-2-create-the-spss-complex-samples-plan-file",
    "href": "16-complex-survey-analysis.html#step-2-create-the-spss-complex-samples-plan-file",
    "title": "",
    "section": "Step 2: create the SPSS “complex samples plan file”",
    "text": "Step 2: create the SPSS “complex samples plan file”\nVideo instructions: create the SPSS “complex samples plan file”\nWritten instructions: create the SPSS “complex samples plan file”\n\n\nRead/hide\n\nAs mentioned previously in order to undertake valid analysis of a complex design dataset we must first tell SPSS which variables code for the complex design features in our sample. We only need to do this once.\n\nTo do this from the main menu go: Analyse > Complex Samples > Prepare for Analysis. Then with the Create a plan file button selected click Next. Then in the Save Data As window that opens we need to first create a complex samples plan file that SPSS can store the information in about which variables code for the complex design features. Therefore, navigate to a suitable folder and enter a suitable name for the plan into the File name: box, such as “water_survey”, and then click Save. You can now identify the complex design variables coding for the strata, clusters, and weights to SPSS. Note: if you are working with data that comes from a complex sample using few than all three of these design features you can of course just identify those.\nIn the Analysis Preparation Wizard simply add the strata variable into the Strata: box, the cluster variable into the Clusters: box, and the survey_weight variable into the Sample Weight: box. You can then just click Finish, because the default options are that you can set on the next pages by clicking Next are suitable.\n\nYou have now created your sampling plan file which can be reused for any analysis of this dataset, and we can now run any analysis that can account for a complex design using this file and SPSS will adjust for the stratification, clustering and weights."
  },
  {
    "objectID": "16-complex-survey-analysis.html#step-3-describe-the-sample",
    "href": "16-complex-survey-analysis.html#step-3-describe-the-sample",
    "title": "",
    "section": "Step 3: describe the sample",
    "text": "Step 3: describe the sample\nVideo instructions: describe the sample\nWritten instructions: describe the sample\n\n\nRead/hide\n\nNote: if you feel you are short on time then skip this exercise and move onto the next exercise that focuses on producing the inferential results.\nWe will calculate the mean for water_time and age_hhh and the frequency and percentage for each category level of location and wealth. Let’s start with the numerical independent variables.\n\nFrom the main menu go: Analyse > Complex Samples > Descriptives, then in the Complex Samples Plan for Descriptives Analysis tool just click Continue as the complex samples plan file will be automatically selected. If for any reason it is not click Browse and locate and select it before clicking Continue.\nNow in the Complex Samples Descriptives tool add time_water and age_hhh to the Measures: box. Then click the Statistics button and ensure that the Mean button is ticked and click Continue. For some reason there is no option to calculate the standard deviation or median. The range isn’t affected by the complex design features though so you could still calculate that using the standard tool.\n\nYou’ll see that water_time has a mean of 24.61 and age_hhh has a mean of 45.48. These are both adjusted for the complex design features of the sample. To compare to an analysis ignoring these features right click on the water_time variable and select the Descriptive Statistics option. You’ll see that the mean for water_time is 23.74 when you ignore the complex design features. In this instance it’s not a big difference, but this is the affect of accounting for the weights variable.\nNow let’s calculate the frequency and percentage for each category level of location and wealth.\n\nFrom the main menu go: Analyse > Complex Samples > Frequencies, then in the Complex Samples Plan for Descriptives Analysis tool just click Continue as the complex samples plan file will be automatically selected. If for any reason it is not click Browse and locate and select it before clicking Continue.\nNow in the Complex Samples for Frequencies Analysis tool add location and wealth to the Frequency Tables: box, then click the Statistics button and select the Population size, Table percent, and the Unweighted count tick boxes, and then click Continue and then OK.\n\nThe resulting tables are not helpfully laid out! The Estimate column gives the frequencies adjusted for the weights in the top half of the table and the corresponding percentages adjusted for the weights in the bottom half of the table. Then the Unweighted count gives the frequencies not adjusted for the weights in both the top and bottom halves of the table, i.e. the values are just repeated for some unclear reason.\nFor both means and frequencies and percentages you would usually want to provide the weight-adjusted descriptive statistics because they reflect the target population that you are making inferences to, given that the unadjusted sample results will be, by the sampling design, not representative of that target population while the adjusted results aim to be."
  },
  {
    "objectID": "16-complex-survey-analysis.html#step-4-run-the-complex-design-adjusted-linear-regression-model",
    "href": "16-complex-survey-analysis.html#step-4-run-the-complex-design-adjusted-linear-regression-model",
    "title": "",
    "section": "Step 4: run the complex design adjusted linear regression model",
    "text": "Step 4: run the complex design adjusted linear regression model\nVideo instructions: run the complex design adjusted linear regression model\nWritten instructions: run the complex design adjusted linear regression model\n\n\nRead/hide\n\nWe are now ready to produce our analytical inferential statistics via our linear regression model. Again we can just use our existing complex samples plan file to adjust for the complex design features using the appropriate complex design linear regression tool.\n\nFrom the main menu go: Analyse > Complex Samples > General Linear Model. Remember a general linear model is a synonym for a linear regression.\nOn the Complex Samples Plan for General Linear Model tool click Continue. Now add water_time into the Dependent Variable: box, location and wealth into the Factors: box, and age_hhh into the Covariates: box. By default just the main effects of each variable are included in the model so unless you want to specify interactions you can ignore the Model button.\nNext click the Statistics button and in the Model Parameters area ensure the Estimate and Confidence interval boxes are ticked, along with the Model Fit and Sample design information boxes below. Then click Continue.\nWe can ignore the Hypothesis Tests button and the Estimated Means button, but click the Save button and in the Save Variables are ensure the Predicted Values and Residuals boxes are ticked then click Continue.\nWe can also ignore the Options box so just click OK."
  },
  {
    "objectID": "16-complex-survey-analysis.html#step-5-check-the-assumptions-of-the-complex-design-linear-regression-model",
    "href": "16-complex-survey-analysis.html#step-5-check-the-assumptions-of-the-complex-design-linear-regression-model",
    "title": "",
    "section": "Step 5: check the assumptions of the complex design linear regression model",
    "text": "Step 5: check the assumptions of the complex design linear regression model\nHowever, as always before interpreting the results we should check the model’s assumption are not violated. In particular, we should check that the residuals are approximately normally distributed and not heteroscedastic. To check the first of these just plot a histogram of the saved residual values (the variable should be called Residual).\nWhat do you see?\n\n\nRead/hide\n\nThe residuals are clearly strongly right-skewed!\n\nTherefore, we can either try and transform the outcome or use an alternative analysis, such as a logistic regression after dichotomising the outcome. For now let’s see how we would interpret the results assuming that the assumptions were not violated, and then we will leave it as an exercise for you to re-run the model with a transformed outcome to see if that solves things, and then interpret the back-transformed results.\nNote: in a real analysis you should check all assumptions are not violated. Just follow the instructions in the “Step 2: check the assumptions of the linear regression” section in the linear regression practical for guidance, and use the Residuals and Predicted variables produced by the complex design adjusted linear regression we just ran accordingly."
  },
  {
    "objectID": "16-complex-survey-analysis.html#step-6-understand-the-results-tables",
    "href": "16-complex-survey-analysis.html#step-6-understand-the-results-tables",
    "title": "",
    "section": "Step 6: understand the results tables",
    "text": "Step 6: understand the results tables\nAfter running the model we see four tables (scroll up from the residual histogram you should have just created and anything else you’ve done since running the model).\nThe Sample Design Information table tells us some important things about our sample and design. In particular, it tells us how many cases (in our case women) were missing or not missing from the analysis, i.e. our effective sample size: these are the valid and invalid N values, as well as the total unweighted sample size (Total N). It also tells us the estimated size of the target population, but this is only valid if the weights are not “normalised” to have a mean of 1, which DHS weights areso you can ignore this. It also tells us the number of strata (Strata) and clusters (Units), which serves as a check if all strata and clusters were included in the analysis or if any were left out due to missing observations in those strata/clusters.\nThe Model Summary table tells us the R² value for the model, although frustratingly not the more robust and useful adjusted R² value!\nWe can ignore the Test of Model Effects table as this is an ANOVA type table and not really useful.\nThen we have our familar Parameter Estimates table where we get our key inferential results about the relationships between the independent variables and the outcome in the model. These are interpreted in exactly the same way as the linear regression coefficients from a linear regression that does not adjust for any complex design features. Therefore, refer back to the “Step 6: report and interpret the results” section of the linear regression practical, and in particular the sub-sections “Numerical variables” and “Categorical variables”, for a reminder if needed."
  },
  {
    "objectID": "16-complex-survey-analysis.html#exercise-complex-design-adjusted-linear-regression",
    "href": "16-complex-survey-analysis.html#exercise-complex-design-adjusted-linear-regression",
    "title": "",
    "section": "Exercise: complex design adjusted linear regression",
    "text": "Exercise: complex design adjusted linear regression\n\nIn the MSc & MPH computer sessions practical files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Complex design adjusted linear regression: the causes of variation in time taken to fetch water. Then follow the instructions in the exercise.\n\nExample statistical interpretation of the relationship between household location, household wealth, and age of household head, and the time taken for household members to fetch water\n\n\nRead/hide\n\nThere was a clear, statistically significant relationship between household location and time taken for household members to fetch water, with women from households in location=1 reporting that it took household members a mean of 7.9 minutes (95% CI: 1.5, 14) longer to fetch water than women from households in location=2. However, there was no clear relationship between household wealth or age of household head on the time taken by household members to fetch water. The R² value associated with the model was just 0.03, and therefore the model explained just 3% of the variation in the time taken by household members to fetch water. Therefore, the model explained very little about what is likely to drive differences in the time taken by household members to fetch water. As always the accuracy of these inferences depend on the amount of bias in the study and analysis.\n\nExample statistical interpretation of the relationship between household location, household wealth, and age of household head, and the ln-transformed time taken for household members to fetch water\n\n\nRead/hide\n\nOnce the outcome was log-transformed the model residuals appeared very normally distributed, but there were no longer any clear relationships between any of the independent variables and the log-transformed outcome.\n\n\n\nWell done for getting to the end! If all of this seemed like very hard work that’s because it is, but hopefully you feel like all your hard work has gained you some powerful new knowledge and skills. However, probably the most important lesson we hope you take away from this course is this: anyone can run complicated statistical analyses with modern statistical software, but it takes a lot of hard work, careful thought, and experience from continuous practice and learning, to make robust, valid, and meaningful statistical inferences. Hopefully you can also see that being able to do robust and useful statistical analyses is a hugely powerful skill to have.\nWe wish you all the best with the rest of your studies and in your future careers."
  },
  {
    "objectID": "2-probability-sampling.html",
    "href": "2-probability-sampling.html",
    "title": "",
    "section": "",
    "text": "Two key terms/definitions to understand before we go further are:\n\nUnit of observation: the entities (e.g. people, health facilities) that are described by the data, or more practically speaking the entities that you collect data from. The unit of observation should be further definable in terms of the range of characteristics that the entities have (e.g. the eligibility criteria of the study), and in terms of the spatial and temporal parameters of the study and data collection process (e.g. patients visiting any of the study health facilities between January 1st 2020 to January 1st 2021, including multiple visits).\nUnit of analysis: the entities that you are analysing in your study and aiming to make statistical inferences about using the data collected. The unit of analysis is therefore defined both by how you collect your data but also by how you analyse it.\n\n\nOften the unit of observation and analysis will be the same. For example, in most typical cross-sectional surveys the individual will be both the unit of observation and analysis. However, this won’t always be the case. For example, in a cluster randomised controlled trial you may collect data from individual patients but their outcomes might be aggregated at the cluster-level (e.g. binary patient-level outcomes converted to cluster-level proportions) and then compared between treatement arms, in which case the unit of observation would be the patient but the unit of analysis would be the cluster (e.g. the health facility).\n\n\n\n\nYou may or may not wish to review this before working through the instructions for each method and the exercises.\n\n\nRead/hide\n\n\n\nBroadly speaking sampling is the process whereby we choose which units of observation or “sampling units” within our target population we will collect data from. Therefore, the first stage of sampling, which should always occur very early on in the research and study design process, is when you clearly and explicitly define your target population. In summary, the only sampling methods that are guaranteed to produce unbiased statistical inferences, albeit only on average or “in the long-run”, are “probability sampling” methods. This is because all standard statistical methods of analysis assume that sample data are randomly sampled from a given target population using some specific form of probability sampling. Therefore, the only way to robustly satisfy this assumption is clearly to take some form of probability sample.\nTo take a probability sample you require a sampling frame, which is just a list of all units of observation in the target population that you are aiming to generalise your results to (e.g. all individuals in a region, all primary care health facilities in a country etc). Therefore, the second stage of any sampling process is to obtain or create a sampling frame for your target population. Note: for a probability sampling method to be valid it must be possible, at least in theory although you wouldn’t actually do it in practice, to calculate each unit of observation’s probability of being selected, which may or may not be equal for all units, and should not be zero for any unit.\nFor most sampling methods we require a complete and comprehensive sampling frame before we actually sample. Although for some multi-stage sampling approaches, including those typically used by large-scale household surveys (as discussed in the lecture), you can use incomplete sampling frames to start your sampling and construct additional but still incomplete sampling frame(s) during your sampling process to achieve a robust probability sample. However, as these multi-stage approaches are quite complicated and you would not be advised to undertake them without extensive statistical support we will not practice them here.\nIn these sampling practical sessions we will therefore look at how you can apply most of the commonly used simple probability sampling methods, specifically simple random sampling, a stratified random sampling with proportionate stratification and disproportionate stratification, and systematic random sampling. We will assume we have a complete sampling frame for our hypothetical target population. We will also note how these methods can also be used to take single-stage cluster samples with complete second/final-stage sampling, and how they can be combined into multi-stage cluster samples. However, we will not actually practice these much more complicated methods as it’s very unlikely you would ever need to implement them yourself (unless you become a survey methodologist etc).\n\n\n\nWhile desirable it is clearly often not feasible to use probability sampling methods, usually because of the lack of a suitable sampling frame. For example, if you are studying patients attending a healthcare facility over a given period and you need to collect data on sampled patients on the day they attend the facility it’s typically impossible to construct a sampling frame because you won’t know who will be visiting each day! Therefore, you may often need to resort to non-probability sampling methods such as quota sampling or convenience sampling, and just try to make your samples as representative of your target population as you can, while minimising the opportunity for any researcher sampling bias.\nBroadly speaking it is often more justifiable to generalise results from analytical (associational) studies that use non-probability sampling methods than from descriptive studies (surveys) that use non-probability sampling methods, and consequently if you are aiming to describe certain characteristics of a given target population you should aim to take a probability sample. However, you may often be able to make a good justification that results from an analytical study can be generalised despite the study using non-probability sampling methods. For example, many (maybe most) trials of patient-level treatments use non-probability samples, often convenience samples taken at health facilities as patients attend, due to the difficulty of taking a true probability sample. In such circumstances it is often justifiable to generalise the results to other individuals who share similar key characteristics with the trial sample (e.g. age, sex, and biomedical characteristics distributions), because patient-level treatments often work fairly similarly across different populations with similar characteristics. This may be much less justified for complex interventions delivered at higher levels, such as complex interventions with staff-level and facility-level components, because the effects of these types of interventions are likely to be much more dependent on the study’s context. Hence, generalising the results of complex interventions from studies like cluster trials that use non-probability samples may be much more questionable. The only “solution” is to think very carefully when planning your sampling methods and think explicitly from the start about who you want your results to robustly generalise to."
  },
  {
    "objectID": "2-probability-sampling.html#method-1-simple-random-sampling",
    "href": "2-probability-sampling.html#method-1-simple-random-sampling",
    "title": "",
    "section": "Method 1: simple random sampling",
    "text": "Method 1: simple random sampling\n\nA brief overview of the method\nSimple random sampling involves taking a random sample of a given size from a sampling frame, which results in each unit of observation having an equal probability of being selected. You can take a simple random sample from a sampling frame in various ways, such as by generating random numbers that correspond to IDs that are pre-allocated to all units of observation, or you can do it by simply randomly sorting the list and just selecting the first n units of observation, where n is your sample size. We will use this second approach as it’s easy to implement in Excel.\n\n\nAdvantages of simple random sampling\n\nEasy to implement and explain.\nRequires minimal data on your target population units of observation: just a list of all units of observation, plus usually some way of identifying/contacting them once selected.\n\n\n\nDisadvantages/limitations of simple random sampling\n\nAlthough simple random sampling guarantees that you will select a sample that is representative (i.e. unbiased) of your target population on average/in the long-run, i.e. over many hypothetically repeated samples, it is not the best approach at achieving this goal for any given sample, and obviously in practice you typically only ever take one sample! This is especially the case when the population is heterogeneous (highly varied in the characteristics/relationships of interest) and/or if sample size is small, say in the tens or low hundreds rather than the high hundreds or thousands (as the sample size increases the chances of getting an unrepresentative sample decrease). As these two situations are often true it is often better to use stratified random sampling instead, if possible.\nIf your sampling units cover a large geographical area simple random sampling can produce a very logistically inefficient, costly, and geographically spread-out sample. See cluster sampling for an possible solution to this problem.\n\n\n\nScenario\nYou work for a district governmental health department and you have been tasked with assessing the capacity and service delivery characteristics of the public primary care facilities across the whole district. Specifically, you need to report the typical staffing levels, resources and equipment levels, and services delivery levels for all public primary care facilities (e.g. the mean no. drs and nurses per facility.\nHowever, while there are 246 such facilities within your district you only have resources to survey 50 facilities (if you could survey all 246 it would be a “census” not a sample). A sample size calculation indicates that your sample size of 50 will be sufficient to provide usefully precise estimates of these characteristics at the district level. All 246 facilities are therefore your target population to which you want to be able to generalise your results.\nLuckily there is an existing comprehensive list (i.e. sampling frame) of all 246 existing public primary care facilities in your district. You have a copy of this list, in the form of an Excel spreadsheet, that includes facility names, addresses and telephone numbers, plus additional information that we will use in subsequent exercises.\n\nOur aim is therefore to take a simple random sample of 50 facilities from the list.\n\n\n\nExercise: taking a simple random sample using Excel\n\nGo into the “Datasets” folder that you should have moved to a suitable folder on your computer and load the “Health facilities list - simple random sample.xlsx” Excel spreadsheet. If you haven’t downloaded the datasets and exercises folders and moved them to a suitable folder go to How to use this website.\n\nVideo instructions: taking a simple random sample using Excel\nWritten instructions: taking a simple random sample using Excel\n\n\nRead/hide\n\n\nIn the “Health facilities list - simple random sample.xlsx” Excel spreadsheet you will see it has various self-explanatory columns/variables including “facility_name”, “facility_address”, and “facility_tel”.\nIn column A (the blank column immediately to the left of the “facility_name” column) enter the following (or similar) word as a heading: random_no\nImmediately under this new column in cell A2 (the first row where the facility data starts) click on the cell and type =rand() and press enter. This Excel function generates a random number between 0 and 1 to six decimal places.\nThen simply ensure that cell A2 is selected (i.e. you’ve clicked on it) and then just double click on the small solid square at the very bottom right of this cell. This should copy and paste the function all the way down to the end of the facility data.\nNext click on the “random_no” column heading (cell A1) and then in the menu “ribbon” click on Data and click the Filter tool. You should see little “drop-down” menu buttons appear in the right of each column heading. Click on the drop-down menu button in the “random_no” column heading (cell A1) and select Sort Smallest to Largest.\nThis will immediately sort all the data in order from those in the same row of the smallest random number value to the largest. Therefore, the list will now be randomly sorted! Note: the random numbers will all change immediately after sorting so they will actually no longer be ordered. This is because they are functions and will get re-calculated each time you change anything. However, this doesn’t matter because once all data have been sorted based on the original random numbers we don’t need those original values anymore.\nYou can now simply take the first 50 facilities in the newly randomly sorted list as being your survey sample selected via simple random sampling (e.g. you could copy and paste the first 50 facilities into a new worksheet). If this was a real study you could then use the contact information to recruit and plan your data collection."
  },
  {
    "objectID": "2-probability-sampling.html#method-2-stratified-random-sample",
    "href": "2-probability-sampling.html#method-2-stratified-random-sample",
    "title": "",
    "section": "Method 2: stratified random sample",
    "text": "Method 2: stratified random sample\n\nA brief overview of the method\nSee the lecture for full details on stratified sampling. In summary though, stratified random sampling involves the following three main steps.\n\nDefine your strata. Strata are simply a set of two or more mutually exclusive and comprehensive groups that cover all your sampling frame’s units of observation. This just means that every unit of observation in your sampling frame is a member of one and only one stratum. For example, if we were sampling individuals and had data on their ages we could stratify the them (i.e. the sampling frame) based on age, most simply by splitting all individuals into two age groups, say those aged <18 and those aged 18 years or more. We will see below what to consider when selecting strata. Note: the singular of strata is stratum, e.g. “we create many strata but sample each stratum separately”. Note: you can define n strata for any single stratification variable, and your total strata will be the product of the number of strata created for each variable. For example, if you stratify based on age, grouped into <18s and ≥18s, and sex, grouped into male or female, you have 2 x 2 = 4 strata in total. As you can see the total number of strata therefore increases rapidly with every extra stratification variable and/or group added! Note also: you cannot create a strata with no units of observation in. For example, if there were no <18 men in your sampling frame you could not create an <18-male strata group as the analysis would not work.\nDecide on the sample size for each strata. There are two different versions of stratified random sampling: one that uses “proportionate stratification” and one that uses “disproportionate stratification”. Proportionate stratification means that the sizes of your sample’s strata are proportional to the size of the strata in the target population. For example, using the example above of stratifying by age with two groups of <18 and ≥18: if 25% of the target population were aged <18 (and therefore 75% are aged ≥18) whatever your sample size was 25% of the sample size would come from your <18 stratum and 75% from your ≥18 stratum. This would result in a representative distribution of ages in your sample and preserve the equal probability of selection for all units of observation in your sampling frame. Disproportionate stratification is when the size of your sample’s strata is not proportional to their size in the target population. If this is the case then the units of observation in your sampling frame no longer have an equal probability of selection. This means you would have to calculate sampling weights to “map” the same back to the target population and avoid biased results when analysing the data.\nTake a simple random sample (or less commonly a systematic random sample) of the relevant size in each strata.\n\n\n\nAdvantages of stratified random sampling\nThe advantages and uses of stratified random sampling differ somewhat depending on whether you are using a proportionate or disproportionate stratification, and the reason for doing either depends on your goals and skills.\n\nFor studies aiming to describe the characteristics of a target population, where you have no particular interest in any specific sub-populations (compare to scenario 2 below), compared to using simple random sampling stratified random sampling with proportionate stratification can help you to: a) reduce the chances of obtaining an unrepresentative sample, at least in terms of the characteristics represented by your chosen strata, and b) increase the precision of your estimates for a given sample size. While for studies aiming to estimate relationships within a target population, where you have no particular interest in any specific sub-populations, compared to using simple random sampling stratified random sampling can similarly help you to increase the precision with which you can estimate your relationships of interest for a given sample size, giving you a more “statistically efficient” sample.\n\n\nHow well you achieve these goals depends on how well your chosen strata capture characteristics or variables that account for variation in your outcomes of interest. That is, you want units of observation to be as similar (homogeneous) as possible within strata and, on average, as dissimilar (heterogeneous) as possible between strata. For example, if we are interested in estimating rates of cardiovascular events then stratifying by age makes a lot of sense, because age is one of if not the biggest “causes” of cardiovascular events, i.e. the likelihood of having experienced a cardiovascular event will be quite similar for individuals within a young-age stratum and very different for those individuals compared to individuals in an geriatric-age stratum.\nNote: using proportionate stratification is not actually necessary to achieve these goals, but it results in a sample that does not need reweighting during analysis to avoid unbiased results, and calculating weights is complicated, plus there is typically no good reason to use disproportionate stratification in this case (again see scenario 2 below).\n\n\nIn other situations however you be particularly interested in specific sub-populations. In this case you can use stratification to “oversample” those sub-populations to ensure you have enough sample size to estimate characteristics/relationships for those sub-populations (strata) with sufficient precision/power. For example, you may wish to ensure you can estimate certain characteristics or relationships within a certain small, ethnic minority group with sufficient precision. With a simple random sample you would, on average, take a sample from the ethnic minority group that was proportional to its population size. For example, if the ethnic minority group were just 1% of the target population and you took a sample of 1000 individuals from the target population then on average you would only sample 10 individuals from the ethnic minority group! Hardly much use. Instead you could create strata for each ethnic group and take a fixed, larger (disproportionate) sample from the relevant ethnic minority group than you would take if you were using simple random sampling. This would be using disproportionate stratification.\n\n\nHowever, as noted earlier if you do this the added complication is that the relative sizes for one or more strata will then, by design, not match their relative sizes in the target population. This means there is not an equal probability of selection for all sampling units, and you would have to calculate and use sampling weights to “map” the sample back onto the target population and avoid obtaining biased results. As this is a more complicated process and this type of stratified sampling is not commonly used (although a form of it is commonly used in multi-stage cluster sampling) we will not look at it further. Note: when using disproportionate stratification you would still be able to gain the advantages mentioned for scenario 1 above if your strata, either those that are disproportionately sampled or indeed other strata, capture important sources of variation within your outcomes of interest.\n\nIn the following exercise we will just look at how to implement the first approach discussed above.\n\n\nLimitations of stratified random sampling with proportionate stratification\n\nYou require data on your chosen strata for all members of your target population, and it is often difficult and costly to obtain this data.\nCompared to simple random sampling it is a somewhat more complicated and time consuming process (although this is typically a minor limitation).\nWhen analysing data from a stratified random sample you need to use non-standard analytical methods (or non-standard versions of typical analytical methods) to account for and obtain the benefits of your stratified sample, in terms of increased precision. However, this is actually very straight forward to do with modern software and we will see how to do this in the complex survey practical sessions.\n\n\n\nScenario\nWe will use the same basic scenario as for the simple random sampling exercise previously, where we are aiming to conduct a survey of public primary care health facilities. However, in these exercises we will take a stratified random sample based on a single stratification variable that classifies each facility in terms of the sub-district that it’s in. We are therefore assuming that there is substantial variation in the characteristics we wish to estimate between health facilities within the sub-districts, and that it’s something we could easily have data on for all health facilities.\nIn the scenario/data there are just three sub-districts. We are also assuming that we are only interested in computing overall, district-level results. We will therefore need to split our sample size into three such that each sub-district’s sample size is proportional to the number of facilities in that sub-district to ensure our results apply to the overall district level.\nNote: in a real study if you had data on additional characteristics that you thought were likely to be related to variation in the outcomes of interest you would probably further increase your chances of getting a representative and more statistically efficient sample by creating additional strata using those data.\n\n\nExercise: taking a proportionate stratified random sample using Excel\n\nLoad the “Health facilities list - stratified random sample.xlsx” Excel spreadsheet.\n\nVideo instructions: taking a proportionate stratified random sample using Excel\nWritten instructions: taking a proportionate stratified random sample using Excel\n\n\nRead/hide\n\n\nIf we know there is substantial variation in health facility characteristics on average between sub-districts we may want to reduce the chances of getting an unrepresentative sample size for one or more sub-districts and increase the precision of our estimates (for a given sample size) compared to taking a simple random sample, which is likely to happen when the sample size is very small (like n = 50). As mentioned previously in reality we would probably want to create strata based on additional variables related to our outcomes, maybe things like some measure of facility size, staffing, or resources etc, but we will just keep things simple here.\nTherefore, instead of risking getting an unrepresentative distribution (number) of health facilities within each sub-district and less precise estimates, as would be likely with a simple random sample, we can take stratified random sample with strata sizes proportional to their relative population sizes.\nIn the “Health facilities list - stratified random sample.xlsx” Excel spreadsheet look at the Full facility list worksheet if it’s not already in view. Look at the sub_district frequency table again. Look at the “Percentage” column. This shows us the percentage of health facilities in each sub-district. We therefore need to work out how to split our overall sample size of 50 between these three percentages. Look at the Strata sample size: strata size proportional to population table below the last table. This gives us the required sample size for each strata. The calculations are very simple. We just multiply our overall sample size (50) by the proportion of the total number of health facilities in the district that each sub-district contains in turn, rounding results upwards to the nearest whole number (this will usually result in a slightly larger sample size than that originally planned). For example, 9% of all health facilities are from the “Hills” sub-district, so we can convert this percentage to a proportion by dividing by 100. Then the strata sample size calculation for the Hills sub-district is:\n\n\n50 x 0.09 = 4.5\n\n\nWhich we round to 5.\n\n\nTherefore, as a stratified random sample is just a repeated simple random sample within each strata all you now need to do is take a simple random sample of the relevant size for each sub-district, i.e. a simple random sample of 5 health facilities in the Hills sub-district and so on. To save you time because it’s not something you probably need to learn I’ve already created three additional worksheets (the tabs along the bottom of the Excel spreadsheet): one containing the data for each sub-district, so that it’s easy to take a separate simple random sample for each sub-district. Therefore, based on the required sample size for each sub-district try and repeat the simple random sample process learned in the last exercise for each of the sub-district lists in turn. Refer back to that exercise for the steps if needed. Note: you could actually also achieve a stratified random sample with proportional strata sizes, maybe more simply, by just randomising the total list of all health facilities and then selecting the first n health facilities from each sub-district as they appear in the randomised order, where n = the required sample size for each strata."
  },
  {
    "objectID": "2-probability-sampling.html#method-3-systematic-random-sampling",
    "href": "2-probability-sampling.html#method-3-systematic-random-sampling",
    "title": "",
    "section": "Method 3: systematic random sampling",
    "text": "Method 3: systematic random sampling\n\nA brief overview of the method\nSystematic random sampling is very similar to simple random sampling in terms of the samples it produces, and the same “versions” of typical analytical methods can be used when analysing data from either type of sampling method. The basic process is as follows.\n\nTake your sampling frame and select a random starting point (i.e. random unit of observation).\nBased on your desired sample size calculate a “skip pattern”. This is just a number which then determines how many units of observation are skipped after your starting point before sampling another unit.\nSample your random starting point and based on your skip pattern all successive units of observation that your skipped sampling pattern “lands on” until you reach the end of your sampling frame, by which time you should have sampled your desired sample size (or just under - see the exercise).\n\n\n\nAdvantages of systematic random sampling\nStudents often find systematic random sampling somewhat confusing in terms of its advantages over simple random sampling and stratified random sampling. This is not surprising as the advantages are typically not very clear.\n\nThe main advantage that systematic random sampling has over simple random sampling is that it can reduce the chances that you will obtain a non-representative sample, but only when the sampling frame is ordered or stratified in some statistically meaningful way in relation to the characteristics or relationships of interest. For example, if you are surveying individuals from a very heterogeneous (mixed) community where individuals are from a variety of different ethnic groups, and getting a sample that is representative of the distribution of ethnicities is critical, then a simple random sample may often result in an unrepresentative distribution (proportional mix) of ethnicities in your sample when the sample size is small. Systematic random sampling could improve your chances of obtaining a representative proportional mix of ethnicities though if you have data on individuals’ ethnicity. This is because you can then order your sampling frame by ethnicity and systematic random sampling will, by its systematic nature, ensure that a proportionally “even spread” of individuals across all ethnic groups are selected. This results will be very similar to a sample obtained via stratified random sampling with strata sample sizes proportional to strata population sizes, and arguably the sampling method is slightly less complicated than for such a stratified sample.\n\n\n\nDisadvantages/limitations of systematic random sampling\n\nSystematic random sampling is arguably slightly more complicated to implement than simple random sampling.\nAs with stratified random sampling you need additional data on important characteristics of your units of observation that are related to the characteristics or relationships of interest to obtain a clear benefit from this approach, and such data is often difficult, costly or impossible to obtain.\nMost critically, if there is a meaningful pattern to the ordering of your sampling frame in terms of the distribution of characteristics related to the characteristics or relationships of interest then a systematic random sample may produce a seriously biased result. This is usually the case when the pattern is “small scale”. For example, if you are using systematic random sampling to sample from a sampling frame of hospital patients and the list is ordered by the time of day the patients first arrived then your skip pattern may mean you only select individuals who came early in the day. This might mean you hugely oversample the unemployed (who can only come to hospital early in the day), such as retired individuals and caregivers - hardly likely to be a representative sample. This is obviously most problematic when the pattern is not obvious.\n\n\n\nWhen to use systematic random sampling?\nClearly systematic random sampling can have some advantages over simple random sampling if used carefully, but what about in relation to stratified random sampling? Remember stratified random sampling can use either simple or systematic random sampling to take the samples within each strata. Therefore, it depends on your aims and there’s no single answer for all circumstances. However, broadly speaking if you have good data on what are likely to be important strata for your characteristics or relationships of interest then a stratified random sample using simple random sampling within strata will probably be the better choice in my opinion. This is because compared to simple random sampling stratified random sampling is more likely to produce a representative sample while also being likely to increase the precision of your estimates (i.e. increase your statistical efficiency) compared to systematic random sampling, but it also avoids the risk of producing a biased sample that systematic random sampling can result in when there are unrecognised, typically small-scale, statistically meaningful patterns in the sampling frame.\nHowever, for completeness we will practice below how to take a systematic random sample when we have some additional data on important strata.\nLastly, note that one form of systematic random sampling is often used in the first stage of multi-stage cluster samples to take a non-stratified random sample of primary-stage clusters (often villages or city blocks) with probability proportional to the size of primary-stage clusters. This involves a slight modification of the approach we will see below, and as it’s typically only used in this specific circumstance we won’t look at it further.\n\n\nScenario\nWe will use the same basic scenario as for the simple random sampling and stratified random sampling exercises previously, where we are aiming to conduct a survey of 50 public primary care health facilities. However, in this exercise we will take a systematic random sample. We will use the stratified random sampling exercise sampling frame where the health facilities were ordered into sub-district groups. As long as there is no smaller scale pattern in the ordering of the list a systematic random sample should ensure a random sample of health facilities while also ensuring the sample is evenly distributed across the three sub-districts.\n\n\nExercise: taking a systematic random sample using Excel\n\nLoad the “Health facility list - systematic random sample.xlsx” Excel spreadsheet.\n\nVideo instructions: taking a systematic random sample using Excel\nWritten instructions: taking a systematic random sample using Excel\n\n\nRead/hide\n\n\nTo save time we have already created a numerical sequence in the first column from 1 to 245. This will be used to identify our sampled health facilities.\nCalculate the skip pattern k. This is calculated as N/n, where N = total sampling frame size and n = sample size. If the result is a whole number (integer) use this value, or if not round up to the nearest whole number. Therefore, in a blank cell enter =245/50 and press enter. You should get the value 4.9 and so our skip pattern k = 5.\nSelect a random starting point between the first unit of observation and that corresponding to our skip pattern k, i.e. a random number between 1 and 5. To do this use the Excel function randbetween.\nIn cell F1 enter the heading “facility_selection_id” or similar. Then in cell F2 click and enter =randbetween(1, 5) and press enter to create a value between 1 and 5 for the random starting point.\nThen click on cell F3 and type =F2+5. Then press enter and then click again on cell F2 and then double click on the small solid square at the bottom right of the selection box that appears around cell F2 to extend the formula down to the bottom of the data. The is then your sampling list up to the point where the values become ≥245. You may notice that this actually only selects 49 health facilities. This is just due to the rounding in the maths and won’t always be the case, but when you end up sampling one too few health facilities you can just randomly select another health facility by going back to the start of the list once you reach the end (e.g. for our skip pattern of 5 if your final selected facility was number 244 then we’d count 245, 1, 2, 3, and then number 4 would be our final selected facility)."
  },
  {
    "objectID": "2-probability-sampling.html#single-stage-cluster-sampling-and-multi-stage-cluster-sampling",
    "href": "2-probability-sampling.html#single-stage-cluster-sampling-and-multi-stage-cluster-sampling",
    "title": "",
    "section": "Single-stage cluster sampling and multi-stage cluster sampling",
    "text": "Single-stage cluster sampling and multi-stage cluster sampling\nThe following optional information is just for awareness and understanding purposes but we will not practice either of these methods.\n\n\nRead/hide\n\n\nCluster random sampling\nCluster sampling involves sampling higher-level units of observation, such as households, schools, villages etc, which contain your lower-level units of observation, typically individuals. If you then also sample units within clusters (i.e. not every unit is selected) then you are using some form of multi-stage cluster sampling method, and that is too complicated for us to look into further, but see below for a bit more detail. However, if you are sampling clusters and then selecting all eligible units within each sampled cluster for data collection you can simply use any of the methods previous covered to sample your clusters. For example, if you wanted to survey community members in a number of communities and you could construct a sampling frame listing all the households in each community you wanted to survey, but not the household members, then you could take a simple random sample of households from your list and then select all eligible individuals within every sampled household for data collection. Or you may wish to stratify the sampling if you also collected data on, say, the total size of each community etc.\nEither way this would result in a representative sample (on average) with no need to re-weight the data, unlike if you had also sampled individuals within households. However, when you take a cluster sample you need to account for the clustering in your analysis. This is because standard methods of analysis assume observations (i.e. data points) are statistically independent from one another, but clearly individuals within the same household etc are not independent, and they therefore do not provide the same amount of statistical information about a population as fully independent observations would. Therefore, ignoring clustering in analyses results in falsely high levels of precision/power. We will see one way that you can account for clustering in the complex survey practical sessions.\n\n\nMulti-stage clustered random sample\nMulti-stage clustered random sampling is far too complicated to go into for this module, but we will see how to analyse data from multi-stage clustered samples in the complex survey practical sessions. It is typically only used by large-scale household surveys, such as the Demographic and Health Surveys by USAID and its in-country partners. Such projects almost always use professional statisticians or survey methodologists. In brief though, this approach is actually some combination of the earlier methods covered above, and usually combines a first stage systematic random sample, with probability proportional to size to select primary sampling units (usually census units, often called “enumeration areas”, that correspond to villages or city blocks), with a second stage of sampling of households, which either again uses systematic random sampling, but typically with equal probabilities of selection, or a simple random sample. However, there are many variations with possibly additional levels of sampling. This means that the probability of selection of the ultimate sampling units is never equal with such methods and complicated sampling weights need to be calculated to ensure analyses produce unbiased results."
  },
  {
    "objectID": "3-sample-size.html",
    "href": "3-sample-size.html",
    "title": "",
    "section": "",
    "text": "In this section we will be looking at how to calculate the sample size required for a quantitative research study where there the main research question or goal requires statistical inference. For example, we may need to know how many health facilities to sample from a region to be able to describe one or more key health facility characteristics in that region to a given level of precision. Or for a randomised controlled trial we may want to know how many individuals to sample from a given population to give us a specific chance (i.e. probability) of being able to detect a statistically significant difference in our primary (key) outcome between our intervention and control groups.\nIf you are comfortable with the concepts and basic theory around sample size calculations covered in the lecture you can skip straight onto the instructions for how to carry out sample size calculations using a web-based tool. Just click on Confidence interval approach for single variables using OpenEpi on the navigation bar on the left. However, if you feel unsure in any way about these issues then it is best to take some time to first refresh yourself about them before below.\n\n\n\nAlmost every quantitative research study involves one or more research questions (usually the most important ones) that require statistical inference to answer. In brief, statistical inference involves sampling members from a population (ideally using a probability sampling method), collecting data from that sample, and then analysing that data using statistical analyses to draw conclusions about the population. There are two main types of research questions we are usually interested.\nFirst, description. For example, what is the prevalence of COVID-19 in a given population at a given time point? Here, we would call the true (but unknown) prevalence in the population at that time point a population parameter. We would then take a sample from the population and estimate the likely value of this population parameter using our study sample data via a sample statistic (the sample prevalence, i.e. the percentage of individuals in the sample with COVID-19), combined with some measure of how precise this point estimate is likely to be of the population parameter. Usually this measure of precision would be a confidence interval.\nSecond, casual inference. For example, what effect does one dose of a given COVID-19 vaccine have on the probability of suffering from COVID-19 in the six-months following vaccination in a given population? Here, we might seek to measure the causal effect in terms of the average difference in the probability of suffering COVID-19 in the six-months after vaccination in a randomly selected individual who had the vaccine compared to if they had not had the vaccine. To estimate this population parameter we might take a sample of individuals from the population and run an RCT with, measuring the difference in the proportion of individuals randomly given the vaccine (intervention) who suffer COVID-19 in the six-months following vaccination compared to the proportion of individuals randomly not given the vaccine (control).\nWe would then again combine this sample statistic with some measure of how precise this point estimate is likely to be of the population parameter. Again, usually this measure of precision would be a confidence interval. However, although our estimate and confidence interval provides the best understanding of the likely direction, size and precision of the causal effect (population parameter) in the population, the dominant approach (at least for RCTs) is to base conclusions about the likely existence of any causal effect (as opposed to any difference being due to sampling error) on a null-significance hypothesis test. We might therefore use an appropriate hypothesis test to test how unlikely it would be to observe an effect at least as great as the one observed if we assume that there is actually no difference in the probability of suffering COVID-19 between our two groups. Then, if the corresponding p-value were less than 0.05 we may conclude the difference is likely to represent a true causal effect of the vaccine in the given population.\nSo where does sample size come into this? When we seek to make statistical inferences, like in the examples above, the sample size can be loosely thought of as simply the number of units of observation or sampling units that we need to give ourselves a reasonable chance of answering our research question satisfactorily. The units of observation may be individuals, health facilities etc, or they may be individuals, health facilities etc at successive time periods, depending on the study design.\nIn the sample size calculation we define what we mean by reasonable chance/satisfactorily, and the reason we cannot guarantee that we will answer our research question in an inferential study is because we can only make probabilistic conclusions. This is because we are basing our inferences on samples, which may or may not be representative of our target population due to sampling error (and that’s ignoring other sources of bias/error).\nMore formally, you make a series of assumptions, such as what level of variation in an outcome we expect to get in our sample, and then a sample size calculation can, in theory, tell you how large a sample size you need to get results with sufficient precision or power (we’ll come back to these concepts shortly). However, the validity or accuracy of a sample size calculation depends entirely on the validity/accuracy of the assumptions, as we’ll discuss. And whether the results can be validly generalised to the target population, assuming the internal validity is perfect, depends on the representativeness of the sample. As always the important thing is to think carefully and critically when planning your sample size, and not just mindlessly plug in some optimistic values.\nThere are two main approaches typically used for calculating sample sizes for quantitative studies which can be thought of as:\n\nThe confidence interval or precision based approach.\nThe hypothesis testing based approach.\n\nIn practice they look quite different, and they do work quite differently in practice. However, they are actually very closely related, particularly in the underlying maths.\nIn this practical we will make use of the sample size calculation tools on the widely-used web-based epidemiology tool site called OpenEpi. Before we go any further open the website in your browser: https://www.openepi.com/Menu/OE_Menu.htm\n\nNote: there are many web-based sample size calculators, and many software-based calculators too. Indeed, SPSS, which we will be using for data analysis in subsequent sessions, has a sample size “module”. However, it doesn’t offer the confidence interval based approach for estimating means and proportions that I want to show you. OpenEpi is also freely available and easy to use.\n\n\n\n\n\n\nIn summary, this is where we calculate the sample size we need to estimate our summary statistic of interest (e.g. a mean, proportion, or linear regression coefficient) with a given level of precision, by which we mean calculate confidence intervals around our sample statistic of interest that are no wider/larger than a pre-specified size/range, assuming all the assumptions that go into the calculation are exactly true. If you need a reminder of the distinction/definition of sample statistics and population parameters see below.\nPopulation parameters and sample statistics:\n\n\nRead/hide\n\nRemember, population parameters are either the exact summary measures of a characteristics’ distribution in the target population, such as the exact mean age of individuals in the target population, or the exact summary measures of a relationship/association, such as the exact mean difference in systolic blood pressure between individuals aged <40 compared to individuals aged ≥40 in the target population. And we estimate the likely values of population parameters, i.e. make statistical inferences about their likely values, which we can rarely if ever know for sure, based on: 1) the equivalent sample statistics that we calculate from our sample, such as the sample mean age of individuals or the sample mean difference in systolic blood pressure between individuals aged <40 compared to individuals aged ≥40, and 2) the associated confidence intervals around those statistics. Or, if we are taking a hypothesis testing based approach to inference (see below), we estimate whether the population parameter of interest is likely to differ from some null hypothesis value based on a p-value calculated from the sample data.\n\nThis approach can be used for any sample statistic including measures of relationships such as differences in means/proportions or regression coefficients, but the confidence interval approach is mainly used when the aim is to describe population characteristics (something we will look at in section 4.2). This is typically in the context of a cross-sectional/repeated cross-sectional/longitudinal survey generating descriptive outcome measures for numerical or categorical variables in terms of means and proportions respectively. Note: any categorical outcome can be analysed as a series of binary outcomes based on each unit of observation having or not having the characteristic represented by each level of the categorical variable. For example, when sex is coded as having two levels, male and female, each level can be analysed as a binary variable, i.e. the proportion of individuals who are male (implicitly compared to the proportion who are not-male, i.e. female), or vice versa. Or for the categorical variable religion where we code it as having three levels, none, Christian, Muslim, we can analyse this as three related binary variables: religion-none = yes/no, religion-Christian = yes/no, and religion-Muslim = yes/no. Therefore, in many surveys most outcomes will be categorical variables (particularly from self-responses), and if there is no clear primary outcome the sample size is often based on obtaining a level of precision for a generic binary variable, when all categorical variables will be analysed as a series of binary variables (i.e. inferential statistics, namely confidence intervals, will be calculated for each binary variable).\n\n\n\nWhy is the confidence interval approach rarely used for analytical studies?\n\n\nRead/hide\n\nThe lack of use of the confidence interval approach in these situations is probably because of the dominance of the null hypothesis significant testing (NHST) approach to analytical inference, which we’ll look at next, whereas with cross-sectional survey studies the main aim is often estimating a range of characteristics to a given level of precision rather than testing hypotheses. And as we will see below the when using a NHST approach then the more natural/logical sample size calculation approach is the hypothesis testing approach to sample size calculations. However, I would argue that given the benefits of using confidence intervals for making statistical inferences rather than NHST tests, we should make use of the confidence interval sample size approach as the norm.\n\n\n\n\nOpenEpi only offers the confidence interval approach for the situation where you are estimating a mean or a proportion, but other sample size software allows you to use this approach for a wider range of outcome measures (e.g. rates).\n\n\n\n\n\n\nThe hypothesis testing approach is primarily used for analytical studies that are trying to understand whether a given relationship exists. It is usually the case that the relationship of interest will be studied and analysed in terms of a difference in a continuous or binary outcome between two groups, which are usually independent groups but they can be related. However, the relationship may also be analysed in terms of a linear regression coefficient for a continuous independent variable, a logistic regression odds ratio, or other similar measure, but these are much less commonly seen. Also, as we explain below, in theory this approach can also be used for descriptive studies, but this is very rarely done. As these other approaches/scenarios are rarely/very rarely used we won’t look at them further and we below will just assume we are considering the approach where our relationship of interest is analysed in terms of a difference in a continuous or binary outcome between two independent groups.\nThe hypothesis testing approach is arguably harder to understand well than the confidence interval approach. It may be easiest to understand once you understand the process. Therefore, we will explain it in summary here by detailing the key steps you go through.\nFirst, we pre-specify a target difference that represents the smallest difference in our outcome between the two groups that we want to be able to detect. This target difference also explicitly or implicitly incorporates our assumption about what level of variation will exist in the outcome variable in the sample data. We also pre-specify the threshold below which we would declare a p-value from a NHST test of the null hypothesis that there is no difference in the target population “statistically significant”. That is, the threshold below which we would reject the null hypothesis and assume that the alternative hypothesis of a difference existing in the target population was more likely. This is usually the conventional P≤0.05 level.\nThen the last main assumption is maybe harder to understand. Here we pre-specify the probability that we will obtain a p-value from a NHST of the the null hypothesis that is statistically significant based on our chosen threshold for significance (i.e. equal to or below our chosen threshold). Technically speaking this probability is the proportion/percentage of the time we would obtain a p-value from a NHST of the the null hypothesis that is statistically significant, based on our chosen threshold for significance, if we were to repeat the study an infinite number of times. The reason we can’t guarantee we will get a statistically significant p-value even if all our other assumptions are correct is because of sampling error. Even if the true difference in the outcome between the two groups that exists in the target population is equal to (or larger) than our pre-specified target difference, in any given randomly selected sample we might, due to sampling error, select a sample where the difference in the sample doesn’t reflect the difference in the target population and is in fact smaller. However, we can ensure that this only happens a given proportion of the time.\nThen conditional on our assumption about the target difference in the sample data being exactly true, and conditional our assumption about the level of variation in the outcome in our sample data being exactly true, and conditional on their being no bias/error in the study and analysis other than sampling error, the resulting calculation will tell us what sample size we need to obtain to ensure that we have the pre-specified probability that we set of obtaining a statistically significant p-value when testing the null hypothesis.\nUnfortunately there is quite a lot of technical terminology associated with this approach that we haven’t used yet for obvious reasons. However, we must familiarise ourselves with this terminology. First, the threshold at which we declare an effect statistically significant and reject the null hypothesis is known as the “alpha level” or the “level of statistical significance”, and the probability that we will be able to detect our target difference if it exists via getting a p-value less than our level of significance is known as the “power” of the hypothesis test. The power is also equivalent to 1 - “beta” (i.e. “1 minus beta”), where beta is the probability (in the long-run) of getting a false negative result, i.e. the probability of not rejecting the null hypothesis when it is actually false. Therefore, power is the probability of correctly rejecting the null hypothesis when it is false in the long-run assuming all assumptions that go into the sample size calculation are true.\nYou may be wondering why we need to pre-specify values for the alpha level and the power. Why not just set both to their maximum so that we always obtain a statistically significant result? The short answer is there is a trade-off with the resulting required sample size, and by convention researchers typically therefore set alpha to 0.05 (or less commonly 0.01) and power to 0.8 (or less commonly 0.9). See below for more details.\nWhy can’t we always minimise our alpha level and maximise our power?\n\n\nRead/hide\n\nMore specifically, alpha is also the false positive rate of a hypothesis test, or the rate at which we will falsely declare a difference statistically significant in the long-run when conducting null hypothesis significance tests in a given scenario. Therefore, we want this to be as small as possible to avoid making mistakes, i.e. falsely declaring there to be a statistically significant difference, which we interpret to mean there is likely to really be a difference in the target population. However, the smaller we set the alpha level the larger the sample size required to detect any given difference as statistically significant for a given power (i.e. the larger the sample size required to detect any given difference as statistically significant with a given probability). Similarly, the higher we set the power level the larger the sample size required to detect any given difference as statistically significant for a given alpha level or level of statistical significance. This trade-off is simply a function of the maths underlying hypothesis testing approach sample size calculations: we need more statistical information and therefore a larger sample size to both reduce how often we make false positive decisions and increase how often we make true positive decisions from NHST. Similarly, for a given alpha and power level we require more statistical information or a larger sample size to detect as statistically significant smaller and smaller target differences (assuming they exist).\nTherefore, for any given target difference we clearly want to minimise our level of statistical significance to reduce our false positive rate while maximising our power level to increase our chances of detecting statistically significant differences (assuming they exist!), while not requiring an unfeasibly large sample size. Consequently, typical values of alpha used in almost all sample size calculations are either 0.05 (the common statistical significance threshold) or less commonly 0.01 (i.e. 5% or 1%), while typical values for the power are 0.8 or less commonly 0.9 (i.e. 80% or 90% power). Very broadly speaking, these values typically allow you to come up with a feasible sample size calculation for not “unreasonably” small target differences. However, these are not magic values and they are very much arbitrary conventions with no logical or natural basis for them other than the fact that people like round numbers, and a 5% false positive rate (0.05) and an 80% chance of detecting a difference as statistically significant if it exists seemed like “reasonable” values to researchers who have gone before us, given how much these two values/assumptions “cost” in terms of the required sample size.\n\n\n\n\nHypothesis based approach to sample sizes for descriptive studies:\n\n\nRead/hide\n\nThis hypothesis testing based approach can also be used for estimating sample sizes related to sample statistics that measure characteristics, e.g. means and proportions as estimated in a cross-sectional survey, if you wanted to test hypotheses about whether those characteristics differ from a given null hypothesis value (i.e. an assumed population value). However, this approach is almost only ever used to calculate sample sizes required for primarily analytical studies where the main research questions are about associations/relationships, and then this is usually typically further restricted/framed just in terms of differences between two groups, e.g. differences between two means or two proportions, where the intention is to use NHST to see whether the observed sample difference differs significantly from the assumed null hypothesis difference (usually of no difference).\n\n\n\n\nOpenEpi offers the hypothesis testing approach for differences between independent means and proportions and paired means and proportions. Here we just cover sample size calculations for hypothesis tests of differences between independent means and proportions, because the paired situation is more complicated and should be avoided unless you know what you are doing or have assistance from a statistician. Note: in the literature the term “sample size calculation” is often used simply to mean the hypothesis testing approach, because outside of cross-sectional studies the confidence interval based approach is so infrequently used given the dominance of null hypothesis significance testing.\n\n\n\n\n\n\n\n\nYou work for a regional ministry of health in a region where public health facilities have reported increasing numbers of patients seeking treatment for cardiovascular diseases over the last decade. However, there is no good data on the cardiovascular health of the population. Therefore, your department has tasked you with conducting a population survey on the cardiovascular health of the population. As it is hard to measure actual cardiovascular health you will focus on systolic blood pressure as a key proxy indicator or risk factor for cardiovascular health. The primary aim of the survey is therefore to estimate the distribution of blood pressure, specifically systolic blood pressure (mmHg), values within the target population, along with collecting other relevant health and socio-demographic data. As the primary or key outcome variable is systolic blood pressure and your aim is to estimate the distribution of this outcome in the target population a confidence interval based approach to the required sample size makes most sense. This is because this approach will allow you to plan a sample size that will enable you to estimate the distribution of the outcome to a certain level of precision (i.e. to estimate it with a certain maximum confidence interval width).\n\n\n\nWhen estimating the sample size required to estimate a mean with confidence intervals of a maximum desired width there are just three assumption inputs to consider.\n1. What confidence level do you want for your confidence interval?\nThe convention is a 95% confidence interval, and unless you have a good reason to choose otherwise use this.\n\nTherefore, we will use 95%.\n\n2. What standard deviation (SD) is your outcome variable expected to have in your sample data?\nThis may be estimated from values in the literature from similar studies, or from pilot data (although this is risky as pilot studies by definition are small and cannot produce unreliable estimates of any sample statistics), or you can use the following rough rule of thumb:\n\nTake the range of values for your outcome that roughly 95% of the population are likely to fall between/within. Divide this by 4 for a conservative estimate of the population SD for the outcome.\n\nFor our example, in our scenario we might assume, from clinical knowledge/existing literature, that about 95% of people in our target population have systolic blood pressure values between 80 and 150 mmHg. Therefore, the expected range is 150 - 80 = 70. We then divide this by 4: 70/4 = 17.5. You should then round this up for safety to at least 18, although for greater safety you might round up further (say to 20). The larger the assumed SD the larger the sample size required, but the safer you will be because you have less chance of finding out that the SD in your sample is actually higher than you assumed, which could mean you won’t then achieve your desired precision level. Note: this rule of thumb only applies to variables that are, at least approximately, normally distributed. See the following paper for an improved but slightly more complicated approach to estimate SDs: https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-135\n\nTherefore, we will use 18 mmHg.\n\n3. What is the minimum level of precision that you want your confidence intervals to have?\nThis should be based on practical considerations, such as what level of precision will be useful for users of the results of the study such as clinicians, health administrators, and policy makers etc. Here we will assume that our result would only be judged to robust and useful by clinicians if our confidence intervals are a maximum of +/- 2.5 mmHg. This means that whatever mean we estimate we want the confidence intervals to be no wider than that mean plus 2.5 mmHg and that mean minus 2.5 mmHg. Note: the desired confidence interval precision does not depend on the likely/assumed value of the outcome. Also note: we have defined the precision of our desired confidence interval in terms of the “half-width” of the confidence interval, which is just the upper confidence interval value minus the lower confidence interval value (i.e. the confidence interval range) divided by 2. You will also often see this half-width referred to as the “margin of error”: https://en.wikipedia.org/wiki/Margin_of_error\n\nTherefore, we will set our confidence interval half-width to 2.5 mmHg.\n\n4. What response rate do you expect?\nIt is very rare to achieve a 100% response rate, so typically you should use previous values in the literature, if they exist, along with any pilot data, and past experience, to decide on a likely response rate. This should be a conservative/safe assumption, i.e. round down a “sufficient” extent, because experience shows researchers typically overestimate the response rate. OpenEpi does not allow you to automatically adjust the sample size for the response rate so we have to do this manually, but we can do this very easily as follows:\n\nSample size adjusted for <100% response rate = desired sample size (i.e. the result from the calculation, which assumes a 100% response rate) / assumed response rate as a proportion.\n\nFor example, if we did our sample size calculation and we got a required sample size of 92, then if we assume we will likely only get a response rate of 90% or higher (0.9 on the proportion scale, i.e. 90/100), then we actually need to aim to collect a sample size of 92/0.9 = 103 (rounding up). So we need to approach 103 people to end up with at least 92 who participate (assuming our response rate assumption is accurate).\n\nNote: while this will ensure that you have at least 92 individuals and that you achieve your desired level of precision, if peoples’ participation is related to their values of the outcome of interest then recruiting more people won’t stop your results from suffering response bias. Increasing the sample size will increase the precision of the estimate but it can never reduce any such bias! So you can certainly have a very precisely estimated (narrow confidence intervals) but very biased result.\n\n\nTherefore, we will assume a response rate of 0.9.\n\n\n\n\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size required to estimate a population mean with a given level of precision in OpenEpi\n\n\nRead/hide\n\n\nOn the OpenEpi website from the navigation menu on the left click the tiny “+” symbol next to the Continuous Variables folder to open it. Then click on Mean CI. Now either click on the Enter tab at the top of the screen of the Enter New Data button at the top below the Open Soruce Statistics for Public Health title.\nNow let’s enter our assumptions/the calculation parameters. In the form titled Confidence Intervals for a Sample Mean in the first row opposite Sample Mean enter our value for the assumed sample mean as 0. Just click on the existing value of 150 and type 0. However, note that for this type of sample size calculation it doesn’t actually matter what assumed mean we enter, as the confidence intervals won’t change (so you could equally enter any other value). Next on the Sample Std. Deviation row we will enter a value for the assumed SD as 18. You can also instead enter a value for the assumed standard error or variance (i.e. the square of the SD), but we made an assumption about the SD earlier so we will use that.\nNow we want to know what sample size we need to estimate our population mean such that the 95% confidence intervals are of a given width, specifically ±2.5 (mmHg). Unfortunately, one limitation of OpenEpi is that for this type of sample size calculation you can’t ask for the sample size directly. The way they have setup the calculator is that you give it an assumed SD, a desired confidence level and an expected sample size, and then it tells you the precision or width of the cofidence intervals you would obtain when estimating the mean. Therefore, we need to work backwards and try out different sample sizes until we find one that will give us our desired confidence interval width.\nLet’s just start with a sample size of 100, so on the next row opposite Sample Size enter 100. We can leave the assumed Population Size as effectively infinite. We can also leave the desired confidence interval confidence level as 95%.\nNow click the Calculate button above the Confidence Intervals for a Sample Mean form. This takes us to the Results tab. It shows us our input data (our assumptions), and then below 95% Confidence Limits for the Mean of 0 it tells us the lower and upper limits (i.e. interval values) we would get based if we calculated the confidence intervals assuming the data follow a normal distribtion (see the values on the row Based on: z-test) or a t-distribution (see the values on the row Based on: t-test). We will use the t-test row values as the t-distribution is best when you can assume your outcome is approximately normal but you may only have a relatively small sample size, as the t-distribution allows for more variation at smaller sample sizes and becomes identical to the normal distribution at large sample sizes. We will also see how to estimate population means using the t-distribution in section 4.2. Population characteristics.\nWe can see our t-based 95% confidence intervals would be -3.6 and 3.6 (for normal or t-based confidence intervals they will always be symmetrical) for our initial assumptions. We want greater precision though: specifically 95% confidence intervals with width ±2.5. So we can go back to the Enter tab and our assumptions will be the same as before. So let’s try changing the assumed sample size to 200 (we want more precision, which requires a larger sample size).\nGreat! We’re nearly there. If we want to be really precise then we the sample size that gives us confidence interval limits of 2.5 exactly or just under though, not just over, so we can play about with some slightly higher values. If you do so you will find that a sample size of 202 gives us what we want.\nFinally, we need to adjust our sample size for our assumed response rate of 90%. Remember we just need to divide our sample size assuming a 100% response rate by our assumed response rate on the proportion scale:\n\n\n202 / 0.9 = 244.4.\n\n\nSo, rounding up, our final required sample size is 245.\n\n\nTherefore, assuming the expected population SD is 18, and employing the t-distribution to calculate our confidence intervals, and assuming a response rate of 90%, the study would require a sample size of 245 to estimate the population mean systolic blood pressure (mmHg) with 95% confidence intervals of width ± 2.5.\n\nRemember though, if you achieve the required sample size you will only achieve your desired level of precision if the other assumptions in the calculation are accurate, i.e. if the SD of the outcome in the sample equals the assumed SD. If the actual SD in the sample data is larger than the pre-specified expected value then the precision you achieve, i.e. the half-width of the confidence intervals around your estimated mean, will be larger than your desired level of precision. Note: the opposite is also true, i.e. you’ll get better precision than expected if the SD turns out to be smaller than expected. Also, irrespective of the level of precision, the estimated population mean will only be unbiased on average if there is no systematic difference in outcome values between non-responders and responders, and if there are no other sources of bias impacting the data. Similarly, if our sample is not taken via a probability sampling method then formally we cannot be sure that the interpretation of our result applies to the population.\n\n\n\n\nWhat if you are estimating multiple means in a quantitative survey? For example, for our scenario where we are conducting a survey on cardiovascular health we might also measure salt intake, cigarettes smoked per day, BMI etc. Then you probably have two main approaches depending on the situation. First, if there is a clear, primary research question/objective then you can base the sample size of the outcome that allows you to answer that research question/acheive that objective. For example, if the primary aim of our survey was to estimate the distribution of systolic blood pressure in our target population then we could base the sample size on this outcome alone, and essentially hope that this is also a sufficient sample size to estimate our other numerical outcomes with sufficient precision. Second, if there is no clear, primary research question/objective then decide which outcomes you need to achieve a given level of precision for when estimating their distribution, and simply choose the sample size that is largest. That way you ensure that you will achieve at least sufficient precision for all your outcomes. For example, if we found we needed a sample size of 100 to estimate our systolic blood pressure outcome with sufficient precision and a sample size of 150 to estimate our salt intake outcome with sufficient precision, and these were our two key outcomes, then we would aim for a sample size of 150.\n\n\n\n\n\n\nYou work for the regional ministry of health and the ministry leaders wish to know how frequently the public primary care facilities in the region run out of one or more drugs on the essential medicines list (known as a drug stock-out). They therefore plan to conduct a cross-sectional survey of public primary care facilities in the region and record whether each facility ran out of one or more drugs on the essential medicines list within the last month of the survey or not. Therefore, the outcome is binary (yes/no) and is most naturally summarised as a proportion/percentage. The ministry of health want a clear answer on this issue so they want to be fairly certain of the proportion of public primary care facilities across the whole region that have experienced such a drug “stock-out” within the last month. After some discussion it is agreed that you will try to give them an answer to this percentage within ± 5 percentage points.\n\n\n\nWhen estimating a proportion for a binary outcome there are also three assumptions to input when calculating your sample size based on confidence intervals.\n1. What confidence level do you want for your confidence interval?\nThe convention is a 95% confidence interval, and unless you have a good reason to choose otherwise use this.\n\nTherefore, we will use 95%.\n\n2. What is your expected prevalence/proportion/percentage?\nRemember, these are all equivalent measures but on potentially different scales, and you can covert between proportions and prevalences/percentages (prevalences are usually given as percentages, but can be given as proportions) by multiply/dividing by 100. We’ll just refer to the percentage from here on as that’s the scale that most people prefer to use.\nNote: unlike for a mean and for technical reasons related to the assumed probability distribution that a binary variable follows we do not specify an expected level of variation in the outcome like a SD, because for a binary variable the variation is assumed to be related to the mean, i.e. to the underlying probability or proportion. Therefore, we just need to specify the assumed proportion. As with the SD, you may either base your expected proportion on prior estimates from the literature, and/or from pilot studies, or if there is no information to go on then the safest (most conservative) option is to use an expected proportion of 0.5. This is because for any given sample size the confidence intervals you obtain when estimating a proportion will be widest around a proportion of 0.5, so if you assume a proportion of 0.5 then whatever level of precision you pre-specify you will guarantee that you will achieve that level of precision (conditional on obtaining the required sample size) or greater (if the proportion turns out to be <0.5 or >0.5. Again, technically this is because the probability distribution of a binary variable assumes that the variance is greatest when the underlying probability (i.e. proportion) is 0.5, and declines proportionally for values <0.5 or >0.5.\nHowever, an assumption of a proportion of 0.5 can be very conservative and result in a much larger sample size than might be necessary even if you are “playing it safe”. Therefore, it’s usually best to play around with this assumption based on your best estimate/guess and see how conservative you can go while still requiring a feasible sample size. For example, drug stock-outs are thought to be fairly rare and unlikely to occur in the previous month in more than 5 percent of facilities on average. Therefore, it doesn’t make sense to assume a proportion 0.5 as it’s very unlikely that you’d be this wrong, but to be conservative it is agreed you will assume a higher percentage of 10% to be the true frequency.\n\nTherefore, we’ll assume an outcome proportion of 0.1 (10%).\n\n3. What is the minimum level of precision that you want your confidence intervals to have?\nThe same considerations apply as for the case when estimating a mean, i.e. consider the practical implications and requirements of your results. However, unlike for continuous outcomes the level of precision for a binary outcome can be specified in absolute terms or in relative terms, i.e. relative to the expected proportion. For example, if we have an expected outcome proportion of 0.1 (10%) and we want our confidence intervals to be no wider than the range: lower confidence interval = 0.05 (5%), upper confidence interval = 0.15 (15%). Then on on the absolute scale of percentage points our half-width is 0.05 or 5 percentage points, because our confidence interval range is simply our expected proportion plus or minus 0.05, or our expected percentage plus or minus 5 percentage points. Note: loosely speaking percentage points are what you call differences between percentages when viewed on an absolute scale. If you are not clear on the difference between percentage points and percentage difference see here for more explanation: https://www.mathsisfun.com/percentage-points.html\nIn OpenEpi we just enter our desired confidence interval width on the absolute scale, but if you want to understand about what is meant by the relative scale you can read the below.\n\n\nRead/hide\n\nOn a relative scale our confidence interval half-width is 0.5 or 50%. This is because our confidence interval range is what we get when we take 0.5 or 50% of our expected proportion and add and subtract the result from our expected proportion:\n\n0.1 X 0.5 = 0.05.\n\nAnd then our lower confidence interval =\n\n0.1 - 0.05 = 0.05.\n\nAnd our upper confidence interval =\n\n0.1 + 0.05 = 0.15.\n\nIf you are struggling to understand all this then don’t worry it is confusing and takes a while to “get your head around”, so keep at it.\n\n\nTherefore, we’ll assume we want a confidence interval that goes from 0.05 to 0.15, so a width of 0.1 or a half-width of 0.05.\n\n4. What is your expected response rate?\nAs discussed for the estimating a mean situation if you expect <100% response rate you should adjust the sample size for the assumed response rate.\n\nTherefore, again we’ll assume a response rate of 90% (0.9).\n\n\n\n\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size to estimate a single proportion with a given level of precision in OpenEpi\n\n\nRead/hide\n\n\nFrom the OpenEpi navigation menu expand the Sample Size folder if required and click on Proportion. At the top go to the Enter tab. Then in the Sample Size for % Frequency in a Population (Random Sample) form we can leave the first row (Population size) as it is.\nChange the Anticipated % frequency(p) row value from 50 to 10 (i.e. our assumption that the population percentage is likely to be 10% or higher).\nNow we need to decide on our desired confidence interval width. Remember we want a confidence interval that, assuming the population percentage is 10%, goes from 5% to 15%, i.e. with a half-width of 5 percentage points. In the row Confidence limits as +/- percent of 100 the assumed value is already actually 5, i.e. 5 percentage points, so we can leave this as it is.\nYou can ignore the final row as this relates to the situation where you are using cluster or multi-stage sampling. You can read a very brief description of this issue at the end of this page. Therefore, you can now click the Calculate button.\nAgain, we then are moved to the Results tab and given a table with our inputs/assumptions listed. However, we are now given a list of results where each row indicates the required sample size (under the Sample Size heading) for a given confidence interval confidence level (the ConfidenceLevel(%) heading), even though we could select our desired confidence interval confidence level previously. Odd! Anyway, we only need to look at the first row as this relates to a 95% confidence interval.\nHere we can see we required a sample size of 139. Let’s adjust for our response rate:\n\n\n139 / 0.9 = 154.4.\n\n\nSo, rounding up, our final required sample size is 155.\n\n\nTherefore, assuming that the sample and population percentage of facilities experiencing a drug stock-out within the last month is 10%, and assuming a response rate of 90% in the survey, then the study would require a sample size of 154 to estimate the expected proportion such that the 95% confidence intervals of this estimate are ±5 percentage points (i.e. go from 5% to 15%).\n\nNote that this assumes you are estimating your population proportion based on 95% confidence intervals calculated via the “Wald method”, which assumes the outcome variable is normally distributed. Formally speaking this is not possible for a binary variable, such as you would use to estimate the proportion. However, when the proportion is not too extreme (>0.1 and <0.9) and the sample size is at least 30, this normal approximation works fairly well compared to other potentially more technically appropriate confidence interval approaches for proportions. We will see how to calculate Wald-based proportion confidence intervals and other approaches in section 4.2. Population characteristics. For other proportion confidence interval approaches this sample size may be somewhat downward biased, and it would make sense to either use a different sample size calculation based on the specific approach you will use to calculate the confidence intervals, or at least make your sample size inputs slightly more conservative.\nAlso remember that achieving the level of precision implied by the sample size calculation depends on whether the estimated proportion in the sample data is equal to the expected proportion and on any other assumptions such as the true response rate being equal to your assumption. If the estimated proportion is further from 0.5 than your expected proportion (e.g. 0.07) then you will have better precision than your pre-specified level of precision, assuming your response rate is equal or better than the assumed rate, and similarly if your response rate is better than your assumed rate you will have better precision than your pre-specified level of precision, assuming the estimated proportion is equal to or further from 0.5 than your expected proportion. And vice versa, i.e. your precision will be worse than the pre-specified value if the estimated proportion is closer to 0.5 than the expected proportion and/or if the response rate is worse than the assumed rate.\nThen as with the sample size for a mean, the same considerations apply around biases. Increasing the sample size will always increase the precision of your estimates, but it will never affect the influence of any study bias!\n\n\n\n\nSee the “Additional considerations” section in the “Estimating a mean” section above for a discussion of some useful guidance when facing common additional considerations.\n\n\n\n\n\n\n\n\n\nThe typical scenario where you would use this type of sample size calculation is when the research question is concerning whether there is a difference in the mean of a continuous outcome (or a discrete numerical outcome if it is distributed approximately normally) between two independent groups within a population. For example, when comparing the effect of an intervention on an outcome in a RCT. The typical analysis for such a research question here would be via an independent t-test, or more powerfully a linear regression with a binary independent categorical variable to estimate the between-group difference (again based on a t-based null-hypothesis significance test), and additional independent variables, measuring characteristics of the subjects related to the outcome, to increase the precision of the estimate. We will see how to carry out independent t-tests in section 5.1. Independent t-test, and linear regression in section 7.1. Linear regression.\n\n\n\nYou work for a research NGO in a country where public health facilities have reported increasing numbers of patients seeking treatment for cardiovascular diseases over the last decade. You have received funding to develop and pilot test an intervention that aims to improve the diagnosis and treatment of cardiovascular disease in your country’s public health facilities. Briefly, as this is a pilot test you plan to select one typical health facility and use an (individually) randomised controlled trial to compare relevant patient health outcomes in patients who are randomly allocated to be diagnosed and treated using the new intervention processes to patients who are randomly allocated to be diagnosed and treated using the existing processes. As cardiovascular disease events are rare you will use systolic blood pressure (mmHg) as a proxy outcome for cardiovascular health/risk. You will measure a range of other outcomes (i.e. secondary outcomes) and relevant health and socio-demographic data to use as independent variables in your analysis. However, as systolic blood pressure is your sole primary outcome you will base the sample size on this outcome alone. The idea is that if the intervention appears potentially effective based on this pilot study a multi-facility cluster trial will follow to provide definitive evidence. Therefore, based on consultations with clinicians and health officials in your country it has been agreed/decided that the smallest mean reduction in systolic blood pressure which will considered clinically meaningful/significant, and therefore indicating that the intervention should be tested in a large-scale cluster trial, is 5 mmHg.\n\n\n\nThere are more assumption inputs to consider when taking a hypothesis testing approach to sample size calculations, but several of the inputs are typically fixed at conventional levels - although this should never mean that you just mindlessly select those levels, there’s always room for thought!\n1. What alpha (α) level or level of significance do you want?\nThe lower this is set the less likely the resulting hypothesis test is to generate a type I error or a false positive result on average (or in the long-run), assuming you achieve the required sample size and that all other sample size assumption inputs are accurate. However, the lower this is set the larger the sample size required holding all other assumption inputs constant, so there’s a trade-off with the sample size. By convention the level of significance is usually set to be 0.05 or 0.01, i.e. the levels at which we commonly determine statistical significance: when P≤0.05 or less commonly P≤0.01. Again this is probably because it’s a nice round number and assumed to be reasonable trade-off. Unless you have a good reason to change this and know what you are doing then leave this as the default 0.05.\n\nTherefore, we’ll leave the level of significance as its default at 0.05.\n\n2. What power (1-β) do you want?\nThe higher this is set the more likely it is that the resulting hypothesis test will correctly reject a false null hypothesis, if the hypothesis is indeed false and the mean difference in the sample data is at least as large as the one assumed (see below), on average (or in the long-run), and assuming you achieve the required sample size and that all other sample size assumption inputs are accurate. However, again there is a trade-off: the larger this is set the larger the sample size required. By convention this is set at either 0.8 or less commonly 0.9. Again this is probably because it’s a nice round number and assumed to be reasonable trade-off. There’s absolutely no reason not to aim for a higher level of power though if you can afford the resulting sample size, and you can play around with this input and see how it affects your sample size.\n\nWe’ll leave the power as its default at 0.8.\n\n3. What is the expected difference in your outcome between the two independent groups (i.e. the expected difference in the two group means) and the expected variation in the outcome?\nThis is where we pre-specify the target difference that we want to be able to detect, which is a reduction of 5 mmHg in our scenario. In Statulator you can enter this information either in terms of the “Expected Means” or as an “Expected Difference between Means”. If you enter it as using the “Expected Means” option then you enter the expected “Mean of the Reference Group”, the expected “Mean of the Test Group”, and the expected “Standard Deviation”. For our scenario the expected mean of the reference group is the expected mean of the control or usual care group, while the expected mean of the test group is the expected mean of the new intervention group. However, for a sample size comparing independent means the actual means don’t matter, just their difference (e.g. a mean of 5 vs 10 requires the same sample size as a mean of 105 vs 110 as the difference is 5 in each case). Therefore, we can compare 0 (mmHg) in the reference group to 5 (mmHg) in the test group.\nThe expected standard deviation is the expected pooled standard deviation, i.e. assuming the outcome has the same standard deviation in both groups. As with the confidence interval approach to estimating a mean we can select this based on values in the literature, pilot data, and/or using the rule of thumb previously discussed. If you expect the standard deviation to be different in each group, which is often the case (e.g. outcomes in intervention arms are often less variable as processes are more standardised), then just use the bigger of the two standard deviations as the pooled standard deviation will always be smaller than this, i.e. it’s a conservative/safe approach. Let’s assume our expected shared or pooled standard deviation is 10 mmHg.\n\nTherefore, we’ll use the expected difference between means approach and enter the expected difference as -5 (mmHg), i.e. a reduction of 5 mmHg, and the expected shared or pooled standard deviation as 10 (mmHg), to ensure we obtain a sample size based on our target difference.\n\nIf the true difference in the target population is greater than this assumption then we’ll have more power on average than we expect. Note: it doesn’t make any difference to the calculation whether the target difference is positive or negative only the absolute value matters, so entering 5 or -5 won’t change the result.\n4. What is the ratio of group sizes that you expect?\nFor any given total sample size, if the sample size in each of the two groups differs then you will not have the expected power that you pre-specified, and your power reduces as the ratio of group sizes gets further from 1 (i.e. the group sizes become increasingly different). Therefore, as it is common that you will not get exactly equal group sizes, and often quite different group sizes, you should always plan accordingly and build this into your sample size assumptions. As with the expected response rate it makes sense to base your assumed group size ratio on data from previous studies, pilot data, and/or past experience, and that you make it a conservative/safe assumption. Note: you adjust for non-equal group sizes by specifying the expected ratio of group sizes. In Statulator it asks for the ratio of the group size in the reference group compared to the test group. Therefore, you have to specify the ratio in terms of this direction of comparison.\nHowever, as is commonly the case, if there is no reason to believe that any one particular group will be larger than the other, but there is reason to believe that one of the groups will be larger than the other but you just can’t predict which, then it’s probably easiest to just think in terms of how much larger in percentage terms one group is likely to be compared to another. If you then convert this percentage to a proportion and add it to 1 that’s your group size ratio. For example, if you believe one group is likely to be 25% larger than another then 25% = 0.25, and therefore our group size ratio is 1.25. As our scenario is that we are planning for an RCT, and you can usually guarantee a fairly equal group size ratio in an RCT, we’ll assume our group size ratio will be no more than 5% different in favour of either group.\n\nTherefore, we’ll assumed an expected group size ratio of 1.05.\n\n5. What is the expected response rate?\nAs with the estimating a mean tool the independent group means tool doesn’t have an option to automatically adjust for the expected response rate and so we’ll have to do it manually.\n\nWe’ll assume a response rate of 0.9 (90%).\n\n\n\n\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size required when comparing two independent means in OpenEpi\n\n\nRead/hide\n\n\nFrom the menu on the left expand the Sample Size folder if required and click Mean Difference.\nClick on the Enter tab. Unfortunately, in another quirk of OpenEpi, in the Sample Size for Comparing Two Means form the first row refers to the Confidence Interval % (two-sided). However, this is an unusual and probably confusing way to refer to what would normally be termed the significance level or alpha. This is what we’ve termed this parameter above when describing the inputs required. The usual value for this is 0.05 or 5%, which corresponds to a 95% confidence interval, so we can leave it as it is.\nThe next row Power refers to our desired power. Again, the conventional value for this is 80%, although many studies also use higher values like 85% or 90%. However, we decided earlier to accept a power of 80% so we can also leave this value unchanged.\nIn the next row (Ratio of sample size (Group 2/Group 1)) we need to specify the expected ratio of group sizes. It asks for this in terms of group 2’s size divided by group 1’s size. If we expect the SD of the outcome to be the same in each group then it doesn’t matter if we enter the ratio as group 1’s / group 2’s or vice versa. However, if we expect the SDs to vary then we need to be sure we calculate the ratio the correct way around. As we are assuming equal SDs per group here we can just enter our previously assumed ratio of 1.05.\nNext, on the Mean row we can either enter each group’s mean, or as the absolute values don’t matter but just the Difference. Remember, this is the minimum difference we want to be able to detect, but not necessarily the difference we think most likely exists. Let’s delete the values for Group 1 and Group 2 and just enter the or Difference value as 5. Again, as previously stated we can just enter the absolute value. Whether it’s +5 or -5 makes no difference to the required sample size.\nFinally, on the Std. Dev. row we enter the assumed SD for each group. We assume a shared SD of 10, so enter this under Group 1 and Group 2. Then click the Calculate button.\nWe are taken to the Results tab. Below the input data look for the Sample Size of Group 1, Sample Size of Group 2 and Total sample size rows. Here are our required sample sizes. Remember, in our scenario we only assumed a group size ratio of 1.05 to allow for chance imbalances in group size, but we had no expectation about which group would be larger. Therefore, in this scenario we would just take the total sample size and allocate it equally to our intervention and control groups.\nHowever, we would finally need to adjust this for the assumed response rate:\n\n\n128 / 0.9 = 142.2\n\n\nAs this is an odd number when rounded and we need to divide it we would add 2.\n\n\nTherefore, we need to sample and recruit 144 / 2 = 72 individuals to the intervention and control groups to ensure we have an 80% chance (our power) of detecting a difference between each groups’ mean systolic blood pressure (mmHg) of 5mmHg or greater, on the assumption that such a difference exists in the population, via a null-hypothesis significance test based on a two-sided p-value with a significance level of 5%.\n\n\nThat’s quite a complicated interpretation. Another way to think about it is that if we did our study an infinite number of times, each time calculating the p-value for the null-hypothesis that the difference between the two means we obtain = 0, and rejecting that null-hypothesis when the p-value is ≤0.05, then if the true difference in mean systolic blood pressure in the population between individuals given the intervention and those not given it is ≥5mmHg, then on average 80% of the time (in 80% of our repeated studies) we would get a p-value ≤0.05 and correctly reject the null-hypothesis. Still confused? I’m afraid it’s a complex series of concepts without an easy interpretation, and you just have to keep coming back to it until it clicks.\n\nAgain, this calculation assumes you will be carrying out an independent t-test of the difference between your group means, either via a classical independent t-test or within the context of a linear regression model.\nHowever, as for the previous sample size calculations, the accuracy of this interpretation depends entirely on the underlying assumption that there is no bias in the study. If, for example, the 90% response rate is correct, but reflects the fact that 10% of individuals drop out of the study. Then if all 10% drop out from the intervention group due to side effects of the new treatment regimes, even if the true effect of the intervention is to reduce systolic blood pressure by >5mmHg, we might have a power <80%. That is, even if we recruit 72 participants per group we might actually only have a 10% chance of detecting a statistically significant difference between the two groups’ mean systolic blood pressures, because of the influence of this bias (known as differential loss to follow-up).\n\n\n\n\n\n\n\nThe typical scenario where you would use this type of sample size calculation is when the research question is concerning whether there is a difference in a binary outcome between two independent groups (and remember you can always convert a numerical outcome to a binary outcome, and you can always convert a categorical outcome with >2 categories into a binary outcome, although it may not always make sense to do so). The typical study design for such a research question would either an RCT (most robust) or some form of observational comparison design, like a cohort study or an uncontrolled before and after study if the before group was independent of the after group (less common). The typical analysis for such a research question in these designs would be via a chi-square test of independence or a logistic regression with a binary independent categorical variable (plus maybe additional independent variables).\n\n\n\nYou work for a research NGO in a country where public health facilities have reported increasing numbers of patients seeking treatment for diabetes over the last decade. You have received funding to develop and pilot test an intervention that aims to improve the diagnosis and treatment of diabetes in your country’s public health facilities. Briefly, as this is a pilot test you plan to select one typical health facility and use an (individually) randomised controlled trial to compare relevant patient health outcomes in patients who are randomly allocated to be diagnosed and treated using the new intervention processes to patients who are randomly allocated to be diagnosed and treated using the existing processes. You decide to define diabetes based on the international guideline of having a fasting plasma glucose level ≥7.0 mmol/l, and your primary outcome will therefore be the binary outcome of whether a patient has a fasting plasma glucose ≥7.0 mmol/l or <7.0 mmol/l, i.e. whether they currently “have” diabetes/not. You will measure a range of other outcomes (i.e. secondary outcomes) and relevant health and socio-demographic data to use as independent variables in your analysis. However, as having diabetes/not is your sole primary outcome you will base the sample size on this binary outcome alone. The idea is that if the intervention appears potentially effective based on this pilot study a multi-facility cluster trial will follow to provide definitive evidence.\nBinary outcomes are naturally summarised as proportions/percentages, and a point prevalence is simply the proportion of individuals (or other units) that have a characteristic of interest at a certain point in time. Therefore, we can view our comparison of interest for our analysis as being a comparison between the proportion/percentage or point prevalence of diabetes in the intervention group compared to the proportion/percentage or point prevalence of diabetes in the control group.\nUnlike when you are calculating a sample size when comparing independent means, as we’ll discuss further below, there is a complicating issue. It is not just the difference in the outcome between the two groups that affects the sample size but also the value of the outcome (i.e. the assumed proportion) in each group.\nFor our scenario though we’ll assume that we have good data on the existing point prevalence of diabetes among patients being treated for diabetes (i.e. the proportion of patients who have a fasting plasma glucose ≥7.0 mmol/l) in public health facilities, and that this is no greater than 0.3 (30%). We’ll also assume that based on consultations with clinicians and health officials in your country it has been agreed/decided that the smallest reduction in the prevalence of diabetes which will be considered clinically meaningful/significant, and therefore indicating that the intervention should be tested in a large-scale cluster trial, is a reduction from 0.3 to 0.2 (i.e. from 30% to 20%).\n\n\n\nSeveral of the assumptions are identical to those for the comparing two means sample size calculation, and so we will not explain them again.\n1. What alpha (α) level or level of significance do you want?\n\nWe’ll leave the level of significance as its default at 0.05.\n\nSee the relevant description in the comparing two means section above if you need a reminder about this parameter.\n2. What power (1-β) do you want?\n\nWe’ll leave the power as its default at 0.8.\n\nSee the relevant description in the comparing two means section above if you need a reminder about this parameter.\n3. What is the expected difference in your outcome between the two independent groups (i.e. the expected difference in the two group proportions)?\nAs mentioned earlier we must be explicit about the outcome proportion expected in each group to set our minimum target difference we want to detect. We cannot just specify the difference alone. In OpenEpi we can specify the difference either by specifying the assumed percentage of the outcome in each group, or the assumed percentage of the outcome in the “Unexposed” group (i.e. the control group for an RCT or a comparison group for a non-randomised study) plus the assumed risk ratio or risk difference (or prevalence ratio or prevalence difference - they are the same thing).\nAs with the sample size for the mean difference, the difference we specify here should be the minimum target difference: the minimum difference we want to be able to detect, should it exist. Many studies instead base their sample size on the difference they expect (hope!) to see, but this inevitably is usually overly optimistic given the resource and monetary costs associated with a larger sample size. Therefore, such an optimistic approach often leads to wasted effort and money, because the sample size turns out to be too small to produce useful results.\n\nWe’ll specify the expected proportions for each group explicitly, based on the minimum difference we want to be able to detect. We will therefore specify the assumed percentage outcome for the unexposed (control) group as 30 and the expected percentage for the exposed (intervention) group as 20 to ensure we obtain a sample size based on our minimum target difference.\n\nIf the true difference we find is greater than this then we’ll have more power than we expect.\nNote: similar to the sample size calculation for estimating a population proportion, this one assumes the sampling distribution for the difference in proportions is normally distributed. Although we are estimating a difference in proportions, and each proportion comes from a binary variable that cannot formally be normally distributed, the central limit theorem allows us to assume the difference of these two proportions will be normally distributed (at least when we have a reasonable sample size - usually expected to be 30+ per group). OpenEpi does however offer two slightly different variations on the sample size approach for this situation. We won’t go into the details here, but you can look at the Documentation link on the test page for more details.\nDue to the normal distribution assumption the test therefore also assumes you will use a “z-test” to test for the difference in proportions. A z-test is similar to a t-test but assumes the data are normally distributed. They are rarely used compared to t-tests though so we do not cover them in this course. However, in practice most analysts would probably use a logistic regression model to analyse the difference in a binary outcome between two groups, usually also adjusting for other variables that are likely confounding variables or to increase precision. We will look at logistic regression in section 7.1. Linear regression. Without going into the details the standard null-hypothesis significance test of the “model coefficient” estimating the relevant difference in a logistic regression model is the “Wald test”, which is effectively a z-test.\n4. What is the ratio of group sizes that you expect?\nSee the description in the comparing two means section above.\n\nAgain we’ll assume we have no reason to believe that either group will be larger than the other, but we do expect that one of the groups is likely to be slightly larger than the other by chance due to recruitment differences, and so we’ll assume a group size ratio of 1.05.\n\n5. What is the expected response rate?\n\nAgain, we’ll assume a response rate of 0.9 (90%).\n\n\n\n\nSorry, there is currently no video instructions for this method. Please use the written instructions below.\nWritten instructions: calculate the sample size required when comparing two independent proportions in OpenEpi\n\n\nRead/hide\n\n\nFrom the menu on the left expand the Sample Size folder if required and click Cohort/RCT.\nGo to the Enter tab at the top. In the first row ( Two-sided confidence interval(%)) we again have the unusually-worded alpha value, which we leave at the 5% level (i.e. here specified as a 95% confidence level for the test).\nIn the next row (Power (1-beta or % chance of detecting)) we have our desired power, which we can also leave at the default value of 80%.\nIn the next row (Ratio of Unexposed to Exposed in sample) we set our assumed group size ratio, so let’s change this to 1.05.\nIn the next row (Percent of Unexposed with Outcome) we set our assumed “Unexposed” group outcome percentage, which for us means our assumed control group percentage. So set this to 30(%).\nIn the next section it says we just need to fill in one of the following. Skip the next row and on the row Percent of Exposed with Outcome we can enter our assumed “Exposed” group outcome percentage, which for us means our assumed intervention group percentage, such that the difference between the two groups is our desired target minimum difference we want to detect. Set this to 20(%).\nNow click the Calculate button. Again, we can see all the inputs made to the calculation, and below them the results. As mentioned there are three sets of results corresponding to three slightly different sample size calculations: Kelsey Fleiss, and Fleiss with CC. There’s not necessarily much good research of which to prefer, but generally speaking the one with the largest sample size is probably the safest and makes the most conservative assumptions, which is often the opposite of what occurs in research planning in reality. So let’s use the Fleiss with CC approach.\nIt tells us we need 617 overall. Although we’ve made an assumption about the group ratio we didn’t actually have a reason to believe one specific group would be higher than other, just that one would likely differ from the other. So let’s just stick with the overall value and divide by 2. First let’s adjust for the assumed response rate:\n\n\n617 / 0.9 = 685.5.\n\n\nTherefore, we need to sample and recruit 686 / 2 = 343 individuals to the intervention and control groups to ensure we have an 80% chance (our power) of detecting a difference in the prevalence of diabetes between the groups of 10 percentage points or more, on the assumption that such a difference exists in the population and that the difference is control = 30% intervention = 10%, via a null-hypothesis significance test based on a two-sided p-value with a significance level of 5%.\n\n\nAgain, that’s quite a complicated interpretation. Another way to think about it is that if we did our study an infinite number of times, each time calculating the p-value for the null-hypothesis that the difference between the proportions we obtain = 0, and rejecting that null-hypothesis when the p-value is ≤0.05. Then if the true difference in diabetes prevalence in the population between individuals given the intervention and those not given it 10 percentage points lower in individuals given the treatment (and the prevalence is 30% in individuals not given the treatment), then on average 80% of the time (in 80% of our repeated studies) we would get a p-value ≤0.05 and correctly reject the null-hypothesis. Still confused? I’m afraid it’s a complex series of concepts without an easy interpretation, and you just have to keep coming back to it until it clicks.\n\nAgain, as for the previous sample size calculations, the accuracy of this interpretation depends entirely on the underlying assumption that there is no bias in the study. If, for example, the 90% response rate is correct, but reflects the fact that 10% of individuals drop out of the study. Then if all 10% drop out from the intervention group due to side effects of the new treatment regimes, even if the true effect of the intervention was to reduce the prevalence of diabetes by >10 percentage points (and the true prevalence in untreated individuals was 30%), we might have a power <80%. That is, even if we recruit 343 participants per group we might actually only have a 10% chance of detecting a statistically significant difference between the two groups’ prevalence of diabetes, because of the influence of this bias (known as differential loss to follow-up).\nAnd as always the validity and accuracy of this calculation depends entirely on the accuracy of the assumptions that you made when calculating the sample size.\n\n\n\n\n\n\nIn the MSc & MPH statistics computer sessions practical files “Exercises” folder open the “Exercises.docx” Word document and click on the Planning sample sizes heading in the contents and follow the instructions, then check your answers with the model answers below.\nOptional hint: type of sample size calculations required for each scenario\n\n\nRead/hide\n\n\nScenario 1: given the scenario we are aiming to calculate the sample size required to estimate a single proportion (although in reality it will apply to all the proportions we estimate in the scenario survey).\nScenario 2: given the scenario we are aiming to calculate the sample size required to detect compare two means (or more specifically we are aiming to detect a difference between two means as statistically significant, i.e. with a p-value less than some threshold, with a given level of power).\n\n\n\n\n\n\nRead/hide\n\n\n\nFor our survey we estimated that for any population proportion we wish to estimate (e.g. from a binary outcome or a category level from a categorical outcome with >2 levels) we require a sample size of 122 (i.e. we need to try and recruit 97 facilities into the survey) to achieve a 10 percentage point level of precision (95% confidence interval ±10 percentage points), assuming the proportion we estimate is 0.5, and assuming a response rate of 80% (remember to adjust the OpenEpi sample size for the assumed response rate).\n\n\n\nWe estimated that we required a total sample size of 94 health facilities, i.e. 47 per group, to detect a difference in the mean number of essential drug stock-outs during the three-month study period (the primary outcome) in the intervention group compared to the comparison group of -15 or greater, assuming a standard deviation of 25, based on a hypothesis test of the difference in the primary outcome between the two groups (assuming the outcome is t-distributed), with a level of significance of 0.05 and a power of 0.8. We also assumed a follow-up rate of 95%.\nRemember to adjust for the response rate, and then to add 1 if you end up with an odd number to allow you to divide by 2 for the group sizes."
  },
  {
    "objectID": "3-sample-size.html#reporting-sample-size-calculations-in-methods-sections",
    "href": "3-sample-size.html#reporting-sample-size-calculations-in-methods-sections",
    "title": "",
    "section": "Reporting sample size calculations in methods sections",
    "text": "Reporting sample size calculations in methods sections\n\n\nRead/hide\n\nFor any methods and results reporting, such in a paper or report, you should explain what your study sample size was and how you calculated it, including all the assumptions that went into it. You should also report where any assumptions came from, i.e. what they were based on/how you chose them, unless they were chosen by convention, such as a 95% confidence interval or a level of significance of 5%. There is no set sequence in which you have to report the different assumptions/inputs used in your sample size calculation, but the below examples are one way you might do it. Lastly, you don’t have to explicitly state whether you are reporting a sample size calculated via a confidence interval or hypothesis testing based approach as this should be clear from the assumptions reported.\nIn the below examples we will not report any assumptions about the response rate or group size ratio, but if you make an assumption for these other than 100% or 1 respectively this should also be reported, and the basis for these assumptions justified. Also, the assumption values are just for illustration and not real!\n\nConfidence interval based sample size calculation\nWhen estimating a mean you simply need to report the expected standard deviation, the level of precision or margin of error, and the confidence level (this is often not presented, presumably because the assumption is 95%, but why not be clear?). You should also explain where your assumption of the expected standard deviation and level of precision came from.\nExample methods reporting text for a confidence interval based sample size calculation for a survey of systolic blood pressure:\n\nWe estimated that we required a sample size of 100 to estimate the mean of our primary outcome of systolic blood pressure (mmHg) with a level of precision (95% confidence intervals) at most ± 5 mmHg, assuming a standard deviation of 10. We based our assumption of the expected standard deviation on data from our previously reported pilot study (reference), which we rounded up from 7 to 10 to be conservative. Our level of precision was chosen based on consultations with relevant clinicians and health officials about what level of precision they required for usable results.\n\nWhen estimating a proportion you simply need to report the expected proportion, the level of precision or margin of error, and the confidence level. You should also explain where your assumption of the expected proportion and level of precision came from.\nExample methods reporting text for a confidence interval based sample size calculation for a survey of hypertension prevalence:\n\nWe estimated that we required a sample size of 100 to estimate the proportion of individuals with hypertension, which we assumed to be 0.3, with a level of precision (95% confidence intervals) where the lower limit was at most 0.25 and the upper limit was at most 0.3. We based our assumption of the expected proportion on data from our previously reported pilot study (reference) of 0.2, which we rounded up by 0.1 towards the most conservative value of 0.5 to be conservative. Our level of precision was chosen based on consultations with relevant clinicians and health officials about what level of precision they required for usable results.\n\nTake care, when reporting the confidence interval ranges for your proportion/percentage that you are clear whether they are on an absolute scale (probably the easiest to understand and not misinterpret) or a relative scale.\n\n\nHypothesis testing based sample size calculation\nFor a hypothesis test based sample size calculation for a difference between two independent means you simply need to report the expected difference in means (or if you prefer the expected mean in each group) and the expected pooled (or common) standard deviation, plus your pre-specified level of significance/alpha and the power, and that you are using a two-sided hypothesis test (unless of course you are not). You should also explain where your assumption about the expected difference in means (i.e. the target difference) and the expected pooled standard deviation came from. Lastly, although it’s often not done you should explain what distribution you are assuming your outcome follows\nExample methods reporting text for a hypothesis test based sample size calculation for a study comparing the effectiveness of an intervention at reducing systolic blood pressure in an two-group comparison RCT:\n\nWe estimated that we required a sample size of 100 to detect a reduction in our primary outcome of systolic blood pressure (mmHg) between our intervention and control arms that is at least 5 mmHg, based on a two-sided hypothesis test (assuming the t-distribution). This assumes a pooled standard deviation of 10 mmHg, the standard level of significance of 0.05, and a power of 0.8. Our target difference for detection was chosen based on consultations with relevant clinicians and health officials about what size of impact would be required for the intervention to be considered worth funding and scaling up. Our expected standard deviation was chosen based on values from relevant previous studies (references), which we rounded up from 7 to 10 to be conservative.\n\nFor a hypothesis test based sample size calculation for a difference between two independent proportions you simply need to report the expected proportion for each group (or equivalently the expected proportion in the reference/control group and the absolute or relative expected difference in the proportional outcome, but this is arguably less easy to follow), plus your desired level of alpha and power, and that you are using a two-sided hypothesis test (unless of course you are not). You should also explain where your assumption about the expected difference in proportions (i.e. the target difference) came from.\nExample methods reporting text for a hypothesis test based sample size calculation for a study comparing the effectiveness of an intervention at reducing the prevalence/proportion of individuals having hypertension in an two-group comparison RCT:\n\nWe estimated that we required a sample size of 100 to detect a difference in the proportion of individuals with hypertension at study follow-up where we expect the proportion with hypertension in the intervention group is 0.2 and the expected proportion in the control group is 0.3, based on a two-sided hypothesis test. This assumes the standard level of significance of 0.05 and a power of 0.8. Our target difference for detection was chosen based on consultations with relevant clinicians and health officials about what size of impact would be required for the intervention to be considered worth funding and scaling up, with the expected proportion of individuals with hypertension in the control arm based on existing routine clinical data rounded up from 0.25 to 0.3 (towards the most conservative assumption of 0.5) to be conservative."
  },
  {
    "objectID": "3-sample-size.html#compare-paired-means-or-proportions",
    "href": "3-sample-size.html#compare-paired-means-or-proportions",
    "title": "",
    "section": "Compare paired means or proportions",
    "text": "Compare paired means or proportions\nWe will not look at these sample size scenarios/approaches or practice them as they are not commonly needed, but they are for when your study design involves comparing means or proportions between two groups where those groups are not independent. For example, if you are comparing the change in systolic blood pressure (mmHg) in the same individuals where their systolic blood pressure is measured at two separate times, or where you are comparing the proportion of individuals with diabetes where that diagnosis is made at two separate times within the same group of individuals. Most of the assumptions that go into these calculations are exactly the same as for the calculations we’ve already covered and the rest you should be able to work out or get help with if you ever need to use them, which is not likely."
  },
  {
    "objectID": "3-sample-size.html#adjusting-for-clustering",
    "href": "3-sample-size.html#adjusting-for-clustering",
    "title": "",
    "section": "Adjusting for clustering",
    "text": "Adjusting for clustering\n\n\nRead/hide\n\nWe will not look at adjusting for clustering beyond saying that if you believe this issue applies to your study you should seek advice from a statistician/researcher experienced with making the necessary adjustments. What is clustering? As an example, if you collect data on pupils within different schools to look at test scores then those pupils within the same schools are likely to have correlated test scores. This is due to differences at the school level, such as difference in the overall quality of teaching, the socio-economic circumstances of the schools’ catchment areas, whether they charge fees etc. Hence, you don’t have the same amount of statistical information as for a true simple random sample, because pupils are not independent when they come from the same school. Standard sample size calculations, such as those we’ve looked at, assume your sample data are independent. They would assume that two randomly selected pupils from the same school are no more or less likely to have similar outcome values, such as test scores, than two randomly selected pupils from separate schools. Rarely will this be the case. Therefore, unless you adjust for the “level of clustering” in your outcome you won’t achieve the level of precision or power that you expect to get from a given sample size even if all the other assumptions are accurate. There are different ways of measuring the “amount of clustering” in an outcome, and it’s a more advanced topic beyond the scope of this introductory course that you should seek assistance with if you need to carry out such a sample size calculation in the future."
  },
  {
    "objectID": "4-introduction-to-spss.html",
    "href": "4-introduction-to-spss.html",
    "title": "",
    "section": "",
    "text": "SPSS is a widely used statistical software package that allows you to do data processing/editing, data visualisation (i.e. graphs/charts), and carry out a very wide range of data analysis. In these sessions we’ll see how to do all of these things. In SPSS, like most software programs you’ve probably used, you can tell the program what you want to do via different tools accessed through menus and “clicking buttons”. However, you can also tell SPSS what you want it to do via code, or as SPSS calls it “syntax”, i.e. by typing in commands as words and numbers. See the additional information for more details on SPSS and related software.\nAdditional SPSS and related statistical software information\n\n\nRead/hide\n\nSPSS was originally called the “Statistical Package for the Social Sciences” reflecting the original main audience, but over time it has become widely used. Although it might look similar to Microsoft Excel at first glance, rather than using functions within “cells” to do things, all data processing and analysis is done via commands run from the menu, and it has a far wider range of much more powerful and sophisticated statistical analyses available than Excel. Although we will only be using the menus commands to run our analyses, SPSS also has the option to use a programming interface to run all commands via code (written text commands). This approach has a number of huge advantages over using the menu commands. One of the major advantages of using code/scripts is that you can tweak and re-run your entire analysis, which might be the product of days or weeks of work, just by highlighting the script and re-running it. To run all the commands stored in your script manually via the menus might take hours! Another major advantage is it makes it very easy to collaborate on analyses, assuming your collaborators can understand and work with SPSS code, because you can simply share your code. However, getting to grips with this approach is typically a steep learning curve, and so we will not look at this further.\nHowever, there are many other excellent statistical software packages available, with R and Stata being the most commonly used in global health research. And unlike SPSS, if you are looking for a free statistical software package my (JH) recommendation would definitely be R (https://www.r-project.org/). There is a document giving a brief overview to R on the Statistics for Health Sciences Minerva site, but the main advantages of R are 1) it’s open source and therefore completely free, 2) it’s huge user community means you will always find solutions to any problems you run into already waiting for you to read online, and 3) it’s huge range of user-developed “packages” mean that R will almost always be able to do what you want it to do. Note that both R and Stata are intended to be run as programming languages, i.e. by writing and running code rather than clicking through endless menus. However, there are also “graphical user interfaces” available for R so that you can run R (albeit with reduced functionality) like we will be using SPSS, as a menu-based program. Similarly, you can also run quite a lot of the main analyses and tools in Stata via its built-in menus.\n\n\n\n\nBefore we go further a brief word about the datasets we will be using in these SPSS practical sessions.\nThe datasets that we’ll be using in these data analysis sessions are a mix of simulated (i.e. artificially generated) and real data. The main dataset that we will be using for most analyses is the SBP final data.sav dataset, which is an SPSS format dataset as indicated by the “.sav” suffix. We will be using a more basic Excel version of this dataset and the SPSS format version of this dataset for the first few exercises.\nThe SBP final data is a simulated dataset but it is designed to be representative of a typical cross-sectional dataset of individual-level data. Although the dataset is simulated it contains most of the typical challenging features of real-world datasets like confounding and missing data. For simplicity we will therefore talk about the dataset as if it was a real dataset from now on, but be aware that it is somewhat simplified with fewer variables than we would likely collect for such a study. Specifically, the dataset contains data collected from a survey investigating what biological, health and socio-economic factors influenced individuals’ blood pressure and hypertension status. The dataset was collected from a study using a simple random survey of individuals aged 18 or over (the only eligibility criteria) who lived in a single city, and the data were collected at approximately the same point in time (e.g. over the course of a few weeks). The dataset contains data from 556 participants. Therefore, the “unit of observation” in this dataset is the individual. Note: we do not imply any social/political judgments based on how we have coded certain variables in this dataset.\n\n\n\nIf you have a version of SPSS installed locally on your own computer you just need to run the “IBM SPSS Statistics XX” program/app. Mine is called “IBM SPSS Statistics 27”, but yours may be a different version.\nHowever, most if not all of you will be unlikely to have SPSS already installed, in which case you need to run SPSS via the “AppsAnywhere” platform. This is possible from either a campus computer or your own laptop.\n\n\n\nOpen Chrome and navigate to appsanywhere.leeds.ac.uk. You may have to login. Use your usual university ID and remember to add ‘@leeds.ac.uk’ at the end of your username. You should see a blue popup message appear at the bottom right of the screen saying “Validation in progress…”. The AppsAnywhere “Cloudpaging Player” app will also launch while this happens. After a few seconds you should then see a second popup message appear saying “Validation Successful”. If you see a message saying “Validation failed” click the “Retry” button, but if you see the same message again see below for help. You can then click on the search field at the top of the https://appsanywhere.leeds.ac.uk/ page and enter “SPSS”. Look for “SPSS Statistics 28” and click the “Launch” button, which will load SPSS. Once you have run SPSS from a computer it should be available directly from the Cloudpaging Player app, so next time try just running the app, highlighting SPSS in the list of “Applications” and clicking the “Launch” button. If you can’t see SPSS you’ll have to go via the appsanywhere.leeds.ac.uk site again though.\nIf you saw a message saying “Validation failed” even after retrying then you either need to install the “Cloudpaging Player” app on the computer, which shouldn’t take too long but does involve a few steps. Therefore, if possible I would recommend just trying another computer which should already have the app installed, or you can use the university’s Windows Virtual Desktop. Click the “Read/hide” button below for instructions on each approach.\n\n\n\nRead/hide\n\n\nIf you want to install the app on a campus computer see the instructions for installing apps on campus PCs here, and install the “Cloudpaging Player” app from the Software Centre. Once you’ve installed it just follow the first bullet point instructions under Campus computers above.\nIf you want to use the Virtual Windows Desktop click the Start Menu button (the Windows icon button at the bottom left of the screen) and locate the “Remote Desktop” app, or after clicking just start typing “remote desktop” and it should locate the app. Click on the app to run it and in the window that appears double click on the “WVD - Academic” icon. Once the Windows Virtual Desktop appears you can then just follow the first bullet point instructions under Campus computers above. Note: you may see a message appear saying “Open AppsAnywhere Launcher”. Click the tick box to say you want this to happen each time and click “Open”. Look for “SPSS Statistics 28” and click the “Launch” button, which will load SPSS. SPSS may be available to run directly from the Cloudpaging Player app next time, so next time try just running the app, highlighting SPSS in the list of “Applications” and clicking the “Launch” button. If you can’t see SPSS you’ll have to go via the appsanywhere.leeds.ac.uk site again though.\n\n\n\n\n\nMacs\n\nIf you have a Mac then unfortunately you cannot run AppsAnywhere directly and you must access it via the university’s Virtual Windows Desktop. You will first have to install the Virtual Windows Desktop app. To do this follow the instructions here. Once you’ve installed the Windows Virtual Desktop app run the “Remote Desktop” app. You will have to sign in using your university ID. Remember to include @leeds.ac.uk at the end of your username. Then in the window that appears double click on the “WVD - Academic” icon. Once the Windows Virtual Desktop appears you can then just follow the first bullet point instructions under Campus computers above. Note: you may see a message appear saying “Open AppsAnywhere Launcher”. Click the tick box to say you want this to happen each time and click “Open”. Look for “SPSS Statistics 28” and click the “Launch” button, which will load SPSS. Once you have run SPSS from a computer it should be available directly from the Cloudpaging Player app, so next time try just running the app, highlighting SPSS in the list of “Applications” and clicking the “Launch” button. If you can’t see SPSS you’ll have to go via the appsanywhere.leeds.ac.uk site again though.\nIf you need more help on running the Windows Virtual Desktop app see the IT help page here.\n\nWindows-based laptops\n\nYou will need to install the Cloudpaging Player app to access software from the AppsAnywhere platform if you haven’t previously done this. Please follow the instructions on the IT help page here. Once you have run SPSS from a computer it should be available directly from the Cloudpaging Player app, so next time try just running the app, highlighting SPSS in the list of “Applications” and clicking the “Launch” button. If you can’t see SPSS you’ll have to go via the appsanywhere.leeds.ac.uk site again though.\n\n\n\n\n\n\nFirst load SPSS, then follow the instructions below to practice loading a dataset and understanding the layout and key features of SPSS.\n\nVideo instructions: loading data, and understanding the key features of interface\nWritten instructions: opening SPSS, loading data, and understanding the key features of interface\n\n\nRead/hide\n\n\nWhen you load SPSS unless it is an older version you will get two windows appear. The upper one has “Welcome to SPSS” in the top left corner. Just close this window leaving open the “data editor” window, which will say “Untitled…” in the top left corner, and which looks like this:\n\n\n\n\nSPSS Main Window View\n\n\n\nSPSS has two main windows: the data editor, as we’ve just seen in the image above, and the “output window”, which displays the SPSS syntax or code that the buttons you click create to tell SPSS what to do (which you can ignore and you’ll probably find annoying that it’s displayed every time you do something) along with the results of those commands (usually printing tables of results from analyses and producing graphs, which are useful). This only appears once you tell SPSS to do something though. For simplicity we will just refer to the data editor window as the main window from now on.\n\n\n\n\nNext let’s load some data so we can explore the main window (and the output window will appear for the first time). Go back to the main window in SPSS and in the main menu click File > Open > Data. Then in the Open Data tool window that opens navigate to the folder where you saved the “MSc & MPH statistics computer sessions practical files.zip” folders and go into the “Datasets” folder in the same way you would locate a folder in Windows Explorer. Find the file called “Example basic data.sav” and either double click on it or click on it once and then click the OK button.\nThis is an SPSS format dataset (.sav) and therefore contains more data and formatting of the data than other formats such an Excel (.xlsx) or .csv etc. This allows it to store things like variable labels and value codes (see below).\nYou will eventually see the output window appear and display some code that is telling SPSS to load the dataset, and then SPSS should take you back to the main window where you will see the raw values of the data in the Data View tab. See number 3 in the image above. The variable names are in the top row and the raw data values are in the cells. You will see there are three variables.\n\n\n\n\nRefer to the “SPSS main window view” image above and the numbered features:\n\nThe main menu. This is where you select commands to do most things you need to do.\nThe raw data view. Similar to Excel, here you will see the raw data displayed (although you can choose whether to display categorical variables’ labels or raw values if numerically coded - see below).\nThese tabs allow you to either view the Data View, where the raw data is displayed in the main area (2), or the Variable View, where information about each variable is displayed and where you can edit variables’ characteristics.\nVariable names will be displayed here once any data are loaded.\n\n\n\n\nBefore going further we will just set an option (or check that it’s already set) in SPSS so that it displays variables’ names rather than variables’ labels in the various tools we will be using. We will see what the difference between these two things are later. This is because in the instructions below we refer to variables’ names rather than their labels. However, you can certainly choose to display either for your own analyses.\n\nFrom the main menu bar go: Edit > Options and then under the General look under the Variable Lists section and select the Display names option as shown below:\n\n\n\n\nSPSS Main Options\n\n\n\n\n\nNow that we’ve loaded some example data into SPSS let’s look at the variable properties we can edit in the Variable View tab in the main window.\n\nName = the variable’s name. Like a file name, this should be short and descriptive and clear.\nType = the “type” of format in which the variable is stored, e.g. numbers, string (letters and possibly numbers and/or characters) etc. Note this is just how the variable is stored, and not the type of variable in terms of categorical, numerical etc (see “Measure” below). You can often just ignore this and you should not alter this unless you know what you are doing!\nWidth = for string variables, width refers to how many characters a value can hold. Again you can just ignore this and should not alter this unless you know what you are doing!\nDecimals = how many decimal places will be displayed (note: changing this does not alter how many decimal places a value actually has stored though).\nLabel = the descriptive label of the variable, used to briefly describe what the variable measures in more detail than the variable name.\nValues = used to create or edit value labels, i.e. labels attached to different numerical values. We’ll come back to this shortly.\nMissing = used to set or change which specific values (if any) represent different types of missing data. We will not be using this.\nColumns = how wide the displayed columns are currently.\nAlign = whether values are left or right aligned.\nMeasure = what type of data are stored in each variable. In the terminology we’ve been using: nominal = a binary variable or a categorical variable with 3 or more category levels, ordinal = an ordinal categorical variable, and scale = a discrete or continuous numerical variable.\nRole = I’m not actually sure what this does so presumably it’s not anything that important and you can ignore it. Google if interested!"
  },
  {
    "objectID": "5-preparing-a-dataset.html",
    "href": "5-preparing-a-dataset.html",
    "title": "",
    "section": "",
    "text": "The first step before doing any analysis is to understand and prepare the dataset. If you are working with data from someone else’s study (“secondary data”) the data may have been prepared already, but either way you must still go through these processes to ensure you understand the dataset in detail and to ensure the data really are ready for analysis. You may see this stage referred to as data preparation, processing, cleaning, cleansing etc. These terms can mean different things in different contexts and there’s not really any universally accepted terminology, but prior to analysing a dataset to produce results we want to ensure the following:\n\nAll variables are suitably named, labelled and (as appropriate to the software) set to the appropriate variable type.\nAll categorical variables are correctly coded and free from obvious errors.\nAll numerical variables are in the correct format and free from obvious errors.\n\nIn addition there are often other key features of a dataset that must be checked prior to analysis, but these will be dataset specific and we don’t have time to go into the detail here. However, one common example would be checking that any unique IDs have no duplicated values, and another would be cross checking any variables that are related to confirm they are both consistent, e.g. compare ages with dates of birth (they should agree) etc. To save time we will just do these steps for some of the variables in the SBP Excel data dataset to see how they are done, but then use a fully cleaned and prepared dataset for the analyses.\nFirst we’ll load a typical basic Excel dataset that lacks the extra formatting features of an SPSS format dataset, and that also contains typical errors present in raw datasets such as wrongly coded categorical variable observations and clear errors in some numerical variables, so we can see how to deal with these common issues when preparing datasets.\n\n\nSPSS can load all the main types of data you are likely to need to load, including CSV files, Excel spreadsheets, and data files from various other statistical programs like Stata and SAS. During these practical sessions we will use datasets saved in SPSS’s own format (.sav), which preserves all the useful information on things like variable names and labels, categorical variable coding labels etc that we will see explained below. However, if you use SPSS in the future it is likely you will have to load data in an Excel format at some point. Therefore, the first thing we’ll learn to do is to load an Excel spreadsheet.\nVideo instructions: load Excel data into SPSS\nWritten instructions: load Excel data into SPSS\n\n\nRead/hide\n\n\nGo back to the main window in SPSS and in the main menu click File > Open > Data. Then in the Open Data tool window that opens navigate to the folder where you unzipped the MSc & MPH statistics computer sessions practical files and go into the “Datasets” folder in the same way you would locate a folder in Windows Explorer. Then to allow you to see Excel files look for the Files of type drop-down menu beneath the area in the Open Data tool where the folders and files are displayed, and select the All Files option here. Then click on the “SBP Excel data.xlsx” dataset and then click Open (or double click on the file). A tool window will pop-up called Opening Excel Data Source. Ensure the Read variable names from the first row of data box is ticked and the Worksheet drop down menu is on “SBP data [A1:I557]” and then click OK. You will now see the raw data displayed in the Data View with the variable names in the top row and the data values in the cells. You will see there are nine variables.\n\n\n\n\nNow using our “raw” Excel dataset (that just contains the variable names and their values) let’s see how to set key variable properties, check that all categorical variables are correctly coded and free from obvious errors, and check that all numerical variables are free from obvious errors. For this we will need to understand the key details of our variables. If you are working with data that is not your own then assuming you are using a high quality, well documented dataset, this information will often be presented in the form of a “codebook”. We will present the codebook for this dataset in the form of a table below:\n\n\n\n\n\n*SBP data* dataset variable details\n \n  \n    Variable name \n    Variable type \n    Variable description  (units/coding of levels) \n  \n \n\n  \n    id \n    Numerical:discrete \n    Observation ID \n  \n  \n    sbp \n    Numerical: continuous \n    Systolic blood pressure (mmHg) \n  \n  \n    htn \n    Categorical: nominal (binary) \n    Hypertension (SBP >= 140 mmHg, 0 = no/1 = yes) \n  \n  \n    age \n    Numerical: discrete \n    Age (years) \n  \n  \n    sex \n    Categorical: nominal (binary) \n    Sex (1 = male/2 = female) \n  \n  \n    ses \n    Categorical: nominal \n    Socio-economic status (1 = low/2 = medium/3 = high) \n  \n  \n    salt \n    Numerical: discrete \n    Salt consumption (g/day) \n  \n  \n    bmi \n    Numerical: continuous \n    BMI (kg/m2) \n  \n  \n    ace \n    Categorical: nominal (binary) \n    ACE inhibitor usage for at least 3 months (1 = yes/2 = no) \n  \n\n\n\n\n\n\nVideo instructions: set variable properties, code categorical variables, and check categorical variables for obvious errors\nWritten instructions: set variable properties, code categorical variables, and check categorical variables for obvious errors\n\n\nRead/hide\n\n\nFirst we’ll make sure our variables are correctly named, labelled and set to the correct data type, and that all categorical variables are correctly coded. Note: for some of these exercises we will just edit a few of the variables for practice, but leave the rest alone. This is okay because when we move on we will be using a fully updated version of the dataset so we don’t need to update more than enough to understand the processes. To make these updates we need to know what variables are in our dataset and what data they contain, its units/coding etc. This information would either be available to you if you were the one who collected the data, or it would be made available in a “codebook”. Refer to the codebook table above for the key details of all variables in this SBP Excel data dataset when updating the variables’ details.\n\n\n\nOn the Variable View tab look at the Name column. Here you see the variables’ names. Variable names are short terms used to refer to the variables, a bit like file names. Variable names should be short, clear, understandable and unambiguous. We’ll leave these as they are, but just so you can see how to edit them click on the variable name “id”, delete it and re-enter it.\n\n\n\n\n\nNext in the Variable View look along to the Label column where the variable labels can be stored (there are none in this dataset yet as Excel cannot store such information). Variable labels allow you to briefly describe the variable in more detail than the variable name. Good practice is to include the units for numerical variables or the coding scheme for categorical variables. Let’s add some for practice:\nFor the variable sbp click on the variable label cell and enter “Systolic blood pressure (mmHg)”, i.e. the brief description of the variable with the units included take from the codebook table.\nNow click on the variable label for ses and enter the description provided in the codebook table (you can just copy and paste it from the table).\nFeel free to update the variable labels for some other variables if you wish, but once you’ve know how to do it you can move on.\n\n\n\n\n\nIt is very important that variable types are correctly set in SPSS (and all stats packages), because it affects the way analyses and other things like graphs may work, with incorrectly defined variable types resulting in errors. To set the variable type go to the Variable View and click on the relevant cell under Measure (it will either say “Scale” or “Nominal”) and select the correct type from the drop-down menu. SPSS will set the Measure based on the nature of the variable: for any variables with any non-numeric (i.e. letters or special characters) values the variable Type will be String and the variable Measure will be Nominal, and for any variables with only numeric (discrete or continuous) values the variable Type will be Numeric and the variable Measure will be Scale. However, these automatic choices may not be correct for the dataset!\nFor our dataset let’s start with sbp. What do you notice about its Type and Measure? The Type is String and the Measure is Nominal. What about if you click on the Measure cell? Only nominal and ordinal options are available! This means that at least one value in the variable is non-numeric, i.e. a letter or special character. Unfortunately, there are no easy ways to check which values are non-numeric (there are ways using more complicated functions, but we don’t have time for that here), but if you create a frequency table (see later for how to do this) and scroll through all the values you may be able to spot the problem values. However, to save time here the issue is observation number 93, which has a value of 1o3, i.e. with a letter “o” instead of a zero.\nTo update the erroneous value go to the Data View and either scroll down to the value (observation 93) and update it, or click on the sbp column variable name/heading to select the whole variable, then press ctrl and f to open the Find and Replace - Data View tool (below the Find tab it should say “Column: sbp”), then click on the Replace tab and then in the Find text box enter “1o3” (without the quotes) and then in the Replace text box enter “103” (without the quotes) and press the Replace button at the bottom to correct the value.\nOnce the error has been updated go back to the Variable View window and update the variable Type for sbp to Numeric, because only Numeric type variables can be Scale variables (sorry for the confusing terminology, I don’t like SPSS’s choices!). Click on the relevant cell for sbp under Type and click the little box with three dots that appears to the right. Then select Numeric and then OK. As there are no more values with strings or non-numeric characters in you should now be able to update the variable’s Measure to Scale!\nYou can go through all other variables and ensure the variable Type and Measure characteristics are set correctly if you wish, but once you’re happy how this is done feel free to move on.\n\n\n\n\n\nCategorical variables must use some type of coding or coding scheme to be understandable. This just refers to how categorical variables’ levels are stored (e.g. our sex variable has two levels: male and female). There are two approaches:\n\n\nString/character coding: each categorical level is referred to via a string (i.e. a set of letters, numbers or special characters). E.g. sex might take either the value “male” or “female”.\nValue coding: each categorical level is referred to via a number, with the number linked to a “value label” that then describes what that level represents. E.g. sex might take either the value 1 or 2, where 1 is linked to the value label “male” and 2 is linked to the value label “female”.\n\n\nYou can use either approach in SPSS and you shouldn’t have any problems. However, as value coding is often used and requires more understanding of SPSS to add/edit we will follow this approach so you can get the practice.\nSo let’s set-up value labels for our categorical variables sex and ses. To do this we must ensure that they are correctly coded with numbers and that suitable value labels are attached those numbers. In “real life” we would repeat this process for all other categorical variables. Before making any changes though we must first check each categorical variable to find out what levels exist and if there are any errors – you might be surprised. The easiest way to do this is to generate a frequency table: a table listing the number of cases the percentage of cases with each level.\nFrom the main menu go: Analyze > Descriptive Statistics > Frequencies. Note that SPSS tries to work out the type of variable based on the values present, e.g. if there are any non-numerical characters it will assume a categorical variable. In SPSS categorical variables are represented by three coloured circles, and numerical variables by a small ruler symbol. However, these assumptions will not necessarily be correct. Let’s look at sex and ses. Either double click on the sex and ses variables in turn or click once on each while holding the shift button and then click the right pointing arrow to add them. Then ensure the Display frequency tables button is ticked and click OK. The results will appear in a new output window, which is the other main window in SPSS where all results will be displayed along with any error messages.\nNow look at the frequency table to see what levels are present for sex and ses and how they are coded. Note that sex has been entered as a string variable (with levels recorded as words) and ses has been entered as a numeric variable (with levels recorded as numbers). Now you can see why frequency tables are so useful for data cleaning. For sex you can see that there are five levels, and it’s clear that three are errors (both the fact there are clear spelling mistakes/odd values and the fact they only occur once indicate they are mistakes). Specifically the levels “1”, “FM” and “mal” need correcting.\nWe therefore need to first clean/correct the error values, and then convert all values to numerical values and then add the labels. Look at the codebook above and the “Description (units/level coding)” column for the correct coding to apply to each level. To correct the mistakes go back to the Data View and click the top of the sex column (click on the word sex) to highlight the whole column. Then hold ctrl and f to open the Find and Replace - Data View tool. Then in the Find text box enter “1” (without quotes) and in the Replace with text box enter “male” (without quotes) and then click Replace All. Repeat this find and replace process for all other levels that are errors with the obvious correct word. Then use the find and replace process to change all “male” values to the number 1, and all “female” values to the number 2. We can now add value labels to these numerical codes. Be sure to add labels to the correct numerical values or they won’t display.\nNow go back to the main window Variable View and click on the Values cell for sex where it currently says “None”. You’ll see a small box appear to the right of the cell with three dots in it – click on this to open the Value Labels tool. Then click the Value text entry box and type 1. Then click the Label text entry box and type “male”. Then click Add. Then repeat for Value = 2 and Label = “Female” (make sure you remember to click Add). Then once both value labels are added click OK.\nNext let’s correct (if necessary) and code ses. Go back and look at the frequency table for ses. We can see there are four levels with one obviously incorrectly coded level of “11” only observed once. Clearly this should be coded as “1”. Use the find and replace process detailed above to replace level “11” with “1”, and then attach the correct value labels to the levels 1, 2 and 3 based on the information provided in the codebook table (1 = low, 2 = medium and 3 = high).\nNote: variable labels and value labels are not stored in Excel files, so to retain them you must first save your dataset in SPSS “.sav” format. This can be done by going from the main menu to: File > Save As, and then selecting a folder and file name. The .sav format should be the default selected, but ensure that you save your updated dataset in this format."
  },
  {
    "objectID": "5-preparing-a-dataset.html#exercise-check-numerical-variables-for-obvious-errors",
    "href": "5-preparing-a-dataset.html#exercise-check-numerical-variables-for-obvious-errors",
    "title": "",
    "section": "Exercise: check numerical variables for obvious errors",
    "text": "Exercise: check numerical variables for obvious errors\nVideo instructions: check numerical variables for obvious errors\nWritten instructions: check numerical variables for obvious errors\n\n\nRead/hide\n\n\n\nAll we can really hope to find are clearly erroneous values, such as values that are outside the plausible range of values for the variable based on what it is measuring or what we restricted it to via study design, but more modest errors won’t be detectable.\nIf you have a numerical variable with relatively few distinct/unique values, such as age, then probably the easiest check is another frequency table. Produce a frequency table for age and what do you notice about the extreme values (i.e. the lower and upper ends)? One person has an age of 4. This is clearly an error, as the study only recruited participants aged 18+. What to do? There are three choices: 1) leave it as it is, 2) update it or 3) delete it. Option 1 is not a good idea because we know it must be an error. Option 2 would be ideal, but you can’t always do this. In this case an age of 4 could actually be an age of 24, 34, 40, 41 etc, so we would have to be able to go back and contact the participant to check their age if we wanted to update this. Where this is not possible you could conduct a sensitivity analysis by running your analysis with this age value as 40 and then again with the value updated to 49. That way you see what happens when you assume it was actually either end of the possible value range of the likely true value. Or you may take option is 3 and delete the value (but this should be reported in your methods). However, here we’ll assume we were able to check and the correct age should have been 40. To update this go to the Data View, right click on the age variable name and select Sort Ascending. The data should now be ordered with the participant with age 4 at the top (or you could use the find and replace process detailed above). Update their age to 40.\nWhen numeric variables have lots of different unique values, such as when they include decimals, using a frequency table becomes less easy. A better option is to produce a dotplot (also called a dotchart or Cleveland dotplot/dotchart), which is like a histogram but each value is displayed as a dot, with dots from observations of values in a similar range (i.e. in the same “bin”) being stacked like a histogram. As BMI has very few (maybe no) repeated values a frequency table will be huge. Let’s produce a dotplot instead. From the main menu go: Graphs > Legacy Dialogues > Scatter/Dot. Then click Simple Dot > Define. Then in the Simple Dot graph tool add the variable bmi into the X-Axis Variable box. Then click OK. Looking at the graph you’ll see a huge stack of values of similar values, and then far out to the right on the x-axis you should clearly see an anomalous value (a circle) that must be an error because it’s so high. Double click on the graph which will open it in a new window where you can edit the graph. Now click on the error value, which should highlight all circles in yellow, and then click on it again and only the error circle should be highlighted. Then right click on the circle and choose Go to Case. This will take you to the relevant observation in the Data View where you can update it. Again we have three choices, and let’s assume we could go back and get the correct value either from the participant or an earlier dataset (e.g. if weight and height were measured separately), and it was actually 28.175.\nNow re-run the dotplot for BMI and what do you see? Again there is a value to the right of the distribution that is a far way to the right (higher) than any other BMI value. This may be considered an “outlier”. Outliers are extreme values that are far greater/smaller than any other values in a numerical variable. There are no clear thresholds beyond which a value is considered an outlier, but as you can see probably the easiest way to spot possible outliers is via a dotplot. Once you’ve identified any possible outliers such as this very high BMI value you should check to see if they are obvious errors and correct them if possible. However, if they appear to be genuine values the recommended approach is to conduct any analyses that involve the relevant variable(s) with and without the outliers present, i.e. repeat your analysis but delete the observations (e.g. individuals) who have the outliers and see what changes. Then simply clearly report the results of both of these analyses and any implications for the interpretation of your results. This is known as a sensitivity analysis. Broadly speaking, if removing outliers makes an important difference for your results then your results are probably not very robust to start with. Where you have plenty of good quality data removing the odd outlier from an analysis will rarely make much difference unless it’s a relatively hugely larger/smaller value. As you’ll have seen from the dotplot there are no outliers in this dataset."
  },
  {
    "objectID": "5-preparing-a-dataset.html#a-final-note-on-data-cleaning",
    "href": "5-preparing-a-dataset.html#a-final-note-on-data-cleaning",
    "title": "",
    "section": "A final note on data cleaning",
    "text": "A final note on data cleaning\nWe will end our data cleaning there having seen how to clean and prepare all variables as best as we can in isolation. However, with a real dataset these steps would just be the basics, and we would want to look further to check for errors and anomalous values by also looking at relationships between variables. For example, if we looked at a scatter plot of sbp vs age we may see a suspiciously high value for sbp for a very young participant, i.e. one that would be very unlikely in such a young individual. We could then check this with any prior data or the participant if we were able to.\nNote: generally speaking you should not delete any observations (e.g. patients) from any variables when preparing your data. When it comes to analysing your data the presence of missing values in some variables may be a problem, and in the most simplistic approach you may have to conduct “complete case” analyses by removing any observations with missing values for variables in the analyses. However, this is not without implications for possible bias (https://stefvanbuuren.name/fimd/ch-introduction.html), and you should always compute and present the frequency of missing data if present.\nNote: if you are concerned that one or more observations for one or more variables are errors but you cannot confirm this then a good approach is to run your analyses with and without those data points, and then in your results explain if this made any difference to your conclusions and discuss the implications accordingly. This is known as a sensitivity analysis."
  },
  {
    "objectID": "7-describing-a-population.html",
    "href": "7-describing-a-population.html",
    "title": "",
    "section": "",
    "text": "If you are happy with the concepts of sample statistics and inferential statistics, and with the idea and interpretation of a confidence interval, then feel free to skip the below overview of these issues and move to the SPSS instructions and exercise. However, if you feel unsure about these issues or want a refresher then we suggest you first read the below.\n\n\nRead/hide\n\n\nNote: the following brief overview of univariate sample statistics (i.e. what are probably more commonly referred to as descriptive statistics) and univariate inferential statistics assumes we are working with samples that have been taken via probability sampling methods, like a simple random sample. The underlying mathematical theory does not apply to non-probability sampling approaches, and although you can certainly calculate the same statistics it would be unclear how robust or accurate the results would be.\n\nIn the last section we looked at calculating univariate (single variable) sample statistics, which are usually more commonly just called descriptive statistics, to describe the characteristics of a sample. In this section we will look at calculating univariate statistics that allow us to describe, at least probabilistically, the characteristics of a population. By “probabilistically” I mean that we can describe a characteristic in a sample via a given summary statistic without error, because we have data on every member of the sample (e.g. the mean age in a sample is the exact mean age of that sample). However, when calculating the value of a summary statistic for a characteristic in a target population we only have the (usually vastly smaller) sample to work with. This means we can only say what the likely value of the summary statistic is, to a given level of precision (e.g. the mean age in a target population for which we have a sample is just an estimate of the true mean age in the population, to a given level of precision).\nWhen describing characteristics based on single variables in a sample, and when describing their likely values in the target population that the sample came from, we typically use the same univariate summary statistics. As we are usually primarily interested in describing the central tendency or typical value of a variable in a sample or populaton we are usually aiming to summarise numerical variables using their mean (assuming they are approximately normally distributed) and categorical variables using the proportion or percentage frequency of each category level.\nHowever, when we describe characteristics in a sample we can describe them via summary statistics exactly (at least for point at which the data were collected, and assuming no errors or bias in the study methods), but when we describe characteristics in a population via our sample data we call the same summary statistic a “point estimate” of the relevant “population parameter”. The population parameter is then the true but unknown value of the summary statistic in the target population, which we could measure in theory if we had a census (i.e. if we measured the variable from every member of that population). And the point estimate is our best estimate of the unknown population parameter. For example, the mean age in a sample is the point estimate, or best estimate, of the unknown value of the mean age in the target population (where the mean age in the target population is the unknown population parameter).\nHowever, on its own the point estimate tells us very little about the likely value of the population parameter, because it doesn’t take into account the sampling variability in the estimate. We estimate the population parameter from a random sample, but if we took another random sample we would get a different estimate for the population parameter. However, we can use statistical theory to take account of the sample size and variability in the variable that we are estimating the population parameter from, and use this to calculate a measure of precision. We will not go further into the details here (but see the lecture materials on probability distributions, confidence intervals and hypothesis testing for more information), but ultimately we can use this to calculate a “confidence interval” for our point estimate. This confidence interval then allows to make a probabilistic judgement about the likely value of the population parameter.\nConfidence intervals take the form of a lower and upper value that surround the point estimate. For example, let’s say we estimate the mean age in a target population as 30, based on computing the mean age in a simple random sample from that population. Let’s assume we then compute a 95% confidence interval around that mean with lower and upper values of 25 and 35. We would usually write these statistics all together as:\n\nMean age: 30 (95% CI: 25, 35)\n\nWhere CI = confidence interval. Note, we separate the lower and upper interval via a comma not a hyphen “-”, as hyphens can be mistaken for negative symbols.\nWhat this confidence interval is telling us is that if we were to repeat our random sampling process infinitely many times and each time we were to compute the mean age and its 95% confidence interval then 95% of the time those 95% confidence intervals would contain the true (but unknown) value of the mean age in the target population. Hence, we can never be certain what the exact mean age of the target population is based on the confidence interval, but it does give us an interval (i.e. a range of values) within which it is likely to lie. Some would say we can be “95% confident” the true (but unknown) population parameter lies within this interval, although technically speaking this phrase “95% confident” doesn’t really have any precise, concrete meaning mathematically.\nThere is nothing special about a 95% confidence interval compared to a higher or lower level of confidence, such as a 90% confidence interval. However, for a given variable as you increase the confidence level the interval increases. For example, for our hypothetical situation above we may find a 99% confidence interval around our mean age of 30 is 10 and 50. While we may find a 80% confidence interval would be from 28 to 32.\nTherefore, there is a trade-off in the usefulness of the confidence interval as you vary the confidence level. Too high a confidence level results in a high level of certainty but for a very wide interval, while too low a confidence level results in a low level of certainty but for a very narrow (precise) interval. The widespread use of 95% confidence intervals in the health sciences is largely just a convention, i.e. due to tradition. For a given confidence level, all else being equal, if we increase the sample size the interval will become narrower. Therefore, if you have sufficient sample size then a higher level of confidence would clearly be preferable.\n\nNote: it is really important to be aware that the accuracy or validity of the above interpretation of a confidence interval depends entirely on the only uncertainty coming from sampling variability, i.e. the use of a probabilty sample. Confidence intervals tell you nothing about any other sources of bias, such as selection bias, or any non-random missing data. Therefore, if the study creates other sources of bias, which is effectively inevitable, the accuracy of the confidence interval will be affected, usually to an unknown extent. For example, if older people were less likely to agree to have their age recorded (selection bias), then a 95% confidence interval for the mean age of that population would no longer include the true (but unknown) value of the mean age in the population 95% of the time (it would be lower). Again, it is usually not possible to estimate the effect of such biases, so they must be judged more qualitatively via understanding the possible sources and degree of bias in the data.\n\n\n\n\n\n\nAim: estimate the mean systolic blood pressure (mmHg) and the prevalence of hypertension and their associated 95% confidence intervals to allow statistical inferences to be drawn about the typical level of systolic blood pressure and the prevalence of hypertension in the target population.\n\n\nFirst, load the analysis-ready “SBP data final.sav” SPSS dataset.\nBelow are instructions on how to compute univariate inferential statistics for 1) an approximately normally distributed numerical variable and 2) a categorical variable in SPSS. Follow these instructions to get practice and then complete the following exercise where you are asked to repeat the process for some different variables.\n\n\n\n\nSorry: at present there is no video-based set of instructions for this method.\nWritten instructions: calculating means, percentages and their 95% confidence intervals in SPSS\n\n\nRead/hide\n\nComputing means and their 95% confidence intervals for approximately normal variables\n\nComputing a confidence interval requires a probability distribution in the calculation. We therefore we need to make an assumption about what probability distribution would be appropriate given the probability distribution we assume the data to have come from based on our knowledge of the data generating process and the shape of the distribution (as visualised via a histogram). Below we will check that the data are approximately normally distributed.\nWhen a numerical variable is approximately normal we can compute a confidence interval based on a normal distribution. However, we will actually compute one based on a t-distribution. This is because a t-distribution is appropriate for continuous data that follow the same broad bell-curve shape, but it allows for more variation at smaller sample sizes (thicker tails to the distribution), resulting in less risk of bias. At larger sample sizes the normal distribution and the t-distribution become equivalent, so there is no real reason not to use the t-distribution over the normal when possible.\n\n\nNote: technically speaking we are just considering analytical confidence intervals here. However, there are empirical approaches such as bootstrap confidence intervals, which you may can read a bit about here\n\n\nSo let’s check that the variable is approximately normally distributed, otherwise the mean may not be the best measure of the typical value of the variable, and its 95% confidence intervals would not be accurate (technically speaking they may under or over cover the true interval on average). We’ll use the salt variable, which measures each individual’s reported typical salt consumption per day in grams.\nFrom the main menu go to: Graphs > Legacy Dialogues > Histogram Then in the Histogram tool click on the salt variable and add it to the Variable: box by clicking the blue arrow next to the box. Tick the Display normal curve box below. This will display a theoretical normal distribution curve based on the mean and SD of the variable, which helps you to see how closely the data match the distribution we would expect if they were normally distributed. Then just click OK.\nWhat does the distribution of the data look like? I would say there is clearly some right skew, but it’s not huge and we can probably ignore it without causing too much bias to our results (certainly for the purposes of this demonstration).\nNow let’s compute the mean of the variable and a t-based 95% confidence interval to allow us to make a statistical inference about the likely value of the true (but unknown) mean salt intake per day in the target population.\nTo compute our t-based 95% confidence intervals we will actually tell SPSS to do a one-sample t-test. We cover other t-tests in section 5, but we don’t actually look at the one-sample t-test as it is rarely used. If you wish to read about it a brief overview with SPSS instructions is here. However, for our purposes we are simply using the one-sample t-test as a way to get SPSS to compute the mean of a variable plus its 95% confidence interval assuming (i.e using) a t-distribution.\nTo do this from the menu go: Analyze > Compare Means > One-Sample T Test. Then add the salt variable into the Variables: box using the arrow and then click OK.\nYou will get three tables returned in the results window. The “One-Sample Statistics” table presents the sample size for the variable, which is useful because you should always present the sample size (i.e. count) for any estimate and the relative frequency (e.g. percentage) of complete/missing data for the variable.\n\n\nNote: by default SPSS removes any observations from the variable with missing values when computing the results.\n\n\nYou can then look at the “One-Sample Test”. The columns we are interested in are the “Mean Difference”, which actually represents the mean. The reason for this is that technically the one-sample t-test compares the mean of the variable to an assumed population mean, which by default SPSS sets as 0 (you can alter it on the One-Sample T Test tool). Therefore, the variable’s mean - 0 is just the variables mean. You can also see the sample mean or point estimate of the population mean in the “One-Sample Statistics” table. Then finally we can see the t-based 95% lower and upper confidence intervals under the “95% Confidence Interval of the Difference” “Lower” and “Upper” columns.\n\nComputing proportions/percentages and their 95% confidence intervals for categorical variables\n\nNote: this approach is applicable to both binary variables or categorical variables with three or more category levels. However, in the case of a categorical variable with three or more category levels we actually just treat each category level as a separate binary variable in practice. For example, for the variable “education level” with category levels “none”, “primary”, and “more than primary” we estimate the proportion/percentage of individuals in the 1) “none” category vs “primary or more than primary”, 2) the “primary” category vs “none or more than primary”, and 3) the “more than primary” vs “none or primary”.\n\n\nAs above, to compute a confidence interval requires an appropriate probability distribution given the data. Clearly, a binary variable cannot formally be normally distributed: it can only take values of 0 and 1. However, as long as the underlying proportion is not “too extreme” we can actually treat it as an approximately normally distributed variable and compute normal-based confidence intervals. By “too extreme” the usual rule of thumb is the proportion for the variable must be >0.1 and <0.9. Beyond these limits the implied distribution becomes too non-normal for the approximation to be useful. For example, the lower/upper confidence interval will often be <0 or >1, and the bias in the accuracy of the confidence interval’s coverage becomes more substantial.\nAlternatively and more appropriately, we can use a probability distribution formally applicable to a binary variable and our assumed data generating process, such as the binomial distribution. Generally, such a confidence interval is called a binomial proportion confidence interval.\nUnlike for numeric variables that are approximately normal there are actually a large number of different approaches available though, and there is a surprisingly limited amount of research comparing these methods under different situations. However, the current rough consensus appears to be that the “Wilson score interval” method with the continuity correction generally does well. There is a Wikipedia page with a great summary of the situation [here] (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Comparison_and_discussion).\nSPSS offers the option to calculate a big range of these different binomial proportion confidence intervals all at the same time. Let’s see how we can do this for a categorical variable with three category levels for each category level.\nFrom the menu go Analyze > Compare Means > One-Sample Proportions. Then add the ses variable from the [Variables:] box into the Test Variable(s): box using the blue arrow.\nNext click on the button labelled “Confidence intervals” at the top right. Then under Select Type(s) tick Clopper-Pearson (“Exact”), which is based directly on the binomial distribution, Wald, which just uses the normal distribution (often called normal approximation confidence intervals), Wilson Score and lastly Wilson Score (Continuity Corrected). Then click Continue.\nNow we need to decide which category level we want to calculate our proportion and confidence intervals for. The ses variable has three categories coded 1, 2 and 3, corresponding to low, medium and high socio-economic status.\nLet’s look at low status first. In the Define Success box select the Value(s) button and in the box to left of the button enter the value 1 (low socio-economic status). Then click OK.\nAll the results we want are in the “One-Sample Proportions Confidence Intervals” table. As you can see the results for each confidence interval method are on a separate row, and you can see which method is on which row under the “Interval Type” column. Under the “Observed” heading there are three columns giving the number of “Successes”, which means the count of observations (i.e. individuals) in the category we selected, the number of “Trials”, which just means the number of observations or the sample size of the variable, and the corresponding “Proportion”. You can of course multiply this by 100 to convert to the percentage scale. We then have the lower and upper 95% confidence intervals under the correspondingly named final two columns. As you can see for this variable and these methods they actually give very similar results, and indeed the method using the normal approximation (“Wald”) is just as good as the other methods.\nYou could then repeat this process for the other category levels in the ses variable by simply changing the value in the Define Success: Value(s): box, but feel free to move onto the exercise if you don’t feel the need to try this now.\n\n\nNote: if you have a categorical variable coded with letters or words instead of numbers you can also just enter the relevant letter or word into the Define Success: Value(s): box.\n\n\n\n\n\n\nOpen the “Exercises.docx” document and scroll to the “Estimating key population characteristics: blood pressure and hypertension” section. Using the methods practiced above calculate the mean blood pressure and its 95% confidence intervals and the prevalence of hypertension (as a percentage) and its 95% confidence intervals and record them in the space indicated. As per the instructions in the Exercises document, for the mean estimate calculate the confidence intervals using the t-based method and for the prevalence estimate calculate the confidence intervals using the Wilson Score method.\n\nYou can check your answers below once you have completed the exercise:\n\n\nRead/hide\n\n\n\n\nPopulation mean systolic blood pressure (mmHg): 126.5 (95% CI: 124.9, 128.2)\nSample size for estimate: 543\n\n\n\n\n\nPopulation prevalence (%) of hypertension: 25% (95% CI: 21.6%, 28.9%)\nFrequency and sample size for estimate: 136/543"
  },
  {
    "objectID": "8-independent-t-test.html",
    "href": "8-independent-t-test.html",
    "title": "",
    "section": "",
    "text": "The independent t-test allows you to compare whether two independent groups differ in their mean values of some numerical outcome, or less formally whether individuals in one group have higher/lower values for some numerical outcome than individuals in another group, on average. Therefore, it is suitable when your outcome is numerical and your single independent variable is binary. For example, if you wanted to analyse whether a group of women had a different mean systolic blood pressure from a group of men. Therefore, the independent t-test is suitable for when your outcome is numerical and your independent variable is binary. Strictly speaking the outcome should be continuous and normally distributed for each group, but the independent t-test is very robust to non-normally distributed outcomes and as long as the outcome isn’t strongly skewed for each group it should be okay to use this test. Note: with the independent t-test the independent variable is binary and might naturally consist of a categorical variable with just two levels (like sex), or we might construct a binary variable from a categorical variable with >2 levels (e.g. reclassifying socio-economic status from low, medium and high levels to just low/medium and high levels by pooling individuals in the low and medium levels), or we might create a binary variable from a numerical variable based on a threshold cut-off (e.g. BMI ≤20 vs BMI >20). We’ll look at the last of these scenarios as it requires a bit more preparation, so you will know how to do this if needed in SPSS.\n\n\n\nUsing the SBP final dataset we’ll see how we can use the independent t-test to compare whether the mean systolic blood pressure (mmHg) differs between individuals aged ≤40 compared to those aged >40, and then you can try a similar analysis by yourself.\n\n\n\n\nLoad the “SBP data final.sav” SPSS dataset.\n\nVideo instructions: create a binary independent variable from a numerical variable\nWritten instructions: create a binary independent variable from a numerical variable\n\n\nRead/hide\n\nNext we need to create a two-level categorical variable defining our younger and older individuals. Usually you should choose the cut-point at which you define your two-levels based on theory or some sensible motivation (e.g. the age at which prior research suggests blood pressure may often change), but here let’s just compare those individuals above and below the median age, which is 40. We therefore want to create a variable that classifies every individual as either ≤40 or >40.\n\nFirst, we can use the Compute Variable tool to create a new variable based on a logical test of whether the value in the original age variable is ≤40 or not. This will create a new variable that has the value 0 if the participant’s age is >40 and the value 1 if their age is ≤40 (i.e. if the logical check we asked SPSS to do on the original age variable is TRUE then the new variable’s value is 1). From the main menu go: Transform > Compute Variable. Then in the Compute Variable tool enter “age_young_old” as the name for the new variable in the Target Variable: box. Then in the Numeric Expression: box enter:\n\n\nage <= 40\n\nThen click OK.\n\nThe output window will appear. Minimise this to go back to the main window and go to the Variable View. You should now see our new variable age_young_old has appeared. Let’s give it a variable label so it’s well described in SPSS: Age (0 = >40 / 1 = <=40). Then let’s give it some value labels so each level is well described. As it was created from a logical expression it takes only two values: 0 or 1. 1 should be coded as “≤40”, because when the participant’s age was less than or equal to 40 the logical expression would be true and the new variable would take the value 1. Similarly 0 should be coded as “>40” as the logical expression would be false and the value set to 0. Check back to the instructions on adding value labels or ask if you need help.\nLastly, once that’s done you should always do a quick check that the variable has been created correctly using a “cross tabulation” between our old and new age variable. In the main menu go: Analyse > Descriptive Statistics > Crosstabs. Then in the Crosstabs tool add the original age variable into the Row(s): box and the new categorical binary age variable into the Column(s): box and click OK. If you’ve created the new age variable correctly all ages ≤40 should be counted in the age <=40 column and all ages >40 in the new age variable age >40 column. Always perform such checks when creating new variables!\n\n\n\n\n\n\n\nRead/hide\n\nWhenever you run a statistical analysis you must ensure that the assumptions of the analysis are not violated, otherwise the results may not be valid or even meaningful. For simple analyses like the independent t-test we can check the assumptions before we run the test, but for more sophisticated analyses such as regression models we typically check the assumptions after running the analysis because SPSS produces the information to make the necessary checks only after running the analysis.\nThe independent t-test makes the following assumptions:\n1. Continuous outcome\nTechnically an independent t-test assumes a continuous outcome variable, but as long as assumptions 2 and 3 below are satisfied it’s fine to use a discrete outcome with an independent t-test. Here our outcome is continuous. Note: ideally when analysing a discrete outcome you would use a more specific model like a Poisson or negative binomial model, but this is beyond the scope of this introductory course.\n2. Independent groups (hence the “independent t-test”)\nThis means the observations in each group cannot be related. You can only really verify this by knowledge of your study design. With the SBP data the study took a simple random sample of individuals and we are then dividing them into two groups based on an age threshold. Therefore, by definition the two groups are statistically independent as all observations represent separate, randomly sampled individuals.\nTypically groups are only not independent in two situations. First, if you were taking repeated measurements from the same set of individuals but treating the measurements at each time period as different “groups”. Here there would be correlations between the measurements at the different time periods within individuals. Second, if you were taking measurements from two groups of separate individuals, but the individuals in the two groups were not statistically independent. For example, if there were different families with members in both the age groups, or if there were individuals from the same work places within each group. Again, in such situations there would be correlations between members of the same family or work place (e.g. due to genetics or shared risk factors etc).\nHere our groups or observations are clearly independent as the individuals in the study were randomly sampled and the two groups contain separate individuals.\n3. The outcome is approximately normally distributed within each group\nNote: technically it is the “residuals” or “model errors” that need to be normally distributed, which are the differences between the observed values of the data and the “model predicted values”. For a t-test these are simply the differences between the observed values of the data and the mean of the relevant group that the observation comes from, which actually means that for a t-test the residuals are identical to the observed values and so you can just view the distribution of the observed data. For more complicated regression models we’ll see that we need to calculate and plot the residuals separately.\nTo check the distribution of the residuals within each group you can use statistical tests, but most statisticians would recommend using graphical methods. We can visually check these two distributions easily in SPSS using histograms as follows. From the main menu go: Graphs > Legacy Dialogues > Histogram. Then add the sbp variable into the Variable: box, then add the age_young_old variable into the Rows: box, tick the Display normal curve box and then click OK. You’ll see a histogram for the sbp variable for each age group. As you can see the outcome approximates a normal distribution fairly well for the ≤40 age (this is probably as good as you would see with real data), and is slightly right skewed for the >40 age group. However, as previously mentioned the t-test is pretty robust to slightly skewed outcomes so this is nothing to worry about, and we can assume this assumption is met. Below are some examples of suitably and unsuitably distributed data for you to compare with for future reference.\nExamples of suitably and unsuitably distributed data for t-tests\n\n\nRead/hide\n\nStudents (and researchers) often struggle with knowing whether an analysis’s assumptions have been violated or not, and for good reason because it usually involves judgement based on experience. For the t-test like most analyses requiring normality there is no agreed “threshold” for when the skewness of a variable is considered “non-normal”. However, as t-tests and regression models are fairly robust to some non-normality the following examples should hopefully give you a sense of when you are okay to go ahead and when you are violating the assumptions and need to transform the data or use a different model.\nA. Probably suitably approximately normally distributed data within each group for a t-test, with n = 50 per group:\n\n\n\nApproximately normal (and approximately equal variance) data in each group of a t-test, n = 50 per group\n\n\nWith only 50 data points per group the distribution is pretty “lumpy” but you can see an approximate normal distribution in both groups, although less so for the lower group. Also note: these data are generated from a normal distribution, so you can see that when your sample size is low even with artificially simulated data from a known normal distribution the resulting sample can often be only somewhat approximately normal! You can also see that the variance in both groups looks approximately normal, which satisfies our next assumption (see below).\nB. Highly suitably normally distributed data within each group for a t-test, with n = 500 per group:\n\n\n\nApproximately normal (and approximately equal variance) data in each group of a t-test, n = 500 per group\n\n\nNow with 500 data points per group simulated from the same distribution unsurprisingly the distribution looks much more suitably normal.\nC. Probably not suitably normally distributed data within each group for a t-test, with n = 50 per group:\n\n\n\nNon-normal (and non-equal variance) data in each group of a t-test, n = 50 per group\n\n\nYou may think it doesn’t look much different from the first image, and you’d be right. Again, it comes down to judgement but when sample sizes are small there’s a lot of subjectivity involved. These data are simulated from a right-skewed “version” of the normal distribution. If you are concerned one option is to run your test on the raw data and then transform the data (we’ll see later how to do this) and re-run the test and see if the results change much, and just report both. This is called a sensitivity analysis.\nYou can also see that there is a lot more variance in the lower group’s data, which would violate our next assumption (see below).\nD. Definitely not suitably normally distributed data within each group for a t-test, with n = 500 per group:\n\n\n\nNon-normal (and non-equal variance) data in each group of a t-test, n = 500 per group\n\n\nNow with 500 observations per group it’s clear that there is some definite right-skew in the distribution of both group’s data, and that there is considerably more variance in the lower group.\n\n4. Equal variances in each group\nThis means that the variance (dispersion or spread) of outcome values is approximately the same in each group. We can check this by looking at the p-value for a test of this assumption called “Levene’s test of equality of variances” or by comparing the histograms from the two groups. SPSS produces the results of the Levene’s test when we run the t-test, so we will evaluate the results of this test/assumption once we’ve produced our t-test results below. Note: if this test is significant (P<0.05) we can simply use the results from the version of the t-test that doesn’t assume equal variances in each group. The only disadvantage is that we lose a little bit of statistical power, but typically it’s not much and the results will be very similar. Hence, although we’ll interpret the “equal variances assumed” version of the t-test below it’s probably a better practice to just always use the more conservative but robust and safer set of results that do not assume equal variances is each group.\n\n\n\n\nVideo instruction: run the independent t-test\nWritten instruction: run the independent t-test\n\n\nRead/hide\n\n\nIn the main menu go: Analyse > Compare Means > Independent Samples T Test. Then in the Independent-Samples T Test tool add the sbp variable into the Test Variable(s): box. Then add our new independent variable for age group (at the bottom of the list) into the Grouping Variable: box and click Define Groups…. This is where we tell SPSS what numerical values code for each of our two groups, and what way round we want to compare the two groups. Remember we created the age variable so that it took the value “0” for individuals >40 and “1” for individuals ≤40. Let’s compare individuals >40 to individuals ≤40. When it comes to comparing the groups this tells SPSS subtract the mean outcome in group 0 from group 1, i.e. the mean systolic blood pressure for individuals >40 minus the mean systolic blood pressure for individuals ≤40. To do this ensure the Use specified values option is selected and add the value “0” to Group 1: and the value “1” to Group 2:.\nNote: you can of course swap the coding values around and compare individuals ≤40 to individuals >40 and your results will be reversed but otherwise identical. Neither choice is necessarily “correct” it just depends on which way round you want the groups compared. If, in your version of SPSS, there is a box called Estimate effect sizes below the Grouping variable: options make sure this is not ticked. Finally click Continue and then OK. You’ll now see the output window appear with our results!\n\n\n\n\n\nNow we’ve verified the first two assumptions of the independent t-test (independent observations and approximately normally distributed residuals) let’s look at the results tables where we can check the third assumption (equal variances in each group), extract the key results and interpret them.\nSo what do the tables mean?\n\n\nGroup Statistics table explained\n\n\nThe first table we see gives us descriptive statistics for the outcome variable for each level of our categorical independent variable.\n\nFirst, make sure you look at and record the values under the “N” column, which are the group sample sizes, and check they match what they should be. You should see there were 295 individuals in the ≤40 group and 248 individuals in the >40 age group. These sum to 543, which is less than the total number of individuals in the dataset: 556. There are three reasons why this might be: either some individuals are missing values for the outcome or the independent variable or both variables. We know from when we prepared the dataset that some individuals are indeed missing values for their systolic blood pressure. You can quickly check this by going to the Data view, right clicking on the sbp variable and selecting Descriptive Statistics. In the Statistics results table that appears you should see that there are indeed 13 individuals with missing values for sbp, and hence only 543 individuals with complete data on this variable, which explains the sample size of the independent t-test.\nNext make sure you look at and record the mean value of the outcome for each group under the “Mean” column. You should see the mean systolic blood pressure (mmHg) was 128.2 for individuals >40 and 125.1 for individuals ≤40 (rounded to 1 decimal place).\n\n\n\n\n\nIndependent Samples Test\n\n\nThis table gives us the results of the t-test we’re interested in, although SPSS lays out the results less than clearly in my opinion. Here’s what they all mean.\n\n\nLevene’s Test for Equality of Variances section columns explained\n\n\nUnder this part of the table are results that relate to a statistical test that is totally separate to the independent t-test called “Levene’s test for equality of variances”. This tests whether the variance (spread) of the outcome in each group is approximately equal. If they are not then this assumption is violated and we must only use the results from the version of the independent t-test that accounts for this.\nF\n\nThe F-value is the test statistic for the Levene’s test for equality of variances. You can ignore it as it is used to calculate the corresponding p-value of the test (see below), which is all we are interested in.\n\nSig.\n\nThe “Sig.” value is just the p-value associated with the test statistic (i.e. the F-value). For some reason SPSS always refers to p-values with a “Sig.” column heading. The null hypothesis for the Levene’s test is that the variances in both groups are equal. Therefore, if the p-value (“Sig.”) is <0.05 this indicates that the variances in the two groups are unlikely to be equal. If this is the case it is then safer to use the version of the independent t-test that does not assume equal variances. Results from both versions of the independent t-test are presented. Just look at the far left of the table and you can see the top row of results are for the “Equal variances assumed” version and the bottom row for the “Equal variances not assumed” version.\nHere you should see that the p-value for the test is 0.244, i.e. it’s >0.05. Therefore, we can assume the variances are approximately equal in each group. Hence, we can use the results for the independent t-test from the row called Equal variances assumed (see below).\nNote: SPSS only gives p-values to 3 decimal places, so when it says “0.001” it means the p-value is actually <0.001. Therefore, you should write P<0.001.\n\n\n\nt-test for Equality of Means section columns explained\n\n\nUnder this part of the table are the results for the actual independent t-test. We will use those from the row/version that do not assume equal variances.\nt\n\nThis is the value of the t-test statistic which is used when calculating the p-value. We can ignore this and just interpret the p-value directly (and of more use the confidence intervals).\n\ndf\n\nThis is the degrees of freedom which form part of the calculation of the test statistic (calculated as: n - 2). You can typically ignore this but it should closely match your “true” sample size, i.e. the number of genuinely independent observations.\n\nSig. (2-tailed)\n\nThis is the two-tailed p-value for the independent t-test. The “two-tailed” part means that it allows for the possibility that the difference between the two groups may be positive or negative, i.e. either group might have a greater mean than the other. The null hypothesis of the independent t-test is that the “true” difference between the two groups in the target population from which the sample was taken is exactly 0. Therefore, this p-value tells us how likely we are to have observed data giving a mean difference at least as great or greater (in either direction) as the one observed. Alternatively, you can more loosely interpret it as a probability measure of how consistent the data are with the null hypothesis of no difference. Again note that SPSS only gives p-values to 3 decimal places, so when it says 0.001 it means the p-value is <0.001.\nHere you should see that the p-value is 0.072.\n\nMean Difference\n\nThis is the difference between the mean of the outcome for the group that we set as “Group 1” minus the mean of the outcome for “Group 2”. For us this means the mean SBP of participants aged >40 minus the mean SBP of those aged ≤40. It therefore tells us the direction and size of any difference, and is therefore a key result/statistic from the test. You can swap the coding of the groups around if it makes more sense (just go back and re-run the analysis but change the coding), but the difference will be reversed (i.e. a positive difference will become a negative difference).\nHere you should see that the mean difference (rounded to one decimal place) is 3.1.\n\nStd. Error Difference\n\nThis is the standard error of the mean difference, which estimates the sampling variability associated with our estimate of the parameter in the target population. This is used when calculating the 95% confidence intervals and the p-value. You can ignore it and just interpret the confidence intervals and p-value.\n\n95% Confidence Interval of the Difference (Lower and Upper)\n\nThese are the estimated lower and upper 95% confidence intervals around the estimated mean difference, which is the point estimate or best single estimate of the mean difference in the target population. Formally speaking, if we repeated our study an infinite (or very large number) of times and each time we calculated the mean difference and the 95% confidence interval around those mean differences then 95% of those 95% confidence intervals would contain the true mean difference found in the target population (i.e. the mean difference that we would find if we sampled 100% of the target population). Informally speaking we can think of the 95% confidence intervals as defining a range of values that are consistent with the likely/probable true mean difference in the target population based on the sampled data.\nHere you should see that the lower and upper confidence intervals for the mean difference (rounded to one decimal place) are -0.3 and 6.4.\n\n\n\n\n\n\n\n\nIn a methods section you should explain that you used an independent t-test to analyse the data and justify why.\nIn a results section we would want to report the sample size, the mean of the outcome for each group, and the key results of the independent t-test (the mean difference, the 95% confidence intervals and the associated p-value). Therefore, we could write something like the following (using “n” as it is commonly used to refer to sample size):\n\nWe compared the systolic blood pressure among individuals aged >40 (n = 248) to those aged (n = 295). Among those aged >40 the mean systolic blood pressure was 128.2 mmHg and among those aged ≤40 the mean systolic blood pressure was 125.1 mmHg. Using an independent t-test (equal variances assumed) we estimated that the mean difference in systolic blood pressure between those aged >40 compared to those aged ≤40 was 3.1 mmHg (95% CI: -0.3, 6.4).\n\n\nNote: the convention is to compactly present confidence intervals for estimated statistics in brackets like above, indicating the confidence level of the confidence interval (usually 95%), and to use a comma “,” or possibly the word “and” to separate the lower and upper confidence interval, but not a dash “-” as this can look like a negative symbol.\nAlso note: we are focusing on the mean difference and the confidence intervals, not the p-value here as it tells us nothing more and far less than those results do.\nLastly, if you felt it wasn’t clear where the inferential result had come from, say if you were reporting result from different analyses, then you should certainly make this clear in the results section too, e.g. you might say something like “based on an independent t-test the mean difference was…”.\n\n\nDiscussion: interpret the direction and size of the difference in terms of the implications for practice and policy. Is the difference “statistically significant”, i.e. can we make a clear inference that there even is a difference on average between the groups, and if so is it a small difference, a medium difference, a large difference etc in terms of what is being measured and the implications for practice and policy?\n\n\n\n\nHow do we actually interpret these results then? Remember with statistical inference we are aiming to make a “probabilistic inference”, or a generalisation with some level of uncertainty, about the likely value of the population parameter of interest, which here is the mean difference in systolic blood pressure between individuals aged >40 compared to individuals aged ≤40 in our target population, based on the data from our sample. And our confidence intervals around our sample statistic (the sample mean difference) are what allow us to do that. More specifically, the confidence intervals around the mean difference tell us that the mean difference in systolic blood pressure between individuals aged >40 compared to individuals aged ≤40 in the target population is “quite likely” to be somewhere between -0.3 and 6.5 mmHg. Alternatively and equivalently we can interpret the results in terms of an “average individual” in our target population, and say that the confidence intervals tell us that in the target population the systolic blood pressure of individuals aged >40 is “quite likely” to be between 0.3 mmHg lower to 6.5 mmHg higher than individuals aged ≤40 on average.\nNote: strictly or technically speaking our confidence intervals tell us how much error is associated with our sampling process, and that if we repeated our study many times 95% of the time the 95% confidence intervals that we obtained around our estimated mean difference would contain the true mean difference in the target population. Therefore, our interpretation of the value being “quite likely” to fall within our given confidence interval range is somewhat informal and loose, but probably fine for most research purposes.\nAlso note: critically this interpretation assumes that all other sources of bias are negligible, which is extremely unlikely! Therefore, you should always treat inferential results very carefully, and evaluate them in the context of how much likely bias there is in the results other than sampling error, which is the only source of bias that inferential statistics account for. E.g. if you knew there was likely a large amount of other sources of error in the study, such as uncontrolled confounding, then the 95% confidence intervals would not accurately reflect the total error or bias in the results because they can only account for sampling error in the absence of all other errors/bias.\nConsequently, our confidence intervals indicate that we can’t have much confidence over whether there is even a positive or negative difference in systolic blood pressure between these two groups in the target population, let alone how big any difference is with any great precision. However, the result does tell us that the practical significance or clinical significance of the true difference in mean systolic blood pressure between these two groups is likely to be small, given the confidence intervals suggest it is likely to be at most just 6.5 mmHg (the confidence interval furthest from 0). Always remember though, this is a probabilistic result (not certain) and there is always still a non-negligible chance that the true difference is greater, possibly much greater, than we estimated, i.e. outside the range of the 95% confidence intervals, and as above this is actually inevitable with any study unless completely free of other sources of bias.\nLastly, note: when setting up the t-test if we swapped the order of the comparison around we would get the same result but expressed as the ≤40 age group compared to the >40 age group, and so we’d get a mean difference of -3.1 mmHg (95% CI: -6.5, 0.3). Note the 95% confidence interval order is also reversed.\n\n\n\n\nThe key limitations of the independent t-test is the inability to look at differences between >2 groups or between the outcome and numerical variables, and the inability to control for other covariates. Therefore, with observational data there is a very high risk (essentially a certainty) that the result will be biased due to confounding to a greater or lesser extent, and with experimental data you have no ability to increase the precision of your estimate by controlling for variables that may explain some of the variation in outcomes. As we will see in the linear regression practical once we control for other variables, which likely confound the sbp ~ age relationship, we find there is apparently evidence of a (small) relationship between age and SBP.\n\n\n\nUsing the “SBP final data.sav” dataset and via the process outlined above use an independent t-test to analyse the relationship between BMI and systolic blood pressure.\n\nDivide bmi into two groups based on BMI values <30 kg/m² and those ≥30 kg/m², which are the values typically used to define obesity when defined in terms of BMI.\nCompare individuals with BMI values <30 kg/m² to those with BMI values ≥30 kg/m² using the independent t-test.\nExtract the mean difference and confidence intervals around this estimate.\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Independent t-test: the relationship between BMI and systolic blood pressure.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: sample size, group sizes and group outcome means. Also be sure to include sufficient details about the outcome variable and the comparison made, including how the two independent groups were defined, as well as the type of analysis used, and of course the key inferential results. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\n\n\n\n\nRead/hide\n\nUsing an independent t-test I analysed the relationship between BMI and systolic blood pressure (mmHg) for individuals having low compared to high BMI (<30 kg/m² compared to ≥30 kg/m²). Out of a total sample size of 516 individuals, 419 individuals had a BMI <30 kg/m² (mean systolic blood pressure = 121.6 mmHg) and 97 individuals had a BMI ≥30 kg/m² (mean systolic blood pressure = 146.7 mmHg). This represented a mean difference of -25 mmHg (95% CI: -28.9, -21.2) for those with a BMI <30 kg/m² compared to those with a BMI ≥30 kg/m². Therefore, having a BMI < 30 kg/m² appears to be associated with a substantially lower systolic blood pressure on average than having a BMI ≥30 kg/m². However, the key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an observational cross-sectional study like this). Therefore, this is likely to represent a biased estimate of the independent relationship between BMI (as defined/grouped here) and systolic blood pressure in the target population.\n\n\n\n\n\nIf you have time and want to practice and learn more you can try the following exercises:\n\nRepeat the independent t-test but reverse the coding of the groups to convince yourself that the results are identical but just reversed.\nUse the independent t-test to analyse whether there is a difference in the mean systolic blood pressure between individuals from different socio-economic groups (ses variable). As there are three socioeconomic groups in the variable this would require you to first recode the ses variable into a new variable so that two of the socio-economic groups are pooled to give a binary variable."
  },
  {
    "objectID": "9-independent-t-test-skewed.html",
    "href": "9-independent-t-test-skewed.html",
    "title": "",
    "section": "",
    "text": "What if, based on your research question, your data are apparently suitable for analysing via an independent t-test, but you found that the outcome was actually heavily skewed? This is particular common with count data (i.e. variables that can only take integer values from 0 onwards) where outcomes are typically right skewed because there are no negative counts, many low value counts and fewer higher value counts. Given that the independent t-test assumes, at least approximately, normally distributed data within each comparison group, let’s look at how we deal with a right-skewed (non-normal) outcome.\n\n\n\nUsing a different dataset than the SBP dataset we’ll see how to deal with a skewed outcome variable that violates the assumptions of the independent t-test while still using the independent t-test to analyse the data. Then you can try a similar analysis yourself.\n\nLoad the “Ebola data.sav” SPSS dataset.\n\nAgain this is a simulated dataset. There are three variables and every observation is meant to represent a record from a separate village in a hypothetical region suffering from an Ebola outbreak. The variables contain the following data:\n\nn_ebola_cases = number of Ebola cases per village\nn_chw = number of community health workers per village\npop_density = relative population density per village (1 = <10 per 100m², 2 = ≥ 10 per 100m²)\n\nUsing a paired t-test we’ll analyse whether there is a difference in the mean number of Ebola cases per village between villages with <10 individuals per 100m² and those with ≥ 10 per 100m², i.e. between lower and higher density villages.\n\n\n\n\n\nTechnically an independent t-test assumes a continuous outcome variable, but as long as the following two assumptions are satisfied it’s fine to use a discrete outcome with an independent t-test. Here our outcome is a discrete numerical variable.\n\n\n\nWe will assume this holds here as we didn’t build non-independence into the data, but it may well not hold if this was real data. Can you think why? It’s likely that villages that are closer together have correlated Ebola rates, given spread between villages that are closer together is more likely than between villages that are farther apart. If this was a big issue we could most simply aggregate or pool villages that are closer together to create cluster summary values of the outcome, if these clusters were large enough that there was negligible transmission between clusters, or we could use a more sophisticated technique like a multi-level regression model.\n\n\n\nSee the “Step 1: check the assumptions of the independent t-test” section in the Inferential analysis 1: the independent t-test” section above for a reminder of how to do this using histograms, but this time use the n_ebola_cases variable as the outcome and the pop_density variable as the rows grouping variable in your histograms. You’ll see that the distribution of Ebola cases per population density group is right skewed, particularly for the lower density group. Ideally we’d analyse such data using a special model for count data like a Poisson or negative binomial model, but those approaches are beyond the scope of this introductory course.\nInstead, we can try and transform the outcome to address the skew and then use an independent t-test that assumes normal data. There are many possible ways to transform data, and many are quite complicated and again beyond this course. However, for right skewed data two common and simple transforms that are often sufficient are to:\n\nTake the square root of every value.\nTake the logarithm (usually the natural log) of every value.\n\nThese transformations will hopefully “pull in” the right skewed values so the distribution becomes more symmetrical and approximately normal (although it will never be truly normal). Log transforms will tend to pull the skew in more strongly than square root transforms and are often preferred and probably best to try first. Note: as you cannot take the square root or logarithm of negative numbers (simply speaking) you can only apply these transformations to positive values. Also, while you can take the square root of 0 (it’s 0) you cannot take the logarithm of 0 (it’s “undefined”). Therefore, you must add some constant to every value in an outcome variable before taking the logarithm. Strictly speaking it’s not an ideal approach because different choices of constants will affect your results, but for more rough and ready purposes it’s probably okay to stick with the commonly used choice of adding 1.\nNote: a transformation not always work, in which case you would have to use a non-parametric approach.\n\n\n\nWe can either use the histograms we produce to compare variances in each group or use the result of the Levene’s test once we run the independent t-test.\n\n\n\n\nLet’s create a ln (natural log) transformed variable using the Compute tool to deal with the right-skew in the outcome’s distribution within each group.\n\nFirst explore the variable by creating a frequency table. See the “Categorical variables” sub-section in the “Exercise: create a”Table 1” summarising the key characteristics of the SBP data study sample” section above if you need a reminder of how to create a frequency table. Note: these instructions were for categorical variables but you can use numerical variables too.\nWhat do you notice about the range of the outcome? It includes 0. Therefore, we must add a constant before transforming via the logarithm.\nFrom the main menu go: Transform > Compute Variable. Then in the Compute Variable tool call the new variable ln_n_ebola_cases, but this time enter the transform command “LN(n_ebola_cases+1)” (without quotes) and then click OK. This command first adds 1 to every outcome value before taking the natural log.\nOnce you’ve computed your log-transformed variable check the distribution of values for the transformed outcome in each group using histograms again. You should see that they look more normal now.\n\n\n\n\n\nRun an independent t-test comparing the mean of ln_n_ebola_cases between the lower and higher groups of the pop_density variable. If you can’t remember how to refer back to the previous “Step 3: run the independent t-test” section above. When defining the groups enter Group 1 as “1” (the <10 individuals per 100m² group) and Group 2 as “2” to replicate the results I present, but you could of course compare them the other way round.\n\n\n\n\n\nAgain, refer back to the previous “Step 4: understand the results tables and extract the key results” section above if you need a refresher, but we are just extracting the same results here.\n\n\n\n\nYou should get the following result using the equal variances not assumed set of results, given the Levene’s test is significant and the histograms indicate non-equal variances, which could be reported as follows (remember to explain your analysis process and justify it in your methods):\n\nComparing the <10 per 100m² group (mean Ebola cases per village = 1.94) to the ≥ 10 per 100m² group (Ebola mean cases per village = 6.63) there is a mean difference in the natural-log number of Ebola cases per village of -1 (95% CI: -1.1, -0.9).\n\n\nNote: I’ve presented the group means on the original scale so they can be interpreted easily, and I’ve not bothered presenting the associated p-value above. Again it tells us nothing more and far less than the effect size or mean difference and the associated 95% confidence intervals.\nHowever, this mean difference and its 95% CI is for our outcome but on the natural log scale (i.e. how we transformed it), so it isn’t easy to interpret: what does a mean difference of -1 ln Ebola cases mean? Luckily we can transform (back transform) this mean difference back onto the original scale by using exponentiation with the base e applied to each value (https://en.wikipedia.org/wiki/Exponentiation). This is actually maybe most easily and quickly done just using the Google search engine’s calculator functions. Just type “exp(X)” into Google, where X is either the value of the mean difference or the upper or lower confidence interval value, and it will back-transform those values back to their original scales.\nDoing this for the mean difference and each 95% CI and you should get the following result:\n\n\nMean difference = 0.36 (95% CI: 0.32, 0.4).\n\n\nSo how do we interpret this now? We must take care because we have calculated a difference on natural-log transformed data (via the independent t-test) and then back-transformed that mean difference of ln-transformed values. What we actually then ultimately get is a ratio between the geometric mean (https://en.wikipedia.org/wiki/Geometric_mean) of the outcome in the two comparison groups, rather than a difference in the arithmetic mean of the outcome in the two comparison groups, as we get with an independent t-test where we do not ln-transform the data and then back-transform the results.\nFor example, ln(2) – ln(4) is -0.6931472, and if you calculate the exponential (with base e) of -0.6931472 you get 0.5, and the ratio of 2:4, i.e. 2/4 = 0.5. Or vice versa: if you calculate the exponential of ln(4) – ln(2) you get 2, and the ratio of 4/2 is 2! Therefore, the exponential back-transformed result now represents the a ratio of geometric means. Therefore, like with risk/odds ratios as this result is now on a ratio or multiplicative scale the null value (i.e. the value of no difference between the two groups) is now not 0 but 1, because any number divided by itself = 1. And just like with risk/odds ratios we interpret the result in terms of the number of “times” our reference group’s mean value is compared to the comparison group.\nTherefore, in a results section we can say that “in villages with <10 individuals per 100m² the geometric mean number of Ebola cases was 0.36 (95% CI: 0.32, 0.4) times the geometric mean number of Ebola cases found in villages ≥ 10 individuals per 100m².”\nYou can also view this ratio of means in percentage terms by converting the result using one of the following simple sums, which you may find easier to interpret:\n\n\nWhen the exponentiated difference (D) is <1 the % decrease = (1 - D) x 100. When the exponentiated difference (D) is >1 the % increase = (D - 1) x 100.\n\n\nSo for our result we can calculate that the geometric mean number of Ebola cases was (1 - 0.36) x 100 = 64% lower in low density areas compared to high density areas (you should also transform each confidence intervals range value onto the percentage scale and present them along with the point estimate in any results section).\nNote: geometric means are typically very similar to arithmetic means for outcomes that don’t have a huge range, i.e. that don’t span a number of orders of magnitude. Therefore, for many outcomes you can think of the geometric mean as being approximately equivalent to the arithmetic mean, but this won’t be the case for outcomes with big ranges spanning orders of magnitude from the smallest to the largest value.\nLastly, if we were comparing a numerical variable between two groups what if our data are still badly skewed despite transformation? Then we can use a non-parametric test, such as the Mann-Whitney U test (the most common fall back if the independent t-test cannot be used). We will not cover that in this class so we have more time for more sophisticated tests, but you should have no serious difficulties running and interpreting such a test now using one of the many online or text book guides available (see the MWU SPSS.pdf files in the “Computer Practical sessions” “Additional materials” folder on Minerva). The two big limitations of this test are the reduced power and the fact it only gives you a p-value to accompany your difference (which given the skewed data should arguably be summarised via the median) but no confidence intervals.\n\n\n\n\nThe limitations are the same as discussed in the “Limitations” section of the “Inferential analysis 1: the independent t-test” section.\n\n\n\nUsing the “Ebola.sav” dataset and the process outlined above use an independent t-test to analyse the relationship between community health worker number and Ebola case number.\n\nDivide n_chw into two groups based on n_chw values <5 and those ≥5.\nAs we know the outcome n_ebola_cases is right skewed we must first transform it. We know a natural log (ln) transform works reasonably well, so transform this variable (or use the already transformed version you’ll have created earlier). Remember as there are 0s in the outcome we must also first add a constant before transforming. When I did the analysis I used a constant of 1, so I suggest you use this to avoid any differences, although they should be very minor.\nCompare the number of Ebola cases per village for villages with <5 community health workers to those with ≥5 community health workers using the independent t-test.\nExtract the mean difference and confidence intervals around this estimate and back transform them by exponentiation onto their original scale. Remember you can do this quickly via Google by Googling exp(x) where x is the ln-transformed mean difference/upper or lower confidence interval of the ln-transformed mean difference.\nIn the MSc & MPH computer practical sessions files “Exercises” folder open the “Exercises.docx” Word document and scroll down to Independent t-test with a skewed outcome: the relationship between community health worker number and Ebola case rate within villages.\nWrite a couple of sentences reporting the results of your analysis. Include the basic descriptive statistics: sample size, group sizes and group outcome means (on the original scale). Also be sure to include sufficient details about the outcome variable and the comparison made, including how the two independent groups were defined, as well as the type of analysis used, and of course the key inferential results, but you also need to mention the fact that the data were ln-transformed prior to analysis to deal with the right skew/non-normality and that the mean difference and confidence intervals presented were then back-transformed. Remember you need to interpret the result carefully because the transformation and back-transformation mean that the mean difference is now no longer a simple mean difference in reality. See “Step 5: report and interpret the results” if you need reminding. Round results to one decimal place. You don’t need to explain anything about the study or interpret the clinical or practical importance of the result.\nWrite a sentence or two about the key limitations of this analysis in terms of interpreting the result.\nOnce you’ve completed this compare your reporting to the below example text.\n\nExample results reporting text\n\n\nRead/hide\n\nUsing an independent t-test I analysed the relationship between the number of community health workers per village (<5 compared to ≥5) and the number of Ebola cases per village. Out of a total sample size of 250 villages, 107 had <5 community health workers (arithmetic mean Ebola cases = 3.1) and 143 had ≥5 community health workers (arithmetic Ebola cases = 3.4). Due to a strongly right-skewed outcome I first transformed the outcome by taking the natural log of each outcome value (first adding a constant of 1 due to the presence of 0s) before back-transforming the resulting independent t-test results via exponentiation with the base e. This indicated that villages with <5 community health workers had a geometric mean number of Ebola cases that was 0.9 times (95% CI: 0.7, 1.04) the geometric mean number of Ebola cases among villages with ≥5 community health workers, or equivalently the geometric number of Ebola cases among villages with <5 community health workers was 10% lower than the geometric mean number of Ebola cases among villages with ≥5 community health workers. Therefore, based on the confidence intervals there was no clear or statistically significant relationship between the number of community health workers in a village and the number of Ebola cases. The key limitation of this analysis is that it does not adjust for any other confounding variables, of which there are likely to be many (especially in an observational cross-sectional study like this). Therefore, this is likely to represent a biased estimate of the independent relationship between community health worker number per village and the number of Ebola cases per village in the target population."
  }
]
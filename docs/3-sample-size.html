<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>sample-size</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Home</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-website-information.html" class="sidebar-item-text sidebar-link">1. How to use this website</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">2. Sampling and sample size</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-probability-sampling.html" class="sidebar-item-text sidebar-link">2.1. Probability sampling</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-sample-size.html" class="sidebar-item-text sidebar-link active">2.2. Sample size</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">3. Introducing SPSS &amp; preparing data</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-introduction-to-spss.html" class="sidebar-item-text sidebar-link">3.1. Introducing SPSS</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-preparing-a-dataset.html" class="sidebar-item-text sidebar-link">3.2. Preparing a dataset</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">4. Data exploration, sample &amp; population characteristics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-describing-a-dataset.html" class="sidebar-item-text sidebar-link">4.1 Data exploration &amp; sample characteristics</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-describing-a-population.html" class="sidebar-item-text sidebar-link">4.2. Population characteristics</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">5. Statistical tests for numerical variables</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-independent-t-test.html" class="sidebar-item-text sidebar-link">5.1. Independent t-test</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-independent-t-test-skewed.html" class="sidebar-item-text sidebar-link">5.2. Independent t-test with skewed data</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-paired-t-test.html" class="sidebar-item-text sidebar-link">5.3. Paired t-test</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">6 Statistical tests for categorical variables</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-chi-sq-independence.html" class="sidebar-item-text sidebar-link">6.1. Chi-square test of independence</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-chi-sq-goodness.html" class="sidebar-item-text sidebar-link">6.2. Chi-square goodness of fit test</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-paired-categorical-test.html" class="sidebar-item-text sidebar-link">6.3. Paired categorical variable test (McNemar’s test)</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">7. Regresson modelling</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-linear-regression.html" class="sidebar-item-text sidebar-link">7.1. Linear regression</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-logistic-regression.html" class="sidebar-item-text sidebar-link">7.2. Logistic regression</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-complex-survey-analysis.html" class="sidebar-item-text sidebar-link">8. Complex survey design analysis</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sample-size" id="toc-sample-size" class="nav-link active" data-scroll-target="#sample-size">Sample size</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#key-concepts-and-methods" id="toc-key-concepts-and-methods" class="nav-link" data-scroll-target="#key-concepts-and-methods">Key concepts and methods</a></li>
  <li><a href="#confidence-interval-based-approach" id="toc-confidence-interval-based-approach" class="nav-link" data-scroll-target="#confidence-interval-based-approach">Confidence interval based approach</a>
  <ul class="collapse">
  <li><a href="#overview-1" id="toc-overview-1" class="nav-link" data-scroll-target="#overview-1">Overview</a></li>
  <li><a href="#analytical-studies" id="toc-analytical-studies" class="nav-link" data-scroll-target="#analytical-studies">Analytical studies</a></li>
  <li><a href="#openepi-sample-sizes-for-confidence-intervals" id="toc-openepi-sample-sizes-for-confidence-intervals" class="nav-link" data-scroll-target="#openepi-sample-sizes-for-confidence-intervals">OpenEpi sample sizes for confidence intervals</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing-based-approach" id="toc-hypothesis-testing-based-approach" class="nav-link" data-scroll-target="#hypothesis-testing-based-approach">Hypothesis testing based approach</a>
  <ul class="collapse">
  <li><a href="#overview-2" id="toc-overview-2" class="nav-link" data-scroll-target="#overview-2">Overview</a></li>
  <li><a href="#descriptive-studies" id="toc-descriptive-studies" class="nav-link" data-scroll-target="#descriptive-studies">Descriptive studies</a></li>
  <li><a href="#openepi-sample-sizes-for-hypothesis-testing" id="toc-openepi-sample-sizes-for-hypothesis-testing" class="nav-link" data-scroll-target="#openepi-sample-sizes-for-hypothesis-testing">OpenEpi sample sizes for hypothesis testing</a></li>
  </ul></li>
  <li><a href="#confidence-interval-approach-for-single-variables-using-openepi" id="toc-confidence-interval-approach-for-single-variables-using-openepi" class="nav-link" data-scroll-target="#confidence-interval-approach-for-single-variables-using-openepi">1. Confidence interval approach for single variables using OpenEpi</a>
  <ul class="collapse">
  <li><a href="#estimating-means" id="toc-estimating-means" class="nav-link" data-scroll-target="#estimating-means">1.1. Estimating means</a></li>
  <li><a href="#estimating-proportions" id="toc-estimating-proportions" class="nav-link" data-scroll-target="#estimating-proportions">1.2. Estimating proportions</a></li>
  </ul></li>
  <li><a href="#hypothesis-testing-approach" id="toc-hypothesis-testing-approach" class="nav-link" data-scroll-target="#hypothesis-testing-approach">2. Hypothesis testing approach</a>
  <ul class="collapse">
  <li><a href="#comparing-two-independent-means" id="toc-comparing-two-independent-means" class="nav-link" data-scroll-target="#comparing-two-independent-means">2.1. Comparing two independent means</a></li>
  <li><a href="#comparing-two-independent-proportions" id="toc-comparing-two-independent-proportions" class="nav-link" data-scroll-target="#comparing-two-independent-proportions">2.2. Comparing two independent proportions</a></li>
  </ul></li>
  <li><a href="#exercises-estimating-required-sample-sizes" id="toc-exercises-estimating-required-sample-sizes" class="nav-link" data-scroll-target="#exercises-estimating-required-sample-sizes">Exercises: estimating required sample sizes</a>
  <ul class="collapse">
  <li><a href="#sample-size-exercises-model-answers" id="toc-sample-size-exercises-model-answers" class="nav-link" data-scroll-target="#sample-size-exercises-model-answers">Sample size exercises model answers</a></li>
  </ul></li>
  <li><a href="#reporting-sample-size-calculations-in-methods-sections" id="toc-reporting-sample-size-calculations-in-methods-sections" class="nav-link" data-scroll-target="#reporting-sample-size-calculations-in-methods-sections">Reporting sample size calculations in methods sections</a>
  <ul class="collapse">
  <li><a href="#confidence-interval-based-sample-size-calculation" id="toc-confidence-interval-based-sample-size-calculation" class="nav-link" data-scroll-target="#confidence-interval-based-sample-size-calculation">Confidence interval based sample size calculation</a></li>
  <li><a href="#hypothesis-testing-based-sample-size-calculation" id="toc-hypothesis-testing-based-sample-size-calculation" class="nav-link" data-scroll-target="#hypothesis-testing-based-sample-size-calculation">Hypothesis testing based sample size calculation</a></li>
  </ul></li>
  <li><a href="#compare-paired-means-or-proportions" id="toc-compare-paired-means-or-proportions" class="nav-link" data-scroll-target="#compare-paired-means-or-proportions">Compare paired means or proportions</a></li>
  <li><a href="#adjusting-for-clustering" id="toc-adjusting-for-clustering" class="nav-link" data-scroll-target="#adjusting-for-clustering">Adjusting for clustering</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="sample-size" class="level1">
<h1>Sample size</h1>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>In this section we will be looking at how to calculate the sample size required for a quantitative research study where there the main research question or goal requires statistical inference. For example, we may need to know how many health facilities to sample from a region to be able to describe one or more key health facility characteristics in that region to a given level of precision. Or for a randomised controlled trial we may want to know how many individuals to sample from a given population to give us a specific chance (i.e.&nbsp;probability) of being able to detect a statistically significant difference in our primary (key) outcome between our intervention and control groups.</p>
<p>If you are comfortable with the concepts and basic theory around sample size calculations covered in the lecture you can skip straight onto the instructions for how to carry out sample size calculations using a web-based tool. Just click on <em>Confidence interval approach for single variables using OpenEpi</em> on the navigation bar on the left. However, if you feel unsure in any way about these issues then it is best to take some time to first refresh yourself about them before below.</p>
</section>
<section id="key-concepts-and-methods" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts-and-methods">Key concepts and methods</h2>
<p>Almost every quantitative research study involves one or more research questions (usually the most important ones) that require statistical inference to answer. In brief, statistical inference involves sampling members from a population (ideally using a probability sampling method), collecting data from that sample, and then analysing that data using statistical analyses to draw conclusions about the population. There are two main types of research questions we are usually interested.</p>
<p>First, description. For example, what is the prevalence of COVID-19 in a given population at a given time point? Here, we would call the true (but unknown) prevalence in the population at that time point a population parameter. We would then take a sample from the population and estimate the likely value of this population parameter using our study sample data via a sample statistic (the sample prevalence, i.e.&nbsp;the percentage of individuals in the sample with COVID-19), combined with some measure of how precise this point estimate is likely to be of the population parameter. Usually this measure of precision would be a confidence interval.</p>
<p>Second, casual inference. For example, what effect does one dose of a given COVID-19 vaccine have on the probability of suffering from COVID-19 in the six-months following vaccination in a given population? Here, we might seek to measure the causal effect in terms of the average difference in the probability of suffering COVID-19 in the six-months after vaccination in a randomly selected individual who had the vaccine compared to if they had not had the vaccine. To estimate this population parameter we might take a sample of individuals from the population and run an RCT with, measuring the difference in the proportion of individuals randomly given the vaccine (intervention) who suffer COVID-19 in the six-months following vaccination compared to the proportion of individuals randomly not given the vaccine (control).</p>
<p>We would then again combine this sample statistic with some measure of how precise this point estimate is likely to be of the population parameter. Again, usually this measure of precision would be a confidence interval. However, although our estimate and confidence interval provides the best understanding of the likely direction, size and precision of the causal effect (population parameter) in the population, the dominant approach (at least for RCTs) is to base conclusions about the likely existence of any causal effect (as opposed to any difference being due to sampling error) on a null-significance hypothesis test. We might therefore use an appropriate hypothesis test to test how unlikely it would be to observe an effect at least as great as the one observed if we assume that there is actually no difference in the probability of suffering COVID-19 between our two groups. Then, if the corresponding p-value were less than 0.05 we may conclude the difference is likely to represent a true causal effect of the vaccine in the given population.</p>
<p>So where does sample size come into this? When we seek to make statistical inferences, like in the examples above, the sample size can be loosely thought of as simply the number of units of observation or sampling units that we need to give ourselves a reasonable chance of answering our research question satisfactorily. The units of observation may be individuals, health facilities etc, or they may be individuals, health facilities etc at successive time periods, depending on the study design.</p>
<p>In the sample size calculation we define what we mean by reasonable chance/satisfactorily, and the reason we cannot guarantee that we will answer our research question in an inferential study is because we can only make probabilistic conclusions. This is because we are basing our inferences on samples, which may or may not be representative of our target population due to sampling error (and that’s ignoring other sources of bias/error).</p>
<p>More formally, you make a series of assumptions, such as what level of variation in an outcome we expect to get in our sample, and then a sample size calculation can, in theory, tell you how large a sample size you need to get results with sufficient precision or power (we’ll come back to these concepts shortly). However, the validity or accuracy of a sample size calculation depends entirely on the validity/accuracy of the assumptions, as we’ll discuss. And whether the results can be validly generalised to the target population, assuming the internal validity is perfect, depends on the representativeness of the sample. As always the important thing is to think carefully and critically when planning your sample size, and not just mindlessly plug in some optimistic values.</p>
<p>There are two main approaches typically used for calculating sample sizes for quantitative studies which can be thought of as:</p>
<ol type="1">
<li><p>The confidence interval or precision based approach.</p></li>
<li><p>The hypothesis testing based approach.</p></li>
</ol>
<p>In practice they look quite different, and they do work quite differently in practice. However, they are actually very closely related, particularly in the underlying maths.</p>
<p>In this practical we will make use of the sample size calculation tools on the widely-used web-based epidemiology tool site called OpenEpi. Before we go any further open the website in your browser: <a href="https://www.openepi.com/Menu/OE_Menu.htm">https://www.openepi.com/Menu/OE_Menu.htm</a></p>
<blockquote class="blockquote">
<p>Note: there are many web-based sample size calculators, and many software-based calculators too. Indeed, SPSS, which we will be using for data analysis in subsequent sessions, has a sample size “module”. However, it doesn’t offer the confidence interval based approach for estimating means and proportions that I want to show you. OpenEpi is also freely available and easy to use.</p>
</blockquote>
</section>
<section id="confidence-interval-based-approach" class="level2">
<h2 class="anchored" data-anchor-id="confidence-interval-based-approach">Confidence interval based approach</h2>
<section id="overview-1" class="level3">
<h3 class="anchored" data-anchor-id="overview-1">Overview</h3>
<p>In summary, this is where we calculate the sample size we need to estimate our summary statistic of interest (e.g.&nbsp;a mean, proportion, or linear regression coefficient) with a given level of precision, by which we mean calculate confidence intervals around our sample statistic of interest that are no wider/larger than a pre-specified size/range, <em>assuming all the assumptions that go into the calculation are exactly true</em>. If you need a reminder of the distinction/definition of sample statistics and population parameters see below.</p>
<p>Population parameters and sample statistics:</p>
<details>
<summary>
Read/hide
</summary>
<p>Remember, population parameters are either the exact summary measures of a characteristics’ distribution in the target population, such as the exact mean age of individuals in the target population, or the exact summary measures of a relationship/association, such as the exact mean difference in systolic blood pressure between individuals aged &lt;40 compared to individuals aged ≥40 in the target population. And we estimate the likely values of population parameters, i.e.&nbsp;make statistical inferences about their likely values, which we can rarely if ever know for sure, based on: 1) the equivalent sample statistics that we calculate from our sample, such as the <em>sample</em> mean age of individuals or the <em>sample</em> mean difference in systolic blood pressure between individuals aged &lt;40 compared to individuals aged ≥40, and 2) the associated confidence intervals around those statistics. Or, if we are taking a hypothesis testing based approach to inference (see below), we estimate whether the population parameter of interest is likely to differ from some null hypothesis value based on a p-value calculated from the sample data.</p>
</details>
<p>This approach can be used for any sample statistic including measures of relationships such as differences in means/proportions or regression coefficients, but the confidence interval approach is mainly used when the aim is to describe population characteristics (something we will look at in section 4.2). This is typically in the context of a cross-sectional/repeated cross-sectional/longitudinal survey generating descriptive outcome measures for numerical or categorical variables in terms of means and proportions respectively. Note: any categorical outcome can be analysed as a series of binary outcomes based on each unit of observation having or not having the characteristic represented by each level of the categorical variable. For example, when sex is coded as having two levels, male and female, each level can be analysed as a binary variable, i.e.&nbsp;the proportion of individuals who are male (implicitly compared to the proportion who are not-male, i.e.&nbsp;female), or vice versa. Or for the categorical variable religion where we code it as having three levels, none, Christian, Muslim, we can analyse this as three related binary variables: religion-none = yes/no, religion-Christian = yes/no, and religion-Muslim = yes/no. Therefore, in many surveys most outcomes will be categorical variables (particularly from self-responses), and if there is no clear primary outcome the sample size is often based on obtaining a level of precision for a generic binary variable, when all categorical variables will be analysed as a series of binary variables (i.e.&nbsp;inferential statistics, namely confidence intervals, will be calculated for each binary variable).</p>
</section>
<section id="analytical-studies" class="level3">
<h3 class="anchored" data-anchor-id="analytical-studies">Analytical studies</h3>
<p>Why is the confidence interval approach rarely used for analytical studies?</p>
<details>
<summary>
Read/hide
</summary>
<p>The lack of use of the confidence interval approach in these situations is probably because of the dominance of the null hypothesis significant testing (NHST) approach to analytical inference, which we’ll look at next, whereas with cross-sectional survey studies the main aim is often estimating a range of characteristics to a given level of precision rather than testing hypotheses. And as we will see below the when using a NHST approach then the more natural/logical sample size calculation approach is the hypothesis testing approach to sample size calculations. However, I would argue that given the benefits of using confidence intervals for making statistical inferences rather than NHST tests, we should make use of the confidence interval sample size approach as the norm.</p>
</details>
</section>
<section id="openepi-sample-sizes-for-confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="openepi-sample-sizes-for-confidence-intervals">OpenEpi sample sizes for confidence intervals</h3>
<p>OpenEpi only offers the confidence interval approach for the situation where you are estimating a mean or a proportion, but other sample size software allows you to use this approach for a wider range of outcome measures (e.g.&nbsp;rates).</p>
</section>
</section>
<section id="hypothesis-testing-based-approach" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing-based-approach">Hypothesis testing based approach</h2>
<section id="overview-2" class="level3">
<h3 class="anchored" data-anchor-id="overview-2">Overview</h3>
<p>The hypothesis testing approach is primarily used for analytical studies that are trying to understand whether a given relationship exists. It is usually the case that the relationship of interest will be studied and analysed in terms of a difference in a continuous or binary outcome between two groups, which are usually independent groups but they can be related. However, the relationship may also be analysed in terms of a linear regression coefficient for a continuous independent variable, a logistic regression odds ratio, or other similar measure, but these are much less commonly seen. Also, as we explain below, in theory this approach can also be used for descriptive studies, but this is very rarely done. As these other approaches/scenarios are rarely/very rarely used we won’t look at them further and we below will just assume we are considering the approach where our relationship of interest is analysed in terms of a difference in a continuous or binary outcome between two <strong>independent</strong> groups.</p>
<p>The hypothesis testing approach is arguably harder to understand well than the confidence interval approach. It may be easiest to understand once you understand the process. Therefore, we will explain it in summary here by detailing the key steps you go through.</p>
<p>First, we pre-specify a target difference that represents the smallest difference in our outcome between the two groups that we want to be able to detect. This target difference also explicitly or implicitly incorporates our assumption about what level of variation will exist in the outcome variable in the sample data. We also pre-specify the threshold below which we would declare a p-value from a NHST test of the null hypothesis that there is no difference in the target population “statistically significant”. That is, the threshold below which we would reject the null hypothesis and assume that the alternative hypothesis of a difference existing in the target population was more likely. This is usually the conventional P≤0.05 level.</p>
<p>Then the last main assumption is maybe harder to understand. Here we pre-specify the probability that we will obtain a p-value from a NHST of the the null hypothesis that is statistically significant based on our chosen threshold for significance (i.e.&nbsp;equal to or below our chosen threshold). Technically speaking this probability is the proportion/percentage of the time we would obtain a p-value from a NHST of the the null hypothesis that is statistically significant, based on our chosen threshold for significance, if we were to repeat the study an infinite number of times. The reason we can’t guarantee we will get a statistically significant p-value even if all our other assumptions are correct is because of sampling error. Even if the true difference in the outcome between the two groups that exists in the target population is equal to (or larger) than our pre-specified target difference, in any given randomly selected sample we might, due to sampling error, select a sample where the difference in the sample doesn’t reflect the difference in the target population and is in fact smaller. However, we can ensure that this only happens a given proportion of the time.</p>
<p>Then conditional on our assumption about the target difference in the sample data being exactly true, and conditional our assumption about the level of variation in the outcome in our sample data being exactly true, and conditional on their being no bias/error in the study and analysis other than sampling error, the resulting calculation will tell us what sample size we need to obtain to ensure that we have the pre-specified probability that we set of obtaining a statistically significant p-value when testing the null hypothesis.</p>
<p>Unfortunately there is quite a lot of technical terminology associated with this approach that we haven’t used yet for obvious reasons. However, we must familiarise ourselves with this terminology. First, the threshold at which we declare an effect statistically significant and reject the null hypothesis is known as the “alpha level” or the “level of statistical significance”, and the probability that we will be able to detect our target difference if it exists via getting a p-value less than our level of significance is known as the “power” of the hypothesis test. The power is also equivalent to 1 - “beta” (i.e.&nbsp;“1 minus beta”), where beta is the probability (in the long-run) of getting a false negative result, i.e.&nbsp;the probability of not rejecting the null hypothesis when it is actually false. Therefore, power is the probability of correctly rejecting the null hypothesis when it is false in the long-run assuming all assumptions that go into the sample size calculation are true.</p>
<p>You may be wondering why we need to pre-specify values for the alpha level and the power. Why not just set both to their maximum so that we always obtain a statistically significant result? The short answer is there is a trade-off with the resulting required sample size, and by convention researchers typically therefore set alpha to 0.05 (or less commonly 0.01) and power to 0.8 (or less commonly 0.9). See below for more details.</p>
<p>Why can’t we always minimise our alpha level and maximise our power?</p>
<details>
<summary>
Read/hide
</summary>
<p>More specifically, alpha is also the false positive rate of a hypothesis test, or the rate at which we will falsely declare a difference statistically significant in the long-run when conducting null hypothesis significance tests in a given scenario. Therefore, we want this to be as small as possible to avoid making mistakes, i.e.&nbsp;falsely declaring there to be a statistically significant difference, which we interpret to mean there is likely to really be a difference in the target population. However, the smaller we set the alpha level the larger the sample size required to detect any given difference as statistically significant for a given power (i.e.&nbsp;the larger the sample size required to detect any given difference as statistically significant with a given probability). Similarly, the higher we set the power level the larger the sample size required to detect any given difference as statistically significant for a given alpha level or level of statistical significance. This trade-off is simply a function of the maths underlying hypothesis testing approach sample size calculations: we need more statistical information and therefore a larger sample size to both reduce how often we make false positive decisions and increase how often we make true positive decisions from NHST. Similarly, for a given alpha and power level we require more statistical information or a larger sample size to detect as statistically significant smaller and smaller target differences (assuming they exist).</p>
<p>Therefore, for any given target difference we clearly want to minimise our level of statistical significance to reduce our false positive rate while maximising our power level to increase our chances of detecting statistically significant differences (assuming they exist!), while not requiring an unfeasibly large sample size. Consequently, typical values of alpha used in almost all sample size calculations are either 0.05 (the common statistical significance threshold) or less commonly 0.01 (i.e.&nbsp;5% or 1%), while typical values for the power are 0.8 or less commonly 0.9 (i.e.&nbsp;80% or 90% power). Very broadly speaking, these values typically allow you to come up with a feasible sample size calculation for not “unreasonably” small target differences. However, these are not magic values and they are very much arbitrary conventions with no logical or natural basis for them other than the fact that people like round numbers, and a 5% false positive rate (0.05) and an 80% chance of detecting a difference as statistically significant if it exists seemed like “reasonable” values to researchers who have gone before us, given how much these two values/assumptions “cost” in terms of the required sample size.</p>
</details>
</section>
<section id="descriptive-studies" class="level3">
<h3 class="anchored" data-anchor-id="descriptive-studies">Descriptive studies</h3>
<p>Hypothesis based approach to sample sizes for descriptive studies:</p>
<details>
<summary>
Read/hide
</summary>
<p>This hypothesis testing based approach can also be used for estimating sample sizes related to sample statistics that measure characteristics, e.g.&nbsp;means and proportions as estimated in a cross-sectional survey, if you wanted to test hypotheses about whether those characteristics differ from a given null hypothesis value (i.e.&nbsp;an assumed population value). However, this approach is almost only ever used to calculate sample sizes required for primarily analytical studies where the main research questions are about associations/relationships, and then this is usually typically further restricted/framed just in terms of differences between two groups, e.g.&nbsp;differences between two means or two proportions, where the intention is to use NHST to see whether the observed sample difference differs significantly from the assumed null hypothesis difference (usually of no difference).</p>
</details>
</section>
<section id="openepi-sample-sizes-for-hypothesis-testing" class="level3">
<h3 class="anchored" data-anchor-id="openepi-sample-sizes-for-hypothesis-testing">OpenEpi sample sizes for hypothesis testing</h3>
<p>OpenEpi offers the hypothesis testing approach for differences between independent means and proportions and paired means and proportions. Here we just cover sample size calculations for hypothesis tests of differences between independent means and proportions, because the paired situation is more complicated and should be avoided unless you know what you are doing or have assistance from a statistician. Note: in the literature the term “sample size calculation” is often used simply to mean the hypothesis testing approach, because outside of cross-sectional studies the confidence interval based approach is so infrequently used given the dominance of null hypothesis significance testing.</p>
</section>
</section>
<section id="confidence-interval-approach-for-single-variables-using-openepi" class="level2">
<h2 class="anchored" data-anchor-id="confidence-interval-approach-for-single-variables-using-openepi">1. Confidence interval approach for single variables using OpenEpi</h2>
<section id="estimating-means" class="level3">
<h3 class="anchored" data-anchor-id="estimating-means">1.1. Estimating means</h3>
<section id="scenario" class="level4">
<h4 class="anchored" data-anchor-id="scenario">Scenario</h4>
<p>You work for a regional ministry of health in a region where public health facilities have reported increasing numbers of patients seeking treatment for cardiovascular diseases over the last decade. However, there is no good data on the cardiovascular health of the population. Therefore, your department has tasked you with conducting a population survey on the cardiovascular health of the population. As it is hard to measure actual cardiovascular health you will focus on systolic blood pressure as a key proxy indicator or risk factor for cardiovascular health. The primary aim of the survey is therefore to estimate the distribution of blood pressure, specifically systolic blood pressure (mmHg), values within the target population, along with collecting other relevant health and socio-demographic data. As the primary or key outcome variable is systolic blood pressure and your aim is to estimate the distribution of this outcome in the target population a confidence interval based approach to the required sample size makes most sense. This is because this approach will allow you to plan a sample size that will enable you to estimate the distribution of the outcome to a certain level of precision (i.e.&nbsp;to estimate it with a certain maximum confidence interval width).</p>
</section>
<section id="sample-size-assumption-inputs" class="level4">
<h4 class="anchored" data-anchor-id="sample-size-assumption-inputs">Sample size assumption inputs</h4>
<p>When estimating the sample size required to estimate a mean with confidence intervals of a maximum desired width there are just three assumption inputs to consider.</p>
<p>1. What confidence level do you want for your confidence interval?</p>
<p>The convention is a 95% confidence interval, and unless you have a good reason to choose otherwise use this.</p>
<ul>
<li>Therefore, we will use <strong>95%</strong>.</li>
</ul>
<p>2. What standard deviation (SD) is your outcome variable expected to have in your sample data?</p>
<p>This may be estimated from values in the literature from similar studies, or from pilot data (although this is risky as pilot studies by definition are small and cannot produce unreliable estimates of any sample statistics), or you can use the following rough rule of thumb:</p>
<blockquote class="blockquote">
<p>Take the range of values for your outcome that roughly 95% of the population are likely to fall between/within. Divide this by 4 for a conservative estimate of the population SD for the outcome.</p>
</blockquote>
<p>For our example, in our scenario we might assume, from clinical knowledge/existing literature, that about 95% of people in our target population have systolic blood pressure values between 80 and 150 mmHg. Therefore, the expected range is 150 - 80 = <strong>70</strong>. We then divide this by 4: 70/4 = <strong>17.5</strong>. You should then round this up for safety to at least 18, although for greater safety you might round up further (say to 20). The larger the assumed SD the larger the sample size required, but the safer you will be because you have less chance of finding out that the SD in your sample is actually higher than you assumed, which could mean you won’t then achieve your desired precision level. Note: this rule of thumb only applies to variables that are, at least approximately, normally distributed. See the following paper for an improved but slightly more complicated approach to estimate SDs: https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-135</p>
<ul>
<li>Therefore, we will use <strong>18 mmHg</strong>.</li>
</ul>
<p>3. What is the minimum level of precision that you want your confidence intervals to have?</p>
<p>This should be based on practical considerations, such as what level of precision will be useful for users of the results of the study such as clinicians, health administrators, and policy makers etc. Here we will assume that our result would only be judged to robust and useful by clinicians if our confidence intervals are a maximum of +/- 2.5 mmHg. This means that whatever mean we estimate we want the confidence intervals to be no wider than that mean plus 2.5 mmHg and that mean minus 2.5 mmHg. Note: the desired confidence interval precision does not depend on the likely/assumed value of the outcome. Also note: we have defined the precision of our desired confidence interval in terms of the “half-width” of the confidence interval, which is just the upper confidence interval value minus the lower confidence interval value (i.e.&nbsp;the confidence interval range) divided by 2. You will also often see this half-width referred to as the “margin of error”: https://en.wikipedia.org/wiki/Margin_of_error</p>
<ul>
<li>Therefore, we will set our confidence interval half-width to <strong>2.5 mmHg</strong>.</li>
</ul>
<p>4. What response rate do you expect?</p>
<p>It is very rare to achieve a 100% response rate, so typically you should use previous values in the literature, if they exist, along with any pilot data, and past experience, to decide on a likely response rate. This should be a conservative/safe assumption, i.e.&nbsp;round down a “sufficient” extent, because experience shows researchers typically overestimate the response rate. OpenEpi does not allow you to automatically adjust the sample size for the response rate so we have to do this manually, but we can do this very easily as follows:</p>
<blockquote class="blockquote">
<p>Sample size adjusted for &lt;100% response rate = desired sample size (i.e.&nbsp;the result from the calculation, which assumes a 100% response rate) / assumed response rate as a proportion.</p>
</blockquote>
<p>For example, if we did our sample size calculation and we got a required sample size of 92, then if we assume we will likely only get a response rate of 90% or higher (0.9 on the proportion scale, i.e.&nbsp;90/100), then we actually need to aim to collect a sample size of 92/0.9 = <strong>103</strong> (rounding up). So we need to approach 103 people to end up with at least 92 who participate (assuming our response rate assumption is accurate).</p>
<blockquote class="blockquote">
<p>Note: while this will ensure that you have at least 92 individuals and that you achieve your desired level of precision, if peoples’ participation is related to their values of the outcome of interest then recruiting more people won’t stop your results from suffering response bias. Increasing the sample size will increase the precision of the estimate but it can never reduce any such bias! So you can certainly have a very precisely estimated (narrow confidence intervals) but very biased result.</p>
</blockquote>
<ul>
<li>Therefore, we will assume a response rate of <strong>0.9</strong>.</li>
</ul>
</section>
<section id="calculate-the-required-sample-size" class="level4">
<h4 class="anchored" data-anchor-id="calculate-the-required-sample-size">Calculate the required sample size</h4>
<p>Sorry, there is currently no video instructions for this method. Please use the written instructions below.</p>
<p><strong>Written instructions: calculate the sample size required to estimate a population mean with a given level of precision in OpenEpi</strong></p>
<details>
<summary>
Read/hide
</summary>
<ul>
<li><p>On the OpenEpi <a href="https://www.openepi.com/Menu/OE_Menu.htm">website</a> from the navigation menu on the left click the tiny “+” symbol next to the <mark>Continuous Variables</mark> folder to open it. Then click on <mark>Mean CI</mark>. Now either click on the <mark>Enter</mark> tab at the top of the screen of the <mark>Enter New Data</mark> button at the top below the <em>Open Soruce Statistics for Public Health</em> title.</p></li>
<li><p>Now let’s enter our assumptions/the calculation parameters. In the form titled <em>Confidence Intervals for a Sample Mean</em> in the first row opposite <em>Sample Mean</em> enter our value for the assumed sample mean as 0. Just click on the existing value of 150 and type 0. However, note that for this type of sample size calculation it doesn’t actually matter what assumed mean we enter, as the confidence intervals won’t change (so you could equally enter any other value). Next on the <em>Sample Std. Deviation</em> row we will enter a value for the assumed SD as 18. You can also instead enter a value for the assumed standard error or variance (i.e.&nbsp;the square of the SD), but we made an assumption about the SD earlier so we will use that.</p></li>
<li><p>Now we want to know what sample size we need to estimate our population mean such that the 95% confidence intervals are of a given width, specifically ±2.5 (mmHg). Unfortunately, one limitation of OpenEpi is that for this type of sample size calculation you can’t ask for the sample size directly. The way they have setup the calculator is that you give it an assumed SD, a desired confidence level and an expected sample size, and then it tells you the precision or width of the cofidence intervals you would obtain when estimating the mean. Therefore, we need to work backwards and try out different sample sizes until we find one that will give us our desired confidence interval width.</p></li>
<li><p>Let’s just start with a sample size of 100, so on the next row opposite <em>Sample Size</em> enter 100. We can leave the assumed <em>Population Size</em> as effectively infinite. We can also leave the desired confidence interval confidence level as 95%.</p></li>
<li><p>Now click the <mark>Calculate</mark> button above the <em>Confidence Intervals for a Sample Mean</em> form. This takes us to the <em>Results</em> tab. It shows us our input data (our assumptions), and then below <em>95% Confidence Limits for the Mean of 0</em> it tells us the lower and upper limits (i.e.&nbsp;interval values) we would get based if we calculated the confidence intervals assuming the data follow a normal distribtion (see the values on the row <em>Based on: z-test</em>) or a t-distribution (see the values on the row <em>Based on: t-test</em>). We will use the t-test row values as the t-distribution is best when you can assume your outcome is approximately normal but you may only have a relatively small sample size, as the t-distribution allows for more variation at smaller sample sizes and becomes identical to the normal distribution at large sample sizes. We will also see how to estimate population means using the t-distribution in section <em>4.2. Population characteristics</em>.</p></li>
<li><p>We can see our t-based 95% confidence intervals would be -3.6 and 3.6 (for normal or t-based confidence intervals they will always be symmetrical) for our initial assumptions. We want greater precision though: specifically 95% confidence intervals with width ±2.5. So we can go back to the <mark>Enter</mark> tab and our assumptions will be the same as before. So let’s try changing the assumed sample size to 200 (we want more precision, which requires a larger sample size).</p></li>
<li><p>Great! We’re nearly there. If we want to be really precise then we the sample size that gives us confidence interval limits of 2.5 exactly or just under though, not just over, so we can play about with some slightly higher values. If you do so you will find that a sample size of 202 gives us what we want.</p></li>
<li><p>Finally, we need to adjust our sample size for our assumed response rate of 90%. Remember we just need to divide our sample size assuming a 100% response rate by our assumed response rate on the proportion scale:</p></li>
</ul>
<blockquote class="blockquote">
<p>202 / 0.9 = 244.4.</p>
</blockquote>
<ul>
<li>So, rounding up, our final required sample size is <strong>245</strong>.</li>
</ul>
<blockquote class="blockquote">
<p>Therefore, assuming the expected population SD is 18, and employing the t-distribution to calculate our confidence intervals, and assuming a response rate of 90%, the study would require a sample size of <strong>245</strong> to estimate the population mean systolic blood pressure (mmHg) with 95% confidence intervals of width ± 2.5.</p>
</blockquote>
<p>Remember though, if you achieve the required sample size you will only achieve your desired level of precision if the other assumptions in the calculation are accurate, i.e.&nbsp;if the SD of the outcome in the sample equals the assumed SD. If the actual SD in the sample data is larger than the pre-specified expected value then the precision you achieve, i.e.&nbsp;the half-width of the confidence intervals around your estimated mean, will be larger than your desired level of precision. Note: the opposite is also true, i.e.&nbsp;you’ll get better precision than expected if the SD turns out to be smaller than expected. Also, irrespective of the level of precision, the estimated population mean will only be unbiased on average if there is no systematic difference in outcome values between non-responders and responders, and if there are no other sources of bias impacting the data. Similarly, if our sample is not taken via a probability sampling method then formally we cannot be sure that the interpretation of our result applies to the population.</p>
</details>
</section>
<section id="additional-considerations" class="level4">
<h4 class="anchored" data-anchor-id="additional-considerations">Additional considerations</h4>
<p>What if you are estimating multiple means in a quantitative survey? For example, for our scenario where we are conducting a survey on cardiovascular health we might also measure salt intake, cigarettes smoked per day, BMI etc. Then you probably have two main approaches depending on the situation. First, if there is a clear, primary research question/objective then you can base the sample size of the outcome that allows you to answer that research question/acheive that objective. For example, if the primary aim of our survey was to estimate the distribution of systolic blood pressure in our target population then we could base the sample size on this outcome alone, and essentially hope that this is also a sufficient sample size to estimate our other numerical outcomes with sufficient precision. Second, if there is no clear, primary research question/objective then decide which outcomes you need to achieve a given level of precision for when estimating their distribution, and simply choose the sample size that is largest. That way you ensure that you will achieve at least sufficient precision for all your outcomes. For example, if we found we needed a sample size of 100 to estimate our systolic blood pressure outcome with sufficient precision and a sample size of 150 to estimate our salt intake outcome with sufficient precision, and these were our two key outcomes, then we would aim for a sample size of 150.</p>
</section>
</section>
<section id="estimating-proportions" class="level3">
<h3 class="anchored" data-anchor-id="estimating-proportions">1.2. Estimating proportions</h3>
<section id="scenario-1" class="level4">
<h4 class="anchored" data-anchor-id="scenario-1">Scenario</h4>
<p>You work for the regional ministry of health and the ministry leaders wish to know how frequently the public primary care facilities in the region run out of one or more drugs on the essential medicines list (known as a drug stock-out). They therefore plan to conduct a cross-sectional survey of public primary care facilities in the region and record whether each facility ran out of one or more drugs on the essential medicines list within the last month of the survey or not. Therefore, the outcome is binary (yes/no) and is most naturally summarised as a proportion/percentage. The ministry of health want a clear answer on this issue so they want to be fairly certain of the proportion of public primary care facilities across the whole region that have experienced such a drug “stock-out” within the last month. After some discussion it is agreed that you will try to give them an answer to this percentage within ± 5 percentage points.</p>
</section>
<section id="sample-size-assumption-inputs-1" class="level4">
<h4 class="anchored" data-anchor-id="sample-size-assumption-inputs-1">Sample size assumption inputs</h4>
<p>When estimating a proportion for a binary outcome there are also three assumptions to input when calculating your sample size based on confidence intervals.</p>
<p>1. What confidence level do you want for your confidence interval?</p>
<p>The convention is a 95% confidence interval, and unless you have a good reason to choose otherwise use this.</p>
<ul>
<li>Therefore, we will use <strong>95%</strong>.</li>
</ul>
<p>2. What is your expected prevalence/proportion/percentage?</p>
<p>Remember, these are all equivalent measures but on potentially different scales, and you can covert between proportions and prevalences/percentages (prevalences are usually given as percentages, but can be given as proportions) by multiply/dividing by 100. We’ll just refer to the percentage from here on as that’s the scale that most people prefer to use.</p>
<p>Note: unlike for a mean and for technical reasons related to the assumed probability distribution that a binary variable follows we do not specify an expected level of variation in the outcome like a SD, because for a binary variable the variation is assumed to be related to the mean, i.e.&nbsp;to the underlying probability or proportion. Therefore, we just need to specify the assumed proportion. As with the SD, you may either base your expected proportion on prior estimates from the literature, and/or from pilot studies, or if there is no information to go on then the safest (most conservative) option is to use an expected proportion of 0.5. This is because for any given sample size the confidence intervals you obtain when estimating a proportion will be widest around a proportion of 0.5, so if you assume a proportion of 0.5 then whatever level of precision you pre-specify you will guarantee that you will achieve that level of precision (conditional on obtaining the required sample size) or greater (if the proportion turns out to be &lt;0.5 or &gt;0.5. Again, technically this is because the probability distribution of a binary variable assumes that the variance is greatest when the underlying probability (i.e.&nbsp;proportion) is 0.5, and declines proportionally for values &lt;0.5 or &gt;0.5.</p>
<p>However, an assumption of a proportion of 0.5 can be very conservative and result in a much larger sample size than might be necessary even if you are “playing it safe”. Therefore, it’s usually best to play around with this assumption based on your best estimate/guess and see how conservative you can go while still requiring a feasible sample size. For example, drug stock-outs are thought to be fairly rare and unlikely to occur in the previous month in more than 5 percent of facilities on average. Therefore, it doesn’t make sense to assume a proportion 0.5 as it’s very unlikely that you’d be this wrong, but to be conservative it is agreed you will assume a higher percentage of 10% to be the true frequency.</p>
<ul>
<li>Therefore, we’ll assume an outcome proportion of <strong>0.1 (10%)</strong>.</li>
</ul>
<p>3. What is the minimum level of precision that you want your confidence intervals to have?</p>
<p>The same considerations apply as for the case when estimating a mean, i.e.&nbsp;consider the practical implications and requirements of your results. However, unlike for continuous outcomes the level of precision for a binary outcome can be specified in absolute terms or in relative terms, i.e.&nbsp;relative to the expected proportion. For example, if we have an expected outcome proportion of 0.1 (10%) and we want our confidence intervals to be no wider than the range: lower confidence interval = 0.05 (5%), upper confidence interval = 0.15 (15%). Then on on the absolute scale of percentage points our half-width is 0.05 or 5 percentage points, because our confidence interval range is simply our expected proportion plus or minus 0.05, or our expected percentage plus or minus 5 percentage points. Note: loosely speaking percentage points are what you call differences between percentages when viewed on an absolute scale. If you are not clear on the difference between percentage points and percentage difference see here for more explanation: https://www.mathsisfun.com/percentage-points.html</p>
<p>In OpenEpi we just enter our desired confidence interval width on the absolute scale, but if you want to understand about what is meant by the relative scale you can read the below.</p>
<details>
<summary>
Read/hide
</summary>
<p>On a relative scale our confidence interval half-width is 0.5 or 50%. This is because our confidence interval range is what we get when we take 0.5 or 50% of our expected proportion and add and subtract the result from our expected proportion:</p>
<blockquote class="blockquote">
<p>0.1 X 0.5 = 0.05.</p>
</blockquote>
<p>And then our lower confidence interval =</p>
<blockquote class="blockquote">
<p>0.1 - 0.05 = 0.05.</p>
</blockquote>
<p>And our upper confidence interval =</p>
<blockquote class="blockquote">
<p>0.1 + 0.05 = 0.15.</p>
</blockquote>
<p>If you are struggling to understand all this then don’t worry it is confusing and takes a while to “get your head around”, so keep at it.</p>
</details>
<ul>
<li>Therefore, we’ll assume we want a confidence interval that goes from <strong>0.05 to 0.15</strong>, so a width of 0.1 or a half-width of 0.05.</li>
</ul>
<p>4. What is your expected response rate?</p>
<p>As discussed for the estimating a mean situation if you expect &lt;100% response rate you should adjust the sample size for the assumed response rate.</p>
<ul>
<li>Therefore, again we’ll assume a response rate of <em>90% (0.9)</em>.</li>
</ul>
</section>
<section id="calculate-the-required-sample-size-1" class="level4">
<h4 class="anchored" data-anchor-id="calculate-the-required-sample-size-1">Calculate the required sample size</h4>
<p>Sorry, there is currently no video instructions for this method. Please use the written instructions below.</p>
<p><strong>Written instructions: calculate the sample size to estimate a single proportion with a given level of precision in OpenEpi</strong></p>
<details>
<summary>
Read/hide
</summary>
<ul>
<li><p>From the OpenEpi navigation menu expand the <em>Sample Size</em> folder if required and click on <em>Proportion</em>. At the top go to the <mark>Enter</mark> tab. Then in the <em>Sample Size for % Frequency in a Population (Random Sample)</em> form we can leave the first row (<em>Population size</em>) as it is.</p></li>
<li><p>Change the <em>Anticipated % frequency(p)</em> row value from 50 to 10 (i.e.&nbsp;our assumption that the population percentage is likely to be 10% or higher).</p></li>
<li><p>Now we need to decide on our desired confidence interval width. Remember we want a confidence interval that, assuming the population percentage is 10%, goes from 5% to 15%, i.e.&nbsp;with a half-width of 5 percentage points. In the row <em>Confidence limits as +/- percent of 100</em> the assumed value is already actually 5, i.e.&nbsp;5 percentage points, so we can leave this as it is.</p></li>
<li><p>You can ignore the final row as this relates to the situation where you are using cluster or multi-stage sampling. You can read a very brief description of this issue at the end of this page. Therefore, you can now click the <mark>Calculate</mark> button.</p></li>
<li><p>Again, we then are moved to the <em>Results</em> tab and given a table with our inputs/assumptions listed. However, we are now given a list of results where each row indicates the required sample size (under the <em>Sample Size</em> heading) for a given confidence interval confidence level (the <em>ConfidenceLevel(%)</em> heading), even though we could select our desired confidence interval confidence level previously. Odd! Anyway, we only need to look at the first row as this relates to a 95% confidence interval.</p></li>
<li><p>Here we can see we required a sample size of 139. Let’s adjust for our response rate:</p></li>
</ul>
<blockquote class="blockquote">
<p>139 / 0.9 = 154.4.</p>
</blockquote>
<ul>
<li>So, rounding up, our final required sample size is 155.</li>
</ul>
<blockquote class="blockquote">
<p>Therefore, assuming that the sample and population percentage of facilities experiencing a drug stock-out within the last month is 10%, and assuming a response rate of 90% in the survey, then the study would require a sample size of <strong>154</strong> to estimate the expected proportion such that the 95% confidence intervals of this estimate are ±5 percentage points (i.e.&nbsp;go from 5% to 15%).</p>
</blockquote>
<p>Note that this assumes you are estimating your population proportion based on 95% confidence intervals calculated via the “Wald method”, which assumes the outcome variable is normally distributed. Formally speaking this is not possible for a binary variable, such as you would use to estimate the proportion. However, when the proportion is not too extreme (&gt;0.1 and &lt;0.9) and the sample size is at least 30, this normal approximation works fairly well compared to other potentially more technically appropriate confidence interval approaches for proportions. We will see how to calculate Wald-based proportion confidence intervals and other approaches in section <em>4.2. Population characteristics</em>. For other proportion confidence interval approaches this sample size may be somewhat downward biased, and it would make sense to either use a different sample size calculation based on the specific approach you will use to calculate the confidence intervals, or at least make your sample size inputs slightly more conservative.</p>
<p>Also remember that achieving the level of precision implied by the sample size calculation depends on whether the estimated proportion in the sample data is equal to the expected proportion and on any other assumptions such as the true response rate being equal to your assumption. If the estimated proportion is further from 0.5 than your expected proportion (e.g.&nbsp;0.07) then you will have better precision than your pre-specified level of precision, assuming your response rate is equal or better than the assumed rate, and similarly if your response rate is better than your assumed rate you will have better precision than your pre-specified level of precision, assuming the estimated proportion is equal to or further from 0.5 than your expected proportion. And vice versa, i.e.&nbsp;your precision will be worse than the pre-specified value if the estimated proportion is closer to 0.5 than the expected proportion and/or if the response rate is worse than the assumed rate.</p>
<p>Then as with the sample size for a mean, the same considerations apply around biases. Increasing the sample size will always increase the precision of your estimates, but it will never affect the influence of any study bias!</p>
</details>
</section>
<section id="additional-considerations-1" class="level4">
<h4 class="anchored" data-anchor-id="additional-considerations-1">Additional considerations</h4>
<p>See the “Additional considerations” section in the “Estimating a mean” section above for a discussion of some useful guidance when facing common additional considerations.</p>
</section>
</section>
</section>
<section id="hypothesis-testing-approach" class="level2">
<h2 class="anchored" data-anchor-id="hypothesis-testing-approach">2. Hypothesis testing approach</h2>
<section id="comparing-two-independent-means" class="level3">
<h3 class="anchored" data-anchor-id="comparing-two-independent-means">2.1. Comparing two independent means</h3>
<section id="overview-3" class="level4">
<h4 class="anchored" data-anchor-id="overview-3">Overview</h4>
<p>The typical scenario where you would use this type of sample size calculation is when the research question is concerning whether there is a difference in the mean of a continuous outcome (or a discrete numerical outcome if it is distributed approximately normally) between two independent groups within a population. For example, when comparing the effect of an intervention on an outcome in a RCT. The typical analysis for such a research question here would be via an independent t-test, or more powerfully a linear regression with a binary independent categorical variable to estimate the between-group difference (again based on a t-based null-hypothesis significance test), and additional independent variables, measuring characteristics of the subjects related to the outcome, to increase the precision of the estimate. We will see how to carry out independent t-tests in section <em>5.1. Independent t-test</em>, and linear regression in section <em>7.1. Linear regression</em>.</p>
</section>
<section id="scenario-2" class="level4">
<h4 class="anchored" data-anchor-id="scenario-2">Scenario</h4>
<p>You work for a research NGO in a country where public health facilities have reported increasing numbers of patients seeking treatment for cardiovascular diseases over the last decade. You have received funding to develop and pilot test an intervention that aims to improve the diagnosis and treatment of cardiovascular disease in your country’s public health facilities. Briefly, as this is a pilot test you plan to select one typical health facility and use an (individually) randomised controlled trial to compare relevant patient health outcomes in patients who are randomly allocated to be diagnosed and treated using the new intervention processes to patients who are randomly allocated to be diagnosed and treated using the existing processes. As cardiovascular disease events are rare you will use systolic blood pressure (mmHg) as a proxy outcome for cardiovascular health/risk. You will measure a range of other outcomes (i.e.&nbsp;secondary outcomes) and relevant health and socio-demographic data to use as independent variables in your analysis. However, as systolic blood pressure is your sole primary outcome you will base the sample size on this outcome alone. The idea is that if the intervention appears potentially effective based on this pilot study a multi-facility cluster trial will follow to provide definitive evidence. Therefore, based on consultations with clinicians and health officials in your country it has been agreed/decided that the smallest mean reduction in systolic blood pressure which will considered clinically meaningful/significant, and therefore indicating that the intervention should be tested in a large-scale cluster trial, is 5 mmHg.</p>
</section>
<section id="sample-size-assumption-inputs-2" class="level4">
<h4 class="anchored" data-anchor-id="sample-size-assumption-inputs-2">Sample size assumption inputs</h4>
<p>There are more assumption inputs to consider when taking a hypothesis testing approach to sample size calculations, but several of the inputs are typically fixed at conventional levels - although this should never mean that you just mindlessly select those levels, there’s always room for thought!</p>
<p>1. What alpha (α) level or level of significance do you want?</p>
<p>The lower this is set the less likely the resulting hypothesis test is to generate a type I error or a false positive result on average (or in the long-run), assuming you achieve the required sample size and that all other sample size assumption inputs are accurate. However, the lower this is set the larger the sample size required holding all other assumption inputs constant, so there’s a trade-off with the sample size. By convention the level of significance is usually set to be 0.05 or 0.01, i.e.&nbsp;the levels at which we commonly determine statistical significance: when P≤0.05 or less commonly P≤0.01. Again this is probably because it’s a nice round number and assumed to be reasonable trade-off. Unless you have a good reason to change this and know what you are doing then leave this as the default 0.05.</p>
<ul>
<li>Therefore, we’ll leave the level of significance as its default at <strong>0.05</strong>.</li>
</ul>
<p>2. What power (1-β) do you want?</p>
<p>The higher this is set the more likely it is that the resulting hypothesis test will correctly reject a false null hypothesis, if the hypothesis is indeed false and the mean difference in the sample data is at least as large as the one assumed (see below), on average (or in the long-run), and assuming you achieve the required sample size and that all other sample size assumption inputs are accurate. However, again there is a trade-off: the larger this is set the larger the sample size required. By convention this is set at either 0.8 or less commonly 0.9. Again this is probably because it’s a nice round number and assumed to be reasonable trade-off. There’s absolutely no reason not to aim for a higher level of power though if you can afford the resulting sample size, and you can play around with this input and see how it affects your sample size.</p>
<ul>
<li>We’ll leave the power as its default at <strong>0.8</strong>.</li>
</ul>
<p>3. What is the expected difference in your outcome between the two independent groups (i.e.&nbsp;the expected difference in the two group means) and the expected variation in the outcome?</p>
<p>This is where we pre-specify the target difference that we want to be able to detect, which is a reduction of 5 mmHg in our scenario. In Statulator you can enter this information either in terms of the “Expected Means” or as an “Expected Difference between Means”. If you enter it as using the “Expected Means” option then you enter the expected “Mean of the Reference Group”, the expected “Mean of the Test Group”, and the expected “Standard Deviation”. For our scenario the expected mean of the reference group is the expected mean of the control or usual care group, while the expected mean of the test group is the expected mean of the new intervention group. However, for a sample size comparing independent means the actual means don’t matter, just their difference (e.g.&nbsp;a mean of 5 vs 10 requires the same sample size as a mean of 105 vs 110 as the difference is 5 in each case). Therefore, we can compare 0 (mmHg) in the reference group to 5 (mmHg) in the test group.</p>
<p>The expected standard deviation is the expected pooled standard deviation, i.e.&nbsp;assuming the outcome has the same standard deviation in both groups. As with the confidence interval approach to estimating a mean we can select this based on values in the literature, pilot data, and/or using the rule of thumb previously discussed. If you expect the standard deviation to be different in each group, which is often the case (e.g.&nbsp;outcomes in intervention arms are often less variable as processes are more standardised), then just use the bigger of the two standard deviations as the pooled standard deviation will always be smaller than this, i.e.&nbsp;it’s a conservative/safe approach. Let’s assume our expected shared or pooled standard deviation is 10 mmHg.</p>
<ul>
<li>Therefore, we’ll use the expected difference between means approach and enter the expected difference as <strong>-5</strong> (mmHg), i.e.&nbsp;a reduction of 5 mmHg, and the expected shared or pooled standard deviation as <strong>10</strong> (mmHg), to ensure we obtain a sample size based on our target difference.</li>
</ul>
<p>If the true difference in the target population is greater than this assumption then we’ll have more power on average than we expect. Note: it doesn’t make any difference to the calculation whether the target difference is positive or negative only the absolute value matters, so entering 5 or -5 won’t change the result.</p>
<p>4. What is the ratio of group sizes that you expect?</p>
<p>For any given total sample size, if the sample size in each of the two groups differs then you will not have the expected power that you pre-specified, and your power reduces as the ratio of group sizes gets further from 1 (i.e.&nbsp;the group sizes become increasingly different). Therefore, as it is common that you will not get exactly equal group sizes, and often quite different group sizes, you should always plan accordingly and build this into your sample size assumptions. As with the expected response rate it makes sense to base your assumed group size ratio on data from previous studies, pilot data, and/or past experience, and that you make it a conservative/safe assumption. Note: you adjust for non-equal group sizes by specifying the expected ratio of group sizes. In Statulator it asks for the ratio of the group size in the reference group compared to the test group. Therefore, you have to specify the ratio in terms of this direction of comparison.</p>
<p>However, as is commonly the case, if there is no reason to believe that any one particular group will be larger than the other, but there is reason to believe that one of the groups will be larger than the other but you just can’t predict which, then it’s probably easiest to just think in terms of how much larger in percentage terms one group is likely to be compared to another. If you then convert this percentage to a proportion and add it to 1 that’s your group size ratio. For example, if you believe one group is likely to be 25% larger than another then 25% = 0.25, and therefore our group size ratio is 1.25. As our scenario is that we are planning for an RCT, and you can usually guarantee a fairly equal group size ratio in an RCT, we’ll assume our group size ratio will be no more than 5% different in favour of either group.</p>
<ul>
<li>Therefore, we’ll assumed an expected group size ratio of <strong>1.05</strong>.</li>
</ul>
<p>5. What is the expected response rate?</p>
<p>As with the estimating a mean tool the independent group means tool doesn’t have an option to automatically adjust for the expected response rate and so we’ll have to do it manually.</p>
<ul>
<li>We’ll assume a response rate of <strong>0.9 (90%)</strong>.</li>
</ul>
</section>
<section id="calculate-the-sample-size" class="level4">
<h4 class="anchored" data-anchor-id="calculate-the-sample-size">Calculate the sample size</h4>
<p>Sorry, there is currently no video instructions for this method. Please use the written instructions below.</p>
<p><strong>Written instructions: calculate the sample size required when comparing two independent means in OpenEpi</strong></p>
<details>
<summary>
Read/hide
</summary>
<ul>
<li><p>From the menu on the left expand the <em>Sample Size</em> folder if required and click <em>Mean Difference</em>.</p></li>
<li><p>Click on the <mark>Enter</mark> tab. Unfortunately, in another quirk of OpenEpi, in the <em>Sample Size for Comparing Two Means</em> form the first row refers to the <em>Confidence Interval % (two-sided)</em>. However, this is an unusual and probably confusing way to refer to what would normally be termed the significance level or alpha. This is what we’ve termed this parameter above when describing the inputs required. The usual value for this is 0.05 or 5%, which corresponds to a 95% confidence interval, so we can leave it as it is.</p></li>
<li><p>The next row <em>Power</em> refers to our desired power. Again, the conventional value for this is 80%, although many studies also use higher values like 85% or 90%. However, we decided earlier to accept a power of 80% so we can also leave this value unchanged.</p></li>
<li><p>In the next row (<em>Ratio of sample size (Group 2/Group 1)</em>) we need to specify the expected ratio of group sizes. It asks for this in terms of group 2’s size divided by group 1’s size. If we expect the SD of the outcome to be the same in each group then it doesn’t matter if we enter the ratio as group 1’s / group 2’s or vice versa. However, if we expect the SDs to vary then we need to be sure we calculate the ratio the correct way around. As we are assuming equal SDs per group here we can just enter our previously assumed ratio of 1.05.</p></li>
<li><p>Next, on the <em>Mean</em> row we can either enter each group’s mean, or as the absolute values don’t matter but just the <em>Difference</em>. Remember, this is the minimum difference we want to be able to detect, but not necessarily the difference we think most likely exists. Let’s delete the values for <em>Group 1</em> and <em>Group 2</em> and just enter the <em>or Difference</em> value as 5. Again, as previously stated we can just enter the absolute value. Whether it’s +5 or -5 makes no difference to the required sample size.</p></li>
<li><p>Finally, on the <em>Std. Dev.</em> row we enter the assumed SD for each group. We assume a shared SD of 10, so enter this under <em>Group 1</em> and <em>Group 2</em>. Then click the <mark>Calculate</mark> button.</p></li>
<li><p>We are taken to the <mark>Results</mark> tab. Below the input data look for the <em>Sample Size of Group 1</em>, <em>Sample Size of Group 2</em> and <em>Total sample size</em> rows. Here are our required sample sizes. Remember, in our scenario we only assumed a group size ratio of 1.05 to allow for chance imbalances in group size, but we had no expectation about which group would be larger. Therefore, in this scenario we would just take the total sample size and allocate it equally to our intervention and control groups.</p></li>
<li><p>However, we would finally need to adjust this for the assumed response rate:</p></li>
</ul>
<blockquote class="blockquote">
<p>128 / 0.9 = 142.2</p>
</blockquote>
<ul>
<li>As this is an odd number when rounded and we need to divide it we would add 2.</li>
</ul>
<blockquote class="blockquote">
<p>Therefore, we need to sample and recruit 144 / 2 = <strong>72</strong> individuals to the intervention and control groups to ensure we have an 80% chance (our power) of detecting a difference between each groups’ mean systolic blood pressure (mmHg) of 5mmHg or greater, on the assumption that such a difference exists in the population, via a null-hypothesis significance test based on a two-sided p-value with a significance level of 5%.</p>
</blockquote>
<ul>
<li>That’s quite a complicated interpretation. Another way to think about it is that if we did our study an infinite number of times, each time calculating the p-value for the null-hypothesis that the difference between the two means we obtain = 0, and rejecting that null-hypothesis when the p-value is ≤0.05, then if the true difference in mean systolic blood pressure in the population between individuals given the intervention and those not given it is ≥5mmHg, then on average 80% of the time (in 80% of our repeated studies) we would get a p-value ≤0.05 and correctly reject the null-hypothesis. Still confused? I’m afraid it’s a complex series of concepts without an easy interpretation, and you just have to keep coming back to it until it clicks.</li>
</ul>
<p>Again, this calculation assumes you will be carrying out an independent t-test of the difference between your group means, either via a classical independent t-test or within the context of a linear regression model.</p>
<p>However, as for the previous sample size calculations, the accuracy of this interpretation depends entirely on the underlying assumption that there is no bias in the study. If, for example, the 90% response rate is correct, but reflects the fact that 10% of individuals drop out of the study. Then if all 10% drop out from the intervention group due to side effects of the new treatment regimes, even if the true effect of the intervention is to reduce systolic blood pressure by &gt;5mmHg, we might have a power &lt;80%. That is, even if we recruit 72 participants per group we might actually only have a 10% chance of detecting a statistically significant difference between the two groups’ mean systolic blood pressures, because of the influence of this bias (known as differential loss to follow-up).</p>
</details>
</section>
</section>
<section id="comparing-two-independent-proportions" class="level3">
<h3 class="anchored" data-anchor-id="comparing-two-independent-proportions">2.2. Comparing two independent proportions</h3>
<section id="overview-4" class="level4">
<h4 class="anchored" data-anchor-id="overview-4">Overview</h4>
<p>The typical scenario where you would use this type of sample size calculation is when the research question is concerning whether there is a difference in a binary outcome between two independent groups (and remember you can always convert a numerical outcome to a binary outcome, and you can always convert a categorical outcome with &gt;2 categories into a binary outcome, although it may not always make sense to do so). The typical study design for such a research question would either an RCT (most robust) or some form of observational comparison design, like a cohort study or an uncontrolled before and after study if the before group was independent of the after group (less common). The typical analysis for such a research question in these designs would be via a chi-square test of independence or a logistic regression with a binary independent categorical variable (plus maybe additional independent variables).</p>
</section>
<section id="scenario-3" class="level4">
<h4 class="anchored" data-anchor-id="scenario-3">Scenario</h4>
<p>You work for a research NGO in a country where public health facilities have reported increasing numbers of patients seeking treatment for diabetes over the last decade. You have received funding to develop and pilot test an intervention that aims to improve the diagnosis and treatment of diabetes in your country’s public health facilities. Briefly, as this is a pilot test you plan to select one typical health facility and use an (individually) randomised controlled trial to compare relevant patient health outcomes in patients who are randomly allocated to be diagnosed and treated using the new intervention processes to patients who are randomly allocated to be diagnosed and treated using the existing processes. You decide to define diabetes based on the international guideline of having a fasting plasma glucose level ≥7.0 mmol/l, and your primary outcome will therefore be the binary outcome of whether a patient has a fasting plasma glucose ≥7.0 mmol/l or &lt;7.0 mmol/l, i.e.&nbsp;whether they currently “have” diabetes/not. You will measure a range of other outcomes (i.e.&nbsp;secondary outcomes) and relevant health and socio-demographic data to use as independent variables in your analysis. However, as having diabetes/not is your sole primary outcome you will base the sample size on this binary outcome alone. The idea is that if the intervention appears potentially effective based on this pilot study a multi-facility cluster trial will follow to provide definitive evidence.</p>
<p>Binary outcomes are naturally summarised as proportions/percentages, and a point prevalence is simply the proportion of individuals (or other units) that have a characteristic of interest at a certain point in time. Therefore, we can view our comparison of interest for our analysis as being a comparison between the proportion/percentage or point prevalence of diabetes in the intervention group compared to the proportion/percentage or point prevalence of diabetes in the control group.</p>
<p>Unlike when you are calculating a sample size when comparing independent means, as we’ll discuss further below, there is a complicating issue. It is not just the difference in the outcome between the two groups that affects the sample size but also the value of the outcome (i.e.&nbsp;the assumed proportion) in each group.</p>
<p>For our scenario though we’ll assume that we have good data on the existing point prevalence of diabetes among patients being treated for diabetes (i.e.&nbsp;the proportion of patients who have a fasting plasma glucose ≥7.0 mmol/l) in public health facilities, and that this is no greater than 0.3 (30%). We’ll also assume that based on consultations with clinicians and health officials in your country it has been agreed/decided that the smallest reduction in the prevalence of diabetes which will be considered clinically meaningful/significant, and therefore indicating that the intervention should be tested in a large-scale cluster trial, is a reduction from 0.3 to 0.2 (i.e.&nbsp;from 30% to 20%).</p>
</section>
<section id="sample-size-assumption-inputs-3" class="level4">
<h4 class="anchored" data-anchor-id="sample-size-assumption-inputs-3">Sample size assumption inputs</h4>
<p>Several of the assumptions are identical to those for the comparing two means sample size calculation, and so we will not explain them again.</p>
<p>1. What alpha (α) level or level of significance do you want?</p>
<ul>
<li>We’ll leave the level of significance as its default at <strong>0.05</strong>.</li>
</ul>
<p>See the relevant description in the comparing two means section above if you need a reminder about this parameter.</p>
<p>2. What power (1-β) do you want?</p>
<ul>
<li>We’ll leave the power as its default at <strong>0.8</strong>.</li>
</ul>
<p>See the relevant description in the comparing two means section above if you need a reminder about this parameter.</p>
<p>3. What is the expected difference in your outcome between the two independent groups (i.e.&nbsp;the expected difference in the two group proportions)?</p>
<p>As mentioned earlier we must be explicit about the outcome proportion expected in each group to set our minimum target difference we want to detect. We cannot just specify the difference alone. In OpenEpi we can specify the difference either by specifying the assumed percentage of the outcome in each group, or the assumed percentage of the outcome in the “Unexposed” group (i.e.&nbsp;the control group for an RCT or a comparison group for a non-randomised study) plus the assumed risk ratio or risk difference (or prevalence ratio or prevalence difference - they are the same thing).</p>
<p>As with the sample size for the mean difference, the difference we specify here should be the minimum target difference: the minimum difference we want to be able to detect, should it exist. Many studies instead base their sample size on the difference they expect (hope!) to see, but this inevitably is usually overly optimistic given the resource and monetary costs associated with a larger sample size. Therefore, such an optimistic approach often leads to wasted effort and money, because the sample size turns out to be too small to produce useful results.</p>
<ul>
<li>We’ll specify the expected proportions for each group explicitly, based on the minimum difference we want to be able to detect. We will therefore specify the assumed percentage outcome for the unexposed (control) group as <strong>30</strong> and the expected percentage for the exposed (intervention) group as <strong>20</strong> to ensure we obtain a sample size based on our minimum target difference.</li>
</ul>
<p>If the true difference we find is greater than this then we’ll have more power than we expect.</p>
<p>Note: similar to the sample size calculation for estimating a population proportion, this one assumes the sampling distribution for the difference in proportions is normally distributed. Although we are estimating a difference in proportions, and each proportion comes from a binary variable that cannot formally be normally distributed, the central limit theorem allows us to assume the difference of these two proportions will be normally distributed (at least when we have a reasonable sample size - usually expected to be 30+ per group). OpenEpi does however offer two slightly different variations on the sample size approach for this situation. We won’t go into the details here, but you can look at the <em>Documentation</em> link on the test page for more details.</p>
<p>Due to the normal distribution assumption the test therefore also assumes you will use a “z-test” to test for the difference in proportions. A z-test is similar to a t-test but assumes the data are normally distributed. They are rarely used compared to t-tests though so we do not cover them in this course. However, in practice most analysts would probably use a logistic regression model to analyse the difference in a binary outcome between two groups, usually also adjusting for other variables that are likely confounding variables or to increase precision. We will look at logistic regression in section <em>7.1. Linear regression</em>. Without going into the details the standard null-hypothesis significance test of the “model coefficient” estimating the relevant difference in a logistic regression model is the “Wald test”, which is effectively a z-test.</p>
<p>4. What is the ratio of group sizes that you expect?</p>
<p>See the description in the comparing two means section above.</p>
<ul>
<li>Again we’ll assume we have no reason to believe that either group will be larger than the other, but we do expect that one of the groups is likely to be slightly larger than the other by chance due to recruitment differences, and so we’ll assume a group size ratio of <strong>1.05</strong>.</li>
</ul>
<p>5. What is the expected response rate?</p>
<ul>
<li>Again, we’ll assume a response rate of <strong>0.9 (90%)</strong>.</li>
</ul>
</section>
<section id="calculate-the-sample-size-1" class="level4">
<h4 class="anchored" data-anchor-id="calculate-the-sample-size-1">Calculate the sample size</h4>
<p>Sorry, there is currently no video instructions for this method. Please use the written instructions below.</p>
<p><strong>Written instructions: calculate the sample size required when comparing two independent proportions in OpenEpi</strong></p>
<details>
<summary>
Read/hide
</summary>
<ul>
<li><p>From the menu on the left expand the <em>Sample Size</em> folder if required and click <em>Cohort/RCT</em>.</p></li>
<li><p>Go to the <mark>Enter</mark> tab at the top. In the first row ( <em>Two-sided confidence interval(%)</em>) we again have the unusually-worded alpha value, which we leave at the 5% level (i.e.&nbsp;here specified as a 95% confidence level for the test).</p></li>
<li><p>In the next row (<em>Power (1-beta or % chance of detecting)</em>) we have our desired power, which we can also leave at the default value of 80%.</p></li>
<li><p>In the next row (<em>Ratio of Unexposed to Exposed in sample</em>) we set our assumed group size ratio, so let’s change this to 1.05.</p></li>
<li><p>In the next row (<em>Percent of Unexposed with Outcome</em>) we set our assumed “Unexposed” group outcome percentage, which for us means our assumed control group percentage. So set this to 30(%).</p></li>
<li><p>In the next section it says we just need to fill in one of the following. Skip the next row and on the row <em>Percent of Exposed with Outcome</em> we can enter our assumed “Exposed” group outcome percentage, which for us means our assumed intervention group percentage, such that the difference between the two groups is our desired target minimum difference we want to detect. Set this to 20(%).</p></li>
<li><p>Now click the <mark>Calculate</mark> button. Again, we can see all the inputs made to the calculation, and below them the results. As mentioned there are three sets of results corresponding to three slightly different sample size calculations: Kelsey Fleiss, and Fleiss with CC. There’s not necessarily much good research of which to prefer, but generally speaking the one with the largest sample size is probably the safest and makes the most conservative assumptions, which is often the opposite of what occurs in research planning in reality. So let’s use the Fleiss with CC approach.</p></li>
<li><p>It tells us we need 617 overall. Although we’ve made an assumption about the group ratio we didn’t actually have a reason to believe one specific group would be higher than other, just that one would likely differ from the other. So let’s just stick with the overall value and divide by 2. First let’s adjust for the assumed response rate:</p></li>
</ul>
<blockquote class="blockquote">
<p>617 / 0.9 = 685.5.</p>
</blockquote>
<blockquote class="blockquote">
<p>Therefore, we need to sample and recruit 686 / 2 = <strong>343</strong> individuals to the intervention and control groups to ensure we have an 80% chance (our power) of detecting a difference in the prevalence of diabetes between the groups of 10 percentage points or more, on the assumption that such a difference exists in the population and that the difference is control = 30% intervention = 10%, via a null-hypothesis significance test based on a two-sided p-value with a significance level of 5%.</p>
</blockquote>
<ul>
<li>Again, that’s quite a complicated interpretation. Another way to think about it is that if we did our study an infinite number of times, each time calculating the p-value for the null-hypothesis that the difference between the proportions we obtain = 0, and rejecting that null-hypothesis when the p-value is ≤0.05. Then if the true difference in diabetes prevalence in the population between individuals given the intervention and those not given it 10 percentage points lower in individuals given the treatment (and the prevalence is 30% in individuals not given the treatment), then on average 80% of the time (in 80% of our repeated studies) we would get a p-value ≤0.05 and correctly reject the null-hypothesis. Still confused? I’m afraid it’s a complex series of concepts without an easy interpretation, and you just have to keep coming back to it until it clicks.</li>
</ul>
<p>Again, as for the previous sample size calculations, the accuracy of this interpretation depends entirely on the underlying assumption that there is no bias in the study. If, for example, the 90% response rate is correct, but reflects the fact that 10% of individuals drop out of the study. Then if all 10% drop out from the intervention group due to side effects of the new treatment regimes, even if the true effect of the intervention was to reduce the prevalence of diabetes by &gt;10 percentage points (and the true prevalence in untreated individuals was 30%), we might have a power &lt;80%. That is, even if we recruit 343 participants per group we might actually only have a 10% chance of detecting a statistically significant difference between the two groups’ prevalence of diabetes, because of the influence of this bias (known as differential loss to follow-up).</p>
<p>And as always the validity and accuracy of this calculation depends entirely on the accuracy of the assumptions that you made when calculating the sample size.</p>
</details>
</section>
</section>
</section>
<section id="exercises-estimating-required-sample-sizes" class="level2">
<h2 class="anchored" data-anchor-id="exercises-estimating-required-sample-sizes">Exercises: estimating required sample sizes</h2>
<p>In the MSc &amp; MPH statistics computer sessions practical files “Exercises” folder open the “Exercises.docx” Word document and click on the <strong>Planning sample sizes</strong> heading in the contents and follow the instructions, then check your answers with the model answers below.</p>
<p>Optional hint: type of sample size calculations required for each scenario</p>
<details>
<summary>
Read/hide
</summary>
<ul>
<li><p>Scenario 1: given the scenario we are aiming to calculate the sample size required to estimate a single proportion (although in reality it will apply to all the proportions we estimate in the scenario survey).</p></li>
<li><p>Scenario 2: given the scenario we are aiming to calculate the sample size required to detect compare two means (or more specifically we are aiming to detect a difference between two means as statistically significant, i.e.&nbsp;with a p-value less than some threshold, with a given level of power).</p></li>
</ul>
</details>
<section id="sample-size-exercises-model-answers" class="level3">
<h3 class="anchored" data-anchor-id="sample-size-exercises-model-answers">Sample size exercises model answers</h3>
<details>
<summary>
Read/hide
</summary>
<section id="exercise-1-public-primary-care-health-facility-patient-satisfaction-survey" class="level4">
<h4 class="anchored" data-anchor-id="exercise-1-public-primary-care-health-facility-patient-satisfaction-survey">Exercise 1: public primary-care health facility patient satisfaction survey</h4>
<p>For our survey we estimated that for any population proportion we wish to estimate (e.g.&nbsp;from a binary outcome or a category level from a categorical outcome with &gt;2 levels) we require a sample size of 122 (i.e.&nbsp;we need to try and recruit 97 facilities into the survey) to achieve a 10 percentage point level of precision (95% confidence interval ±10 percentage points), assuming the proportion we estimate is 0.5, and assuming a response rate of 80% (remember to adjust the OpenEpi sample size for the assumed response rate).</p>
</section>
<section id="exercise-2-pilot-intervention-study-to-reduce-drug-stock-outs" class="level4">
<h4 class="anchored" data-anchor-id="exercise-2-pilot-intervention-study-to-reduce-drug-stock-outs">Exercise 2: pilot intervention study to reduce drug stock-outs</h4>
<p>We estimated that we required a total sample size of 94 health facilities, i.e.&nbsp;47 per group, to detect a difference in the mean number of essential drug stock-outs during the three-month study period (the primary outcome) in the intervention group compared to the comparison group of -15 or greater, assuming a standard deviation of 25, based on a hypothesis test of the difference in the primary outcome between the two groups (assuming the outcome is t-distributed), with a level of significance of 0.05 and a power of 0.8. We also assumed a follow-up rate of 95%.</p>
<p>Remember to adjust for the response rate, and then to add 1 if you end up with an odd number to allow you to divide by 2 for the group sizes.</p>
</section></details>
</section>
</section>
</section>
<section id="reporting-sample-size-calculations-in-methods-sections" class="level2">
<h2 class="anchored" data-anchor-id="reporting-sample-size-calculations-in-methods-sections">Reporting sample size calculations in methods sections</h2>
<details>
<summary>
Read/hide
</summary>
<p>For any methods and results reporting, such in a paper or report, you should explain what your study sample size was and how you calculated it, including all the assumptions that went into it. You should also report where any assumptions came from, i.e.&nbsp;what they were based on/how you chose them, unless they were chosen by convention, such as a 95% confidence interval or a level of significance of 5%. There is no set sequence in which you have to report the different assumptions/inputs used in your sample size calculation, but the below examples are one way you might do it. Lastly, you don’t have to explicitly state whether you are reporting a sample size calculated via a confidence interval or hypothesis testing based approach as this should be clear from the assumptions reported.</p>
<p>In the below examples we will not report any assumptions about the response rate or group size ratio, but if you make an assumption for these other than 100% or 1 respectively this should also be reported, and the basis for these assumptions justified. Also, the assumption values are just for illustration and not real!</p>
<section id="confidence-interval-based-sample-size-calculation" class="level3">
<h3 class="anchored" data-anchor-id="confidence-interval-based-sample-size-calculation">Confidence interval based sample size calculation</h3>
<p>When estimating a mean you simply need to report the expected standard deviation, the level of precision or margin of error, and the confidence level (this is often not presented, presumably because the assumption is 95%, but why not be clear?). You should also explain where your assumption of the expected standard deviation and level of precision came from.</p>
<p>Example methods reporting text for a confidence interval based sample size calculation for a survey of systolic blood pressure:</p>
<ul>
<li>We estimated that we required a sample size of 100 to estimate the mean of our primary outcome of systolic blood pressure (mmHg) with a level of precision (95% confidence intervals) at most ± 5 mmHg, assuming a standard deviation of 10. We based our assumption of the expected standard deviation on data from our previously reported pilot study (reference), which we rounded up from 7 to 10 to be conservative. Our level of precision was chosen based on consultations with relevant clinicians and health officials about what level of precision they required for usable results.</li>
</ul>
<p>When estimating a proportion you simply need to report the expected proportion, the level of precision or margin of error, and the confidence level. You should also explain where your assumption of the expected proportion and level of precision came from.</p>
<p>Example methods reporting text for a confidence interval based sample size calculation for a survey of hypertension prevalence:</p>
<ul>
<li>We estimated that we required a sample size of 100 to estimate the proportion of individuals with hypertension, which we assumed to be 0.3, with a level of precision (95% confidence intervals) where the lower limit was at most 0.25 and the upper limit was at most 0.3. We based our assumption of the expected proportion on data from our previously reported pilot study (reference) of 0.2, which we rounded up by 0.1 towards the most conservative value of 0.5 to be conservative. Our level of precision was chosen based on consultations with relevant clinicians and health officials about what level of precision they required for usable results.</li>
</ul>
<p>Take care, when reporting the confidence interval ranges for your proportion/percentage that you are clear whether they are on an absolute scale (probably the easiest to understand and not misinterpret) or a relative scale.</p>
</section>
<section id="hypothesis-testing-based-sample-size-calculation" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-testing-based-sample-size-calculation">Hypothesis testing based sample size calculation</h3>
<p>For a hypothesis test based sample size calculation for a difference between two independent means you simply need to report the expected difference in means (or if you prefer the expected mean in each group) and the expected pooled (or common) standard deviation, plus your pre-specified level of significance/alpha and the power, and that you are using a two-sided hypothesis test (unless of course you are not). You should also explain where your assumption about the expected difference in means (i.e.&nbsp;the target difference) and the expected pooled standard deviation came from. Lastly, although it’s often not done you should explain what distribution you are assuming your outcome follows</p>
<p>Example methods reporting text for a hypothesis test based sample size calculation for a study comparing the effectiveness of an intervention at reducing systolic blood pressure in an two-group comparison RCT:</p>
<ul>
<li>We estimated that we required a sample size of 100 to detect a reduction in our primary outcome of systolic blood pressure (mmHg) between our intervention and control arms that is at least 5 mmHg, based on a two-sided hypothesis test (assuming the t-distribution). This assumes a pooled standard deviation of 10 mmHg, the standard level of significance of 0.05, and a power of 0.8. Our target difference for detection was chosen based on consultations with relevant clinicians and health officials about what size of impact would be required for the intervention to be considered worth funding and scaling up. Our expected standard deviation was chosen based on values from relevant previous studies (references), which we rounded up from 7 to 10 to be conservative.</li>
</ul>
<p>For a hypothesis test based sample size calculation for a difference between two independent proportions you simply need to report the expected proportion for each group (or equivalently the expected proportion in the reference/control group and the absolute or relative expected difference in the proportional outcome, but this is arguably less easy to follow), plus your desired level of alpha and power, and that you are using a two-sided hypothesis test (unless of course you are not). You should also explain where your assumption about the expected difference in proportions (i.e.&nbsp;the target difference) came from.</p>
<p>Example methods reporting text for a hypothesis test based sample size calculation for a study comparing the effectiveness of an intervention at reducing the prevalence/proportion of individuals having hypertension in an two-group comparison RCT:</p>
<ul>
<li>We estimated that we required a sample size of 100 to detect a difference in the proportion of individuals with hypertension at study follow-up where we expect the proportion with hypertension in the intervention group is 0.2 and the expected proportion in the control group is 0.3, based on a two-sided hypothesis test. This assumes the standard level of significance of 0.05 and a power of 0.8. Our target difference for detection was chosen based on consultations with relevant clinicians and health officials about what size of impact would be required for the intervention to be considered worth funding and scaling up, with the expected proportion of individuals with hypertension in the control arm based on existing routine clinical data rounded up from 0.25 to 0.3 (towards the most conservative assumption of 0.5) to be conservative.</li>
</ul>
</section>
</details></section>
<section id="compare-paired-means-or-proportions" class="level2">
<h2 class="anchored" data-anchor-id="compare-paired-means-or-proportions">Compare paired means or proportions</h2>
<p>We will not look at these sample size scenarios/approaches or practice them as they are not commonly needed, but they are for when your study design involves comparing means or proportions between two groups where those groups are not independent. For example, if you are comparing the change in systolic blood pressure (mmHg) in the same individuals where their systolic blood pressure is measured at two separate times, or where you are comparing the proportion of individuals with diabetes where that diagnosis is made at two separate times within the same group of individuals. Most of the assumptions that go into these calculations are exactly the same as for the calculations we’ve already covered and the rest you should be able to work out or get help with if you ever need to use them, which is not likely.</p>

</section>
<section id="adjusting-for-clustering" class="level2">
<h2 class="anchored" data-anchor-id="adjusting-for-clustering">Adjusting for clustering</h2>
<details>
<summary>
Read/hide
</summary>
<p>We will not look at adjusting for clustering beyond saying that if you believe this issue applies to your study you should seek advice from a statistician/researcher experienced with making the necessary adjustments. What is clustering? As an example, if you collect data on pupils within different schools to look at test scores then those pupils within the same schools are likely to have correlated test scores. This is due to differences at the school level, such as difference in the overall quality of teaching, the socio-economic circumstances of the schools’ catchment areas, whether they charge fees etc. Hence, you don’t have the same amount of statistical information as for a true simple random sample, because pupils are not independent when they come from the same school. Standard sample size calculations, such as those we’ve looked at, assume your sample data are independent. They would assume that two randomly selected pupils from the same school are no more or less likely to have similar outcome values, such as test scores, than two randomly selected pupils from separate schools. Rarely will this be the case. Therefore, unless you adjust for the “level of clustering” in your outcome you won’t achieve the level of precision or power that you expect to get from a given sample size even if all the other assumptions are accurate. There are different ways of measuring the “amount of clustering” in an outcome, and it’s a more advanced topic beyond the scope of this introductory course that you should seek assistance with if you need to carry out such a sample size calculation in the future.</p>
</details>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>